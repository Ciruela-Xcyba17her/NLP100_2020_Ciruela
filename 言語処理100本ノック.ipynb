{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "言語処理100本ノック.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNf2WD+JH+efUrPQ2h1dHK1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ciruela-Xcyba17her/NLP100_2020_Ciruela/blob/master/%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Nr6K3DfAnS",
        "colab_type": "text"
      },
      "source": [
        "# 言語処理100本ノック2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBlLqxMGfMH4",
        "colab_type": "text"
      },
      "source": [
        "## 第1章: 準備運動"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3ZQkkqHnJ",
        "colab_type": "text"
      },
      "source": [
        "### `00.` 文字列の逆順"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4E3mzkxqiYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 00. 文字列の逆順\n",
        "def q00():\n",
        "  s = 'stressed'\n",
        "  s = s[::-1]\n",
        "  print(s)\n",
        "\n",
        "q00()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4PaJ-rhq95N",
        "colab_type": "text"
      },
      "source": [
        "### `01.` 「パタトクカシーー」"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsLeXDhLfQ0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 01. 「パタトクカシーー」\n",
        "\n",
        "def q1():\n",
        "  s = 'パタトクカシーー'\n",
        "  answer = s[0] + s[2] + s[4] + s[6]\n",
        "  print(answer)\n",
        "\n",
        "q1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9EhR5ZlruCE",
        "colab_type": "text"
      },
      "source": [
        "### `02.` 「パトカー」＋「タクシー」＝「パタトクカシーー」"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjIUikMcfwZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 02. 「パトカー」＋「タクシー」＝「パタトクカシーー」\n",
        "\n",
        "def q2():\n",
        "  s = ['パトカー', 'タクシー']\n",
        "  answer = ''\n",
        "  for i in range(4):\n",
        "    answer += s[0][i] + s[1][i]\n",
        "  print(answer)\n",
        "\n",
        "q2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8Owk_hxr8Ce",
        "colab_type": "text"
      },
      "source": [
        "### `03.` 円周率"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-QEgd_xggS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 03. 円周率\n",
        "import re\n",
        "\n",
        "def q3():\n",
        "  s = 'Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.'\n",
        "  words = [re.sub(r'[^A-Za-z]', '', word) for word in s.split()] # [:-1] ... because any string won't come after sentence terminated\n",
        "  answer = [0 for _ in range(len(words))]\n",
        "  for i in range(len(words)):\n",
        "    answer[i] = len(words[i])\n",
        "  print('words: {0}'.format(words))\n",
        "  print('answer: {0}'.format(answer))\n",
        "\n",
        "q3()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvggIVzxsCT9",
        "colab_type": "text"
      },
      "source": [
        "### `04.` 元素記号"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdKVygBDjeyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 04. 元素記号\n",
        "\n",
        "import re\n",
        "\n",
        "def q4():\n",
        "  s = 'Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.'\n",
        "  words = [re.sub(r'[^A-Za-z]', '', word) for word in s.split()]\n",
        "\n",
        "  answer = {}\n",
        "  extract_onechar_index_list = list(map(lambda x: x - 1, [1, 5, 6, 7, 8, 9, 15, 16, 19]))\n",
        "\n",
        "  for i in range(len(words)):\n",
        "    if i in extract_onechar_index_list:\n",
        "      answer[words[i][:1]] = i + 1\n",
        "    else:\n",
        "      answer[words[i][:2]] = i + 1\n",
        "  \n",
        "  print(answer)\n",
        "\n",
        "q4()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy7yIvUjsLWs",
        "colab_type": "text"
      },
      "source": [
        "### `05.` n-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd013PvFm1vt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 05. n-gram\n",
        "\n",
        "import re\n",
        "\n",
        "def make_word_ngram(sentence, n):\n",
        "  words = [re.sub(r'[^A-Za-z]', '', word) for word in sentence.split()]\n",
        "  return [words[i: i + n] for i in range(len(words) - (n - 1))]\n",
        "\n",
        "def make_character_ngram(sentence, n):\n",
        "  return [sentence[i: i + n] for i in range(len(sentence) - (n - 1))]\n",
        "\n",
        "def q5():\n",
        "  sentence = 'I am an NLPer'\n",
        "  n = 2\n",
        "\n",
        "  # make Word Bi-gram\n",
        "  word_ngram = make_word_ngram(sentence, n)\n",
        "  print('Word N-gram (N = {0}): {1}'.format(n, word_ngram))\n",
        "\n",
        "  # make Character Bi-gram\n",
        "  character_ngram = make_character_ngram(sentence, n)\n",
        "  print('Character N-gram (N = {0}): {1}'.format(n, character_ngram))\n",
        "  \n",
        "q5()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBx3N2Wc0og-",
        "colab_type": "text"
      },
      "source": [
        "### `06.` 集合"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoZjyoD44zyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 06. 集合\n",
        "\n",
        "def make_character_ngram(sentence, n):\n",
        "  return [sentence[i: i + n] for i in range(len(sentence) - (n - 1))]\n",
        "\n",
        "def q6():\n",
        "  X = set(make_character_ngram('paraparaparadise', 2))\n",
        "  Y = set(make_character_ngram('paragraph', 2))\n",
        "\n",
        "  print('X => {0}'.format(list(X)))\n",
        "  print('Y => {0}'.format(list(Y)))\n",
        "  print('Union => {0}'.format(X | Y))\n",
        "  print('Intersection => {0}'.format(X & Y))\n",
        "  print('Does set X contains \\'se\\'? => {0}'.format({'se'} <= X))\n",
        "  print('Does set Y contains \\'se\\'? => {0}'.format({'se'} <= Y))\n",
        "\n",
        "q6()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgDAmRju0uuJ",
        "colab_type": "text"
      },
      "source": [
        "### `07.` テンプレートによる文生成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAmcRFz790tT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 07. テンプレートによる文生成\n",
        "\n",
        "def q7(x, y, z):\n",
        "  print('{0}時の{1}は{2}'.format(x, y, z))\n",
        "\n",
        "q7(x=12, y='気温', z=22.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVmSJkvs02PG",
        "colab_type": "text"
      },
      "source": [
        "### `08.` 暗号文"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIwif5yx_R6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 08. 暗号文\n",
        "\n",
        "def cipher(plaintext):\n",
        "  ciphertext = ''\n",
        "  for p in plaintext:\n",
        "    if ord('a') <= ord(p) <= ord('z'):\n",
        "      ciphertext += chr(219 - ord(p))\n",
        "    else:\n",
        "      ciphertext += p\n",
        "  return ciphertext\n",
        "\n",
        "def q8():\n",
        "  plaintext = 'Now Let\\'s Start The Game!'\n",
        "  ciphertext = cipher(plaintext)\n",
        "  print('plaintext: \\'{0}\\''.format(plaintext))\n",
        "  print('Encryption: \\'{0}\\' => \\'{1}\\''.format(plaintext, ciphertext))\n",
        "  print('Decryption: \\'{0}\\' => \\'{1}\\''.format(ciphertext, cipher(ciphertext)))\n",
        "\n",
        "q8()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CysSZYux0_y2",
        "colab_type": "text"
      },
      "source": [
        "### `09.` Typoglycemia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJhAbPzYCLea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 09. Typoglycemia\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "def q9():\n",
        "  original_sentence = 'I couldn\\'t believe that I could actually understand what I was reading : the phenomenal power of the human mind .'\n",
        "  \n",
        "  modified_words = []\n",
        "  for original_word in original_sentence.split():\n",
        "    if len(original_word) > 4:\n",
        "      middle_part = list(original_word[1:-1])\n",
        "      random.shuffle(middle_part)\n",
        "      modified_words.append(original_word[0] + ''.join(middle_part) + original_word[-1])\n",
        "    else:\n",
        "      modified_words.append(original_word)\n",
        "  modified_sentence = ' '.join(modified_words)\n",
        "\n",
        "  print('--- Original Sentence ---')\n",
        "  print(original_sentence)\n",
        "  print('--- Typoglycemia ---')\n",
        "  print(modified_sentence)\n",
        "\n",
        "\n",
        "q9()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaM_FfFQNj-x",
        "colab_type": "text"
      },
      "source": [
        "## 第2章: UNIXコマンド"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKdKWUE7oj5w",
        "colab_type": "text"
      },
      "source": [
        "### 事前準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z606jr5Ioq5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 第2章\n",
        "!wget -P /content/2/ https://nlp100.github.io/data/popular-names.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1NLEHne1HQO",
        "colab_type": "text"
      },
      "source": [
        "### `10.` 行数のカウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCtlOO2vNqI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 10. 行数のカウント\n",
        "\n",
        "### python part\n",
        "def q10():\n",
        "  f = open('/content/2/popular-names.txt', 'r')\n",
        "  lines = f.readlines()\n",
        "  print(len(lines))\n",
        "\n",
        "q10()\n",
        "\n",
        "### commandline part\n",
        "!wc -l /content/2/popular-names.txt | awk '{print $1}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niZ9uqt81vBZ",
        "colab_type": "text"
      },
      "source": [
        "### `11.` タブをスペースに置換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCOyZXcRdVO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 11. タブをスペースに置換\n",
        "\n",
        "### python part\n",
        "!echo 'python part executing...'\n",
        "\n",
        "def q11():\n",
        "  f = open('/content/2/popular-names.txt', 'r')\n",
        "  g = open('/content/2/11-python.txt', 'w')\n",
        "\n",
        "  content = f.read()\n",
        "  content_tab_to_space = content.replace('\\t', ' ')\n",
        "  g.write(content_tab_to_space)\n",
        "  \n",
        "  f.close()\n",
        "  g.close()\n",
        "  \n",
        "q11()\n",
        "\n",
        "### command-line part\n",
        "!echo 'command-line part executing...'\n",
        "\n",
        "# using sed command\n",
        "!sed -e 's/\\t/ /g' /content/2/popular-names.txt > /content/2/11-command.txt\n",
        "\n",
        "# using tr command\n",
        "#!cat /content/2/popular-names.txt | tr '\\t' ' ' > /content/2/11-command.txt\n",
        "\n",
        "# using expand command\n",
        "#!expand -t 1 /content/2/popular-names.txt > /content/2/11-command.txt\n",
        "\n",
        "!diff --report-identical-files /content/2/11-python.txt /content/2/11-command.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ncua3Tu107k",
        "colab_type": "text"
      },
      "source": [
        "### `12.` 1列目をcol1.txtに，2列目をcol2.txtに保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa8BROrvfwe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 12. 1列目をcol1.txtに，2列目をcol2.txtに保存\n",
        "\n",
        "### python part\n",
        "!echo 'python part executing...'\n",
        "\n",
        "def q12():\n",
        "  f = open('/content/2/popular-names.txt', 'r')\n",
        "  g1 = open('/content/2/col1-python.txt', 'w')\n",
        "  g2 = open('/content/2/col2-python.txt', 'w')\n",
        "  lines = f.readlines()\n",
        "\n",
        "  for line in lines:\n",
        "    line_split = line.split()\n",
        "    g1.write(line_split[0] + '\\n')\n",
        "    g2.write(line_split[1] + '\\n')\n",
        "  \n",
        "  f.close()\n",
        "  g1.close()\n",
        "  g2.close()\n",
        "\n",
        "q12()\n",
        "\n",
        "### command-line part\n",
        "!echo 'command-line part executing...'\n",
        "\n",
        "!cut -f 1 /content/2/popular-names.txt > /content/2/col1-command.txt\n",
        "!cut -f 2 /content/2/popular-names.txt > /content/2/col2-command.txt\n",
        "\n",
        "!diff --report-identical-files /content/2/col1-python.txt /content/2/col1-command.txt\n",
        "!diff --report-identical-files /content/2/col2-python.txt /content/2/col2-command.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBler02Q15Lp",
        "colab_type": "text"
      },
      "source": [
        "### `13.` col1.txtとcol2.txtをマージ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VKaoqeQm3Jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 13. col1.txtとcol2.txtをマージ\n",
        "\n",
        "### python part\n",
        "!echo 'python part executing...'\n",
        "\n",
        "def q13():\n",
        "  f1 = open('/content/2/col1-python.txt', 'r')\n",
        "  f2 = open('/content/2/col2-python.txt', 'r')\n",
        "  g = open('/content/2/13-python.txt', 'w')\n",
        "\n",
        "  f1_line_split = f1.read().split()\n",
        "  f2_line_split = f2.read().split()\n",
        "\n",
        "  for i in range(len(f1_line_split)):\n",
        "    g.write(f1_line_split[i] + '\\t' + f2_line_split[i] + '\\n')\n",
        "  \n",
        "  f1.close()\n",
        "  f2.close()\n",
        "  g.close()\n",
        "  \n",
        "q13()\n",
        "\n",
        "### command-line part\n",
        "!echo 'command-line part executing...'\n",
        "\n",
        "!paste -d '\\t' /content/2/col1.txt /content/2/col2.txt > /content/2/13-command.txt\n",
        "\n",
        "!diff --report-identical-files /content/2/13-python.txt /content/2/13-command.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gElS0g6b1__r",
        "colab_type": "text"
      },
      "source": [
        "### `14.` 先頭からN行を出力"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmYkiYKInYp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 14. 先頭からN行を出力\n",
        "\n",
        "### python part\n",
        "!echo 'python part executing...'\n",
        "\n",
        "def q14():\n",
        "  f = open('/content/2/popular-names.txt', 'r')\n",
        "  g = open('/content/2/14-python.txt', 'w')\n",
        "  lines = f.readlines()\n",
        "\n",
        "  N = int(input('N => '))\n",
        "  for i in range(N):\n",
        "    g.write(lines[i])\n",
        "\n",
        "  f.close()\n",
        "  g.close()\n",
        "\n",
        "q14()\n",
        "\n",
        "### command-line part\n",
        "!echo 'command-line part executing...'\n",
        "\n",
        "!echo -n 'N => '; read N; head -n $N /content/2/popular-names.txt > /content/2/14-command.txt\n",
        "\n",
        "!diff --report-identical-files /content/2/14-python.txt /content/2/14-command.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An-tZV-V2oqb",
        "colab_type": "text"
      },
      "source": [
        "### `15.` 末尾のN行を出力"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWRccBd_qyES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 15. 末尾のN行を出力\n",
        "\n",
        "### python part\n",
        "!echo 'python part executing...'\n",
        "\n",
        "def q15():\n",
        "  f = open('/content/2/popular-names.txt', 'r')\n",
        "  g = open('/content/2/15-python.txt', 'w')\n",
        "  lines = f.readlines()\n",
        "\n",
        "  N = int(input('N => '))\n",
        "  for i in range(N):\n",
        "    g.write(lines[-N+i])\n",
        "\n",
        "  f.close()\n",
        "  g.close()\n",
        "\n",
        "q15()\n",
        "\n",
        "### command-line part\n",
        "!echo 'command-line part executing...'\n",
        "\n",
        "!echo -n 'N => '; read N; tail -n $N /content/2/popular-names.txt > /content/2/15-command.txt\n",
        "\n",
        "!diff --report-identical-files /content/2/15-python.txt /content/2/15-command.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omnK9rNm2uCU",
        "colab_type": "text"
      },
      "source": [
        "### `16.` ファイルをN分割する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbvyPY9zrGyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 16. ファイルをN分割する\n",
        "\n",
        "### python part\n",
        "!echo 'python part executing...'\n",
        "\n",
        "def q16():\n",
        "  f = open('/content/2/popular-names.txt', 'r')\n",
        "  lines = f.readlines()\n",
        "\n",
        "  N = int(input('N => '))\n",
        "  g = [open('/content/2/16-python-%02d.txt' % i, 'w') for i in range(N)]\n",
        "  number_of_lines_per_a_file = len(lines) // N\n",
        "  \n",
        "  index = 0\n",
        "  for i in range(N):\n",
        "    for j in range(number_of_lines_per_a_file):\n",
        "      g[i].write(lines[index])\n",
        "      index += 1\n",
        "  \n",
        "  while index < len(lines):\n",
        "    g[-1].write(lines[index])\n",
        "    index += 1\n",
        "  \n",
        "  f.close()\n",
        "  for i in range(N):\n",
        "    g[i].close()\n",
        "\n",
        "q16()\n",
        "\n",
        "### command-line part\n",
        "!echo 'command-line part executing...'\n",
        "\n",
        "!echo -n 'N => ';read N; split -n $N -d --additional-suffix=.txt /content/2/popular-names.txt /content/2/16-command-\n",
        "\n",
        "!for f in /content/2/16-python-*.txt; do wc -l $f; done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rvampMe26HP",
        "colab_type": "text"
      },
      "source": [
        "### `17.` １列目の文字列の異なり"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcP_h5qls4Pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 17. １列目の文字列の異なり\n",
        "\n",
        "### python part\n",
        "!echo 'python part executing...'\n",
        "\n",
        "def q17():\n",
        "  f = open('/content/2/popular-names.txt', 'r')\n",
        "  g = open('/content/2/17-python.txt', 'w')\n",
        "\n",
        "  lines = f.readlines()\n",
        "  col1 = [lines[i].split()[0] for i in range(len(lines))]\n",
        "  col1_set_sorted = sorted(list(set(col1)))\n",
        "  \n",
        "  for i in range(len(col1_set_sorted)):\n",
        "    g.write(col1_set_sorted[i] + '\\n')\n",
        "  \n",
        "  f.close()\n",
        "  g.close()\n",
        "\n",
        "q17()\n",
        "\n",
        "### command-line part\n",
        "!echo 'command-line part executing...'\n",
        "\n",
        "!cut -f 1 /content/2/popular-names.txt | sort | uniq > /content/2/17-command.txt\n",
        "\n",
        "!diff --report-identical-files /content/2/17-python.txt /content/2/17-command.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk975CVE2-1W",
        "colab_type": "text"
      },
      "source": [
        "### `18.` 各行を3コラム目の数値の降順にソート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59F-525MuOVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 18. 各行を3コラム目の数値の降順にソート\n",
        "\n",
        "### python part\n",
        "!echo 'python part executing...'\n",
        "\n",
        "def q18():\n",
        "  f = open('/content/2/popular-names.txt', 'r')\n",
        "  g = open('/content/2/18-python.txt', 'w')\n",
        "\n",
        "  lines = f.read().split('\\n')[:-1]\n",
        "  lines_and_values = []\n",
        "  for i in range(len(lines)):\n",
        "    lines_and_values.append([lines[i], int(lines[i].split()[2])])\n",
        "  \n",
        "  lines_and_values.sort(key=lambda x:x[1])\n",
        "  lines_and_values.reverse()\n",
        "  \n",
        "  for i in range(len(lines_and_values)):\n",
        "    g.write(lines_and_values[i][0] + '\\n')\n",
        "  \n",
        "  f.close()\n",
        "  g.close()\n",
        "\n",
        "q18()\n",
        "\n",
        "### python part\n",
        "!echo 'command-line part executing...'\n",
        "\n",
        "!sort -k3,3nr -k1,1r /content/2/popular-names.txt > /content/2/18-command.txt\n",
        "\n",
        "# The two files are not identical where values are same but names are different\n",
        "#!diff --report-identical-files /content/2/18-python.txt /content/2/18-command.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7tX_Dn23Ob2",
        "colab_type": "text"
      },
      "source": [
        "### `19.` 各行の1コラム目の文字列の出現頻度を求め，出現頻度の高い順に並べる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXyD10xAv5b8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 19. 各行の1コラム目の文字列の出現頻度を求め，出現頻度の高い順に並べる\n",
        "\n",
        "### python part\n",
        "!echo 'python part executing...'\n",
        "\n",
        "def q19():\n",
        "  f = open('/content/2/popular-names.txt', 'r')\n",
        "  g = open('/content/2/19-python.txt', 'w')\n",
        "\n",
        "  lines = f.read().split('\\n')[:-1]\n",
        "  col1 = [lines[i].split()[0] for i in range(len(lines))]\n",
        "\n",
        "  col1_counter = {}\n",
        "  for value in col1:\n",
        "    if value in col1_counter.keys():\n",
        "      col1_counter[value] += 1\n",
        "    else:\n",
        "      col1_counter[value] = 1\n",
        "  \n",
        "  col1_freq_list = []\n",
        "  for value, freq in col1_counter.items():\n",
        "    col1_freq_list.append([freq, value])\n",
        "  \n",
        "  col1_freq_sorted = sorted(col1_freq_list, key=lambda x:x[0], reverse=True)\n",
        "  for col1_freq_data in col1_freq_sorted:\n",
        "    g.write(('%7d' % col1_freq_data[0]) + '\\t' + col1_freq_data[1] + '\\n')\n",
        "  \n",
        "  f.close()\n",
        "  g.close()\n",
        "\n",
        "q19()\n",
        "\n",
        "### python part\n",
        "!echo 'python part executing...'\n",
        "\n",
        "!cut -f 1 /content/2/popular-names.txt | sort | uniq -c | sort -r -n -k 1 > /content/2/19-command.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk5Bz-Xazj7X",
        "colab_type": "text"
      },
      "source": [
        "## 第3章: 正規表現"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzWlBK5ooyUK",
        "colab_type": "text"
      },
      "source": [
        "### 事前準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arJVQ04ho26z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 第3章\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "\n",
        "!wget -P /content/3/ https://nlp100.github.io/data/jawiki-country.json.gz\n",
        "!gunzip /content/3/jawiki-country.json.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QghWdi_3VXz",
        "colab_type": "text"
      },
      "source": [
        "### `20.` JSONデータの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAv_unkYznoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 20. JSONデータの読み込み\n",
        "\n",
        "def get_UKdata():\n",
        "  f = open('/content/3/jawiki-country.json', 'r')\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    json_data = json.loads(line)\n",
        "    if json_data['title'] == 'イギリス':\n",
        "      f.close()\n",
        "      return json_data['text']\n",
        "\n",
        "def q20():\n",
        "  data = get_UKdata()\n",
        "  print(data)\n",
        "\n",
        "q20()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8OsGJ3V3mPG",
        "colab_type": "text"
      },
      "source": [
        "### `21.` カテゴリ名を含む行を抽出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6gqsx3f6tYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 21. カテゴリ名を含む行を抽出\n",
        "def q21():\n",
        "  # イギリスに関する記事データ全部分を抽出\n",
        "  data = get_UKdata()\n",
        "  # '[[Category:'と']]'で囲まれている部分を抽出する\n",
        "  categories_plain = re.findall(r'\\[\\[Category:.+\\]\\]', data)\n",
        "  \n",
        "  for category_plain in categories_plain:\n",
        "    print(category_plain)\n",
        "  \n",
        "q21()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6b4RWtm3qSP",
        "colab_type": "text"
      },
      "source": [
        "### `22.` カテゴリ名の抽出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrLyvbtcyoOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 22. カテゴリ名の抽出\n",
        "\n",
        "def q22():\n",
        "  # イギリスに関する記事データ全部分を抽出\n",
        "  data = get_UKdata()\n",
        "  # '[[Category:'から始まり、'|'または']]'が現れるまでの部分を抽出する\n",
        "  categories_plain = re.findall(r'\\[\\[Category:(.+?)(?:\\||\\]\\])', data)\n",
        "  # '[[Category:'を消去する\n",
        "  categories = [category_plain.replace('[[Category:', '') for category_plain in categories_plain]\n",
        "  \n",
        "  for category in categories:\n",
        "    print(category)\n",
        "\n",
        "q22()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBNoBZAi3uYX",
        "colab_type": "text"
      },
      "source": [
        "### `23.` セクション構造"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riVOcAKb0ckm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 23. セクション構造\n",
        "\n",
        "def q23():\n",
        "  # イギリスに関する記事データ全部分を抽出\n",
        "  data = get_UKdata()\n",
        "  # 2以上の長さの'='で囲まれている部分を抽出\n",
        "  sections_plain = re.findall(r'={2,}.+={2,}', data)\n",
        "  \n",
        "  for section_plain in sections_plain:\n",
        "    section_name = section_plain.replace('=', '')\n",
        "    section_level = section_plain.count('=') // 2\n",
        "    print('セクション名: {}, レベル: {}'.format(section_name, section_level))\n",
        "\n",
        "q23()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjb34uFa3xsa",
        "colab_type": "text"
      },
      "source": [
        "### `24.` ファイル参照の抽出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqkyqQmwfixF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 24. ファイル参照の抽出\n",
        "\n",
        "def q24():\n",
        "  # イギリスに関する記事データ全部分を抽出\n",
        "  data = get_UKdata()\n",
        "  # '[[ファイル:'から始まり、'|'または']]'が現れるまでの部分を抽出する\n",
        "  mediafiles = re.findall('\\[\\[ファイル:(.+?)(?:\\||\\]\\])', data)\n",
        "\n",
        "  for mediafile in mediafiles:\n",
        "    print(mediafile)\n",
        "\n",
        "q24()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9cnfVeG35_z",
        "colab_type": "text"
      },
      "source": [
        "### `25.` テンプレートの抽出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2wqkugVhH1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 25. テンプレートの抽出\n",
        "\n",
        "def q25():\n",
        "  # 改行も含め、'{{基礎情報'から始まり'}}\\n\\n'で終わる部分を抽出するための正規表現\n",
        "  pattern_basic_info = re.compile(r'\\{\\{基礎情報([\\s\\S]+?)\\}\\}\\n\\n')\n",
        "  # フィールド名と値の組を抽出するための正規表現\n",
        "  pattern_field_and_value = re.compile(r'\\|(.+?)\\s*=\\s*([\\s\\S]+?)(?=\\n\\||\\n\\}\\}\\n\\n)')\n",
        "\n",
        "  # イギリスに関する記事データ全部分を抽出する\n",
        "  data = get_UKdata()\n",
        "  # 改行も含め、'{{基礎情報'から始まり'}}\\n\\n'で終わる部分を抽出する\n",
        "  basic_info = pattern_basic_info.search(data).group()\n",
        "  # フィールド名と値の組を抽出する\n",
        "  field_name_value_pairs = pattern_field_and_value.findall(basic_info)\n",
        "\n",
        "  # フィールド名と値の辞書を作成\n",
        "  field_name_value_dict = {}\n",
        "  for field_name, field_value in field_name_value_pairs:\n",
        "    field_name_value_dict[field_name] = field_value\n",
        "\n",
        "  # フィールド名と値の辞書の内容を確認\n",
        "  for field_name, field_value in field_name_value_dict.items():\n",
        "    print('-' * 50)\n",
        "    print('[フィールド名]\\n{}'.format(field_name))\n",
        "    print('[値]\\n{}'.format(field_value))\n",
        "  \n",
        "q25()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajk5PZ6A4EMR",
        "colab_type": "text"
      },
      "source": [
        "### `26.` 強調マークアップの除去"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAGvYjvJuxFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 26. 強調マークアップの除去\n",
        "\n",
        "def q26():\n",
        "  # 改行も含め、'{{基礎情報'から始まり'}}\\n\\n'で終わる部分を抽出するための正規表現\n",
        "  pattern_basic_info = re.compile(r'\\{\\{基礎情報([\\s\\S]+?)\\}\\}\\n\\n')\n",
        "  # 強調マークアップを除去する('', ''', '''''を除去する)ための正規表現\n",
        "  pattern_sub_emphasize = re.compile(r'(\\'{2,5})(.+?)\\1')\n",
        "  # フィールド名と値の組を抽出するための正規表現\n",
        "  pattern_field_and_value = re.compile(r'\\|(.+?)\\s*=\\s*([\\s\\S]+?)(?=\\n\\||\\n\\}\\}\\n\\n)')\n",
        "\n",
        "\n",
        "  # イギリスに関する記事データ全部分を抽出する\n",
        "  data = get_UKdata()\n",
        "  # 改行も含め、'{{基礎情報'から始まり'}}\\n\\n'で終わる部分を抽出する\n",
        "  basic_info = pattern_basic_info.search(data).group()\n",
        "  # 強調マークアップを除去する('', ''', '''''を除去する)\n",
        "  basic_info = pattern_sub_emphasize.sub(r'\\2', basic_info)\n",
        "  # フィールド名と値の組を抽出する\n",
        "  field_name_value_pairs = pattern_field_and_value.findall(basic_info)\n",
        "  \n",
        "  # フィールド名と値の辞書を作成\n",
        "  field_name_value_dict = {}\n",
        "  for field_name, field_value in field_name_value_pairs:\n",
        "    field_name_value_dict[field_name] = field_value\n",
        "\n",
        "  # フィールド名と値の辞書の内容を確認\n",
        "  for field_name, field_value in field_name_value_dict.items():\n",
        "    print('-' * 50)\n",
        "    print('[フィールド名]\\n{}'.format(field_name))\n",
        "    print('[値]\\n{}'.format(field_value))\n",
        "  \n",
        "q26()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVnfc6ju4HF-",
        "colab_type": "text"
      },
      "source": [
        "### `27.` 内部リンクの除去"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kMHXmev7FGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 27. 内部リンクの除去\n",
        "\n",
        "def q27():\n",
        "  # 改行も含め、'{{基礎情報'から始まり'}}\\n\\n'で終わる部分を抽出するための正規表現\n",
        "  pattern_basic_info = re.compile(r'\\{\\{基礎情報([\\s\\S]+?)\\}\\}\\n\\n')\n",
        "  # 強調マークアップを除去する('', ''', '''''を除去する)ための正規表現\n",
        "  pattern_sub_emphasize = re.compile(r'(\\'{2,5})(.+?)\\1')\n",
        "  # 内部リンクのマークアップを除去するための正規表現\n",
        "  pattern_sub_internal_link = re.compile(r'\\[\\[([\\s\\S]+?)(?:\\|[\\s\\S]+?)*\\]\\]')\n",
        "  # フィールド名と値の組を抽出するための正規表現\n",
        "  pattern_field_and_value = re.compile(r'\\|(.+?)\\s*=\\s*([\\s\\S]+?)(?=\\n\\||\\n\\}\\}\\n\\n)')\n",
        "\n",
        "\n",
        "  # イギリスに関する記事データ全部分を抽出する\n",
        "  data = get_UKdata()\n",
        "  # 改行も含め、'{{基礎情報'から始まり'}}\\n\\n'で終わる部分を抽出する\n",
        "  basic_info = pattern_basic_info.search(data).group()\n",
        "  # 強調マークアップを除去する('', ''', '''''を除去する)\n",
        "  basic_info = pattern_sub_emphasize.sub(r'\\2', basic_info)\n",
        "  # 内部リンクのマークアップを除去する\n",
        "  basic_info = pattern_sub_internal_link.sub(r'\\1', basic_info)\n",
        "  # フィールド名と値の組を抽出する\n",
        "  field_name_value_pairs = pattern_field_and_value.findall(basic_info)\n",
        "  \n",
        "  # フィールド名と値の辞書を作成\n",
        "  field_name_value_dict = {}\n",
        "  for field_name, field_value in field_name_value_pairs:\n",
        "    field_name_value_dict[field_name] = field_value\n",
        "\n",
        "  # フィールド名と値の辞書の内容を確認\n",
        "  for field_name, field_value in field_name_value_dict.items():\n",
        "    print('-' * 50)\n",
        "    print('[フィールド名]\\n{}'.format(field_name))\n",
        "    print('[値]\\n{}'.format(field_value))\n",
        "  \n",
        "q27()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzoT5gHL4KAq",
        "colab_type": "text"
      },
      "source": [
        "### `28.` MediaWikiマークアップの除去"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxqiLGSmFn1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 28. MediaWikiマークアップの除去\n",
        "\n",
        "def q28():\n",
        "  # 改行も含め、'{{基礎情報'から始まり'}}\\n\\n'で終わる部分を抽出するための正規表現\n",
        "  pattern_basic_info = re.compile(r'\\{\\{基礎情報([\\s\\S]+?)\\}\\}\\n\\n')\n",
        "  # 強調マークアップを除去する('', ''', '''''を除去する)ための正規表現\n",
        "  pattern_sub_emphasize = re.compile(r'(\\'{2,5})(.+?)\\1')\n",
        "  # ファイルリンクのマークアップを改修するための正規表現\n",
        "  pattern_sub_file = re.compile(r'\\[\\[ファイル:([\\s\\S]+?)(?:\\|[\\s\\S]+?)*\\]\\]')\n",
        "  # 内部リンクのマークアップを除去するための正規表現\n",
        "  pattern_sub_internal_link = re.compile(r'\\[\\[([\\s\\S]+?)(?:\\|[\\s\\S]+?)*\\]\\]')\n",
        "  # refタグやbrタグを消去するための正規表現\n",
        "  pattern_sub_tag = re.compile(r'(?:<ref>)?(?:</ref>)?(?:<br />)?(?:<ref[\\s\\S]+>?)?')\n",
        "  # {{lang|...}}系を改修するための正規表現\n",
        "  pattern_sub_lang = re.compile(r'\\{\\{lang\\|(?:[\\s\\S]+?)([^\\|]+)\\}\\}')\n",
        "  # 仮リンクを抽出するための正規表現\n",
        "  pattern_sub_tmp_link = re.compile(r'\\{\\{仮リンク\\|([\\s\\S]+?)(?:\\|[\\s\\S]+?)\\}\\}')\n",
        "  # その他{{と}}で囲まれた部分をスペースに変換するための正規表現\n",
        "  pattern_sub_curly_brackets = re.compile(r'\\{\\{(.+?)\\}\\}')\n",
        "  # :en: を削除するための正規表現\n",
        "  pattern_sub_en = re.compile(r'\\:en\\:')\n",
        "  # フィールド名と値の組を抽出するための正規表現\n",
        "  pattern_field_and_value = re.compile(r'\\|(.+?)\\s*=\\s*([\\s\\S]+?)(?=\\n\\||\\n\\}\\}\\n\\n)')\n",
        "\n",
        "  # イギリスに関する記事データ全部分を抽出する\n",
        "  data = get_UKdata()\n",
        "  # 改行も含め、'{{基礎情報'から始まり'}}\\n\\n'で終わる部分を抽出する\n",
        "  basic_info = pattern_basic_info.search(data).group()\n",
        "  # 強調マークアップを除去する('', ''', '''''を除去する)\n",
        "  basic_info = pattern_sub_emphasize.sub(r'\\2', basic_info)\n",
        "  # ファイルリンクのマークアップを改修する\n",
        "  basic_info = pattern_sub_file.sub(r'\\1', basic_info)\n",
        "  # 内部リンクのマークアップを除去する\n",
        "  basic_info = pattern_sub_internal_link.sub(r'\\1', basic_info)\n",
        "  # refタグやbrタグを消去する\n",
        "  basic_info = pattern_sub_tag.sub(r'', basic_info)\n",
        "  # {{lang|...}}系を改修する\n",
        "  basic_info = pattern_sub_lang.sub(r'\\1', basic_info)\n",
        "  # 仮リンクを抽出する\n",
        "  basic_info = pattern_sub_tmp_link.sub(r'\\1', basic_info)\n",
        "  # その他{{と}}で囲まれた部分をスペースに変換する\n",
        "  basic_info = pattern_sub_curly_brackets.sub(r' ', basic_info)\n",
        "  # :en: を削除する\n",
        "  basic_info = pattern_sub_en.sub(r'', basic_info)\n",
        "  # フィールド名と値の組を抽出する\n",
        "  field_name_value_pairs = pattern_field_and_value.findall(basic_info)\n",
        "  \n",
        "  # フィールド名と値の辞書を作成\n",
        "  field_name_value_dict = {}\n",
        "  for field_name, field_value in field_name_value_pairs:\n",
        "    field_name_value_dict[field_name] = field_value\n",
        "\n",
        "  # フィールド名と値の辞書の内容を確認\n",
        "  for field_name, field_value in field_name_value_dict.items():\n",
        "    print('-' * 50)\n",
        "    print('[フィールド名]\\n{}'.format(field_name))\n",
        "    print('[値]\\n{}'.format(field_value))\n",
        "  \n",
        "q28()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8Mw_EPe4M8p",
        "colab_type": "text"
      },
      "source": [
        "### `29.` 国旗画像のURLを取得する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV_5xJVXUm6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 29. 国旗画像のURLを取得する\n",
        "\n",
        "def q29():\n",
        "  # 改行も含め、'{{基礎情報'から始まり'}}\\n\\n'で終わる部分を抽出するための正規表現\n",
        "  pattern_basic_info = re.compile(r'\\{\\{基礎情報([\\s\\S]+?)\\}\\}\\n\\n')\n",
        "  # 強調マークアップを除去する('', ''', '''''を除去する)ための正規表現\n",
        "  pattern_sub_emphasize = re.compile(r'(\\'{2,5})(.+?)\\1')\n",
        "  # ファイルリンクのマークアップを改修するための正規表現\n",
        "  pattern_sub_file = re.compile(r'\\[\\[ファイル:([\\s\\S]+?)(?:\\|[\\s\\S]+?)*\\]\\]')\n",
        "  # 内部リンクのマークアップを除去するための正規表現\n",
        "  pattern_sub_internal_link = re.compile(r'\\[\\[([\\s\\S]+?)(?:\\|[\\s\\S]+?)*\\]\\]')\n",
        "  # refタグやbrタグを消去するための正規表現\n",
        "  pattern_sub_tag = re.compile(r'(?:<ref>)?(?:</ref>)?(?:<br />)?(?:<ref[\\s\\S]+>?)?')\n",
        "  # {{lang|...}}系を改修するための正規表現\n",
        "  pattern_sub_lang = re.compile(r'\\{\\{lang\\|(?:[\\s\\S]+?)([^\\|]+)\\}\\}')\n",
        "  # 仮リンクを抽出するための正規表現\n",
        "  pattern_sub_tmp_link = re.compile(r'\\{\\{仮リンク\\|([\\s\\S]+?)(?:\\|[\\s\\S]+?)\\}\\}')\n",
        "  # その他{{と}}で囲まれた部分をスペースに変換するための正規表現\n",
        "  pattern_sub_curly_brackets = re.compile(r'\\{\\{(.+?)\\}\\}')\n",
        "  # :en: を削除するための正規表現\n",
        "  pattern_sub_en = re.compile(r'\\:en\\:')\n",
        "  # フィールド名と値の組を抽出するための正規表現\n",
        "  pattern_field_and_value = re.compile(r'\\|(.+?)\\s*=\\s*([\\s\\S]+?)(?=\\n\\||\\n\\}\\}\\n\\n)')\n",
        "\n",
        "  # イギリスに関する記事データ全部分を抽出する\n",
        "  data = get_UKdata()\n",
        "  # 改行も含め、'{{基礎情報'から始まり'}}\\n\\n'で終わる部分を抽出する\n",
        "  basic_info = pattern_basic_info.search(data).group()\n",
        "  # 強調マークアップを除去する('', ''', '''''を除去する)\n",
        "  basic_info = pattern_sub_emphasize.sub(r'\\2', basic_info)\n",
        "  # ファイルリンクのマークアップを改修する\n",
        "  basic_info = pattern_sub_file.sub(r'\\1', basic_info)\n",
        "  # 内部リンクのマークアップを除去する\n",
        "  basic_info = pattern_sub_internal_link.sub(r'\\1', basic_info)\n",
        "  # refタグやbrタグを消去する\n",
        "  basic_info = pattern_sub_tag.sub(r'', basic_info)\n",
        "  # {{lang|...}}系を改修する\n",
        "  basic_info = pattern_sub_lang.sub(r'\\1', basic_info)\n",
        "  # 仮リンクを抽出する\n",
        "  basic_info = pattern_sub_tmp_link.sub(r'\\1', basic_info)\n",
        "  # その他{{と}}で囲まれた部分をスペースに変換する\n",
        "  basic_info = pattern_sub_curly_brackets.sub(r' ', basic_info)\n",
        "  # :en: を削除する\n",
        "  basic_info = pattern_sub_en.sub(r'', basic_info)\n",
        "  # フィールド名と値の組を抽出する\n",
        "  field_name_value_pairs = pattern_field_and_value.findall(basic_info)\n",
        "  \n",
        "  # フィールド名と値の辞書を作成\n",
        "  field_name_value_dict = {}\n",
        "  for field_name, field_value in field_name_value_pairs:\n",
        "    field_name_value_dict[field_name] = field_value\n",
        "\n",
        "  # 国旗画像のファイル名\n",
        "  filename = field_name_value_dict['国旗画像']\n",
        "  \n",
        "  # 画像URLを取得する\n",
        "  url = 'https://en.wikipedia.org/w/api.php'\n",
        "  params = {\n",
        "      'action': 'query',\n",
        "      'format': 'json',\n",
        "      'prop': 'imageinfo',\n",
        "      'titles': 'File:{}'.format(filename),\n",
        "      'iiprop': 'url'\n",
        "  }\n",
        "  response = requests.get(url=url, params=params)\n",
        "  if response.status_code != 200:\n",
        "        e = Exception('HTTP status: ' + response.status_code)\n",
        "        raise e\n",
        "  json_data = response.json()\n",
        "  pages = json_data['query']['pages']\n",
        "  target_urls = []\n",
        "  for k, v in pages.items():\n",
        "    for i in range(len(v['imageinfo'])):\n",
        "      target_urls.append(v['imageinfo'][i]['url'])\n",
        "  \n",
        "  print(target_urls)\n",
        "  \n",
        "q29()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZn7qdz5TEs9",
        "colab_type": "text"
      },
      "source": [
        "## 第4章: 形態素解析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6hLz3aqo80n",
        "colab_type": "text"
      },
      "source": [
        "### 事前準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_ZgePhho_pQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 第4章\n",
        "!pip install mecab-python3\n",
        "!pip install japanize_matplotlib\n",
        "!apt install -y mecab\n",
        "!apt install -y libmecab-dev\n",
        "!apt install -y mecab-ipadic-utf8\n",
        "!apt install -y fonts-ipafont-gothic\n",
        "!wget -P /content/4/ https://nlp100.github.io/data/neko.txt\n",
        "\n",
        "import MeCab\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "\n",
        "# 形態素解析して保存\n",
        "f = open('/content/4/neko.txt', 'r')\n",
        "g = open('/content/4/neko.txt.mecab', 'w')\n",
        "text = f.read()\n",
        "\n",
        "tagger = MeCab.Tagger(\"-Ochasen\")\n",
        "node = tagger.parseToNode(text)\n",
        "while node: \n",
        "    surface = node.surface # 文字列の取得\n",
        "    feature = node.feature # 品詞や読み（カンマ区切り）\n",
        "    g.write(surface + '\\t' + feature + '\\n')\n",
        "    node = node.next\n",
        "\n",
        "f.close()\n",
        "g.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxHBPBH14Yhv",
        "colab_type": "text"
      },
      "source": [
        "### `30.` 形態素解析結果の読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlLCHYiTTIkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 30. 形態素解析結果の読み込み\n",
        "def q30():\n",
        "  f = open('/content/4/neko.txt.mecab', 'r')\n",
        "  lines = f.read().split('\\n')[:-1]\n",
        "\n",
        "  parsed_texts = []\n",
        "  parsed_text = []\n",
        "  for line in lines:\n",
        "    surface, feature_part = line.split('\\t')\n",
        "    features = feature_part.split(',')\n",
        "    parsed_text.append({'surface': surface, 'base': features[6], 'pos': features[0], 'pos1': features[1]})\n",
        "    if features[1] == '句点':\n",
        "      parsed_texts.append(parsed_text)\n",
        "      parsed_text = []\n",
        "  return parsed_texts\n",
        "\n",
        "# テスト表示\n",
        "parsed_texts = q30()\n",
        "for parsed_text in parsed_texts[:3]:\n",
        "  print(parsed_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBK7yh-I4bhd",
        "colab_type": "text"
      },
      "source": [
        "### `31.` 動詞"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXxEvZd861Nn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 31. 動詞\n",
        "def q31():\n",
        "  parsed_texts = q30()\n",
        "  verb_and_freq = {}\n",
        "  for parsed_text in parsed_texts:\n",
        "    for morpheme in parsed_text:\n",
        "      if morpheme['pos'] == '動詞':\n",
        "        if morpheme['surface'] not in verb_and_freq.keys():\n",
        "          verb_and_freq[morpheme['surface']] = 1\n",
        "        else:\n",
        "          verb_and_freq[morpheme['surface']] += 1\n",
        "  \n",
        "  verbs_sorted = sorted(verb_and_freq.items(), key=lambda x:x[1], reverse=True)\n",
        "  \n",
        "  print(verbs_sorted[:10])\n",
        "\n",
        "q31()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QMJJ1qp4fb7",
        "colab_type": "text"
      },
      "source": [
        "### `32.` 動詞の原形"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAhlBxWLTaXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 32. 動詞の原形\n",
        "def q32():\n",
        "  parsed_texts = q30()\n",
        "  verb_and_freq = {}\n",
        "  for parsed_text in parsed_texts:\n",
        "    for morpheme in parsed_text:\n",
        "      if morpheme['pos'] == '動詞':\n",
        "        if morpheme['base'] not in verb_and_freq.keys():\n",
        "          verb_and_freq[morpheme['base']] = 1\n",
        "        else:\n",
        "          verb_and_freq[morpheme['base']] += 1\n",
        "  \n",
        "  verbs_sorted = sorted(verb_and_freq.items(), key=lambda x:x[1], reverse=True)\n",
        "  \n",
        "  print(verbs_sorted[:10])\n",
        "\n",
        "q32()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxwcM26Z4mSt",
        "colab_type": "text"
      },
      "source": [
        "### `33.` 「AのB」"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKwVy0gabC8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 33. 「AのB」\n",
        "\n",
        "def q33():\n",
        "  parsed_texts = q30()\n",
        "  noun_phrases = set()\n",
        "  for parsed_text in parsed_texts:\n",
        "    for i in range(1, len(parsed_text) - 1):\n",
        "      if parsed_text[i - 1]['pos'] == '名詞' and parsed_text[i]['surface'] == 'の' and parsed_text[i + 1]['pos'] == '名詞':\n",
        "        noun_phrases.add(''.join([parsed_text[j]['surface'] for j in range(i - 1, i + 2)]))\n",
        "  \n",
        "  print(noun_phrases)\n",
        "\n",
        "q33()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwL-dd-_4unt",
        "colab_type": "text"
      },
      "source": [
        "### `34.` 名詞の連接"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M8u2tJFe4FR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 34. 名詞の連接\n",
        "\n",
        "def q34():\n",
        "  parsed_texts = q30()\n",
        "  noun_sequences = set()\n",
        "  noun_sequence = ''\n",
        "  noun_sequence_length = 0\n",
        "  for parsed_text in parsed_texts:\n",
        "    for morpheme in parsed_text:\n",
        "      if morpheme['pos'] == '名詞':\n",
        "        noun_sequence_length += 1\n",
        "        noun_sequence += morpheme['surface']\n",
        "      else:\n",
        "        if noun_sequence_length >= 2:\n",
        "          noun_sequences.add(noun_sequence)\n",
        "        noun_sequence = ''\n",
        "        noun_sequence_length = 0\n",
        "\n",
        "  print(noun_sequences)\n",
        "\n",
        "q34()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQTv5rrv4xUe",
        "colab_type": "text"
      },
      "source": [
        "### `35.` 単語の出現頻度"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha9OkUzZkN0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 35. 単語の出現頻度\n",
        "def q35():\n",
        "  parsed_texts = q30()\n",
        "  word_and_freq = {}\n",
        "  for parsed_text in parsed_texts:\n",
        "    for morpheme in parsed_text:\n",
        "      if morpheme['surface'] not in word_and_freq.keys():\n",
        "        word_and_freq[morpheme['surface']] = 1\n",
        "      else:\n",
        "        word_and_freq[morpheme['surface']] += 1\n",
        "  \n",
        "  words_sorted = sorted(word_and_freq.items(), key=lambda x:x[1], reverse=True)\n",
        "  \n",
        "  print(words_sorted)\n",
        "\n",
        "q35()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFkfaAup40UH",
        "colab_type": "text"
      },
      "source": [
        "### `36.` 頻度上位10語"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Htnz9-liqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 36. 頻度上位10語\n",
        "def q36():\n",
        "  parsed_texts = q30()\n",
        "  word_and_freq = {}\n",
        "  for parsed_text in parsed_texts:\n",
        "    for morpheme in parsed_text:\n",
        "      if morpheme['surface'] not in word_and_freq.keys():\n",
        "        word_and_freq[morpheme['surface']] = 1\n",
        "      else:\n",
        "        word_and_freq[morpheme['surface']] += 1\n",
        "  \n",
        "  words_sorted = sorted(word_and_freq.items(), key=lambda x:x[1], reverse=True)\n",
        "  print(words_sorted)\n",
        "  \n",
        "  plt.bar([words_sorted[i][0] for i in range(10)],\n",
        "          [words_sorted[i][1] for i in range(10)],\n",
        "          align='center',\n",
        "          )\n",
        "  plt.title(\"出現頻度の上位10位までの単語と頻度\", fontsize=20)\n",
        "  plt.xlabel(\"単語\", fontsize=20)\n",
        "  plt.ylabel(\"頻度\", fontsize=20)\n",
        "  plt.tick_params(labelsize=16)\n",
        "\n",
        "q36()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dSrjWiD44x-",
        "colab_type": "text"
      },
      "source": [
        "### `37.` 「猫」と共起頻度の高い上位10語"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S3VFUvK9zNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 37. 「猫」と共起頻度の高い上位10語\n",
        "# 助詞や接続詞ばかりでつまらないので名詞のみに制限\n",
        "def q37():\n",
        "  parsed_texts = q30()\n",
        "  word_and_freq = {}\n",
        "  for parsed_text in parsed_texts:\n",
        "    is_cat_appeared = False\n",
        "    for morpheme in parsed_text:\n",
        "      if morpheme['surface'] == '猫':\n",
        "        is_cat_appeared = True\n",
        "        break\n",
        "    \n",
        "    if not is_cat_appeared:\n",
        "      continue\n",
        "\n",
        "    for morpheme in parsed_text:\n",
        "      if morpheme['pos'] in ['名詞'] and morpheme['surface'] != '猫':\n",
        "        if morpheme['surface'] not in word_and_freq.keys():\n",
        "          word_and_freq[morpheme['surface']] = 1\n",
        "        else:\n",
        "          word_and_freq[morpheme['surface']] += 1\n",
        "  \n",
        "  words_sorted = sorted(word_and_freq.items(), key=lambda x:x[1], reverse=True)\n",
        "  print(words_sorted)\n",
        "  \n",
        "  plt.bar([words_sorted[i][0] for i in range(10)],\n",
        "          [words_sorted[i][1] for i in range(10)],\n",
        "          align='center',\n",
        "          )\n",
        "  plt.title(\"「猫」と共起頻度の高い上位10名詞と頻度\", fontsize=18)\n",
        "  plt.xlabel(\"名詞\", fontsize=16)\n",
        "  plt.ylabel(\"頻度\", fontsize=16)\n",
        "  plt.tick_params(labelsize=14)\n",
        "\n",
        "q37()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_4PCZcS48CM",
        "colab_type": "text"
      },
      "source": [
        "### `38.` ヒストグラム"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzgutrOcFT54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 38. ヒストグラム\n",
        "def q38():\n",
        "  parsed_texts = q30()\n",
        "  word_and_freq = {}\n",
        "  for parsed_text in parsed_texts:\n",
        "    for morpheme in parsed_text:\n",
        "      if morpheme['surface'] not in word_and_freq.keys():\n",
        "        word_and_freq[morpheme['surface']] = 1\n",
        "      else:\n",
        "        word_and_freq[morpheme['surface']] += 1\n",
        "  \n",
        "  # リストで出現頻度ごとの単語の種類数を確認\n",
        "  hist_data = {}\n",
        "  for freq in word_and_freq.values():\n",
        "    if freq in hist_data.keys():\n",
        "      hist_data[freq] += 1\n",
        "    else:\n",
        "      hist_data[freq] = 1\n",
        "  hist_data_sorted = sorted(hist_data.items(), key=lambda x:x[0])\n",
        "  print([hist_data_sorted[i][1] for i in range(10)])\n",
        "  \n",
        "  plt.hist(word_and_freq.values(), range=(1, 10), align='left', rwidth=0.8)\n",
        "  plt.hist(word_and_freq.values(), range=(1, 10), align='left', rwidth=0.8)\n",
        "  plt.title(\"単語の出現頻度ごとの種類数\", fontsize=18)\n",
        "  plt.xlabel(\"出現頻度\", fontsize=16)\n",
        "  plt.ylabel(\"単語の種類数\", fontsize=16)\n",
        "  plt.tick_params(labelsize=14)\n",
        "\n",
        "q38()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agx3S8qH5BA4",
        "colab_type": "text"
      },
      "source": [
        "### `39.` Zipfの法則"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEpTMhZfvz-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 39. Zipfの法則\n",
        "def q39():\n",
        "  parsed_texts = q30()\n",
        "  word_and_freq = {}\n",
        "  for parsed_text in parsed_texts:\n",
        "    for morpheme in parsed_text:\n",
        "      if morpheme['surface'] not in word_and_freq.keys():\n",
        "        word_and_freq[morpheme['surface']] = 1\n",
        "      else:\n",
        "        word_and_freq[morpheme['surface']] += 1\n",
        "  \n",
        "  words_sorted = sorted(word_and_freq.items(), key=lambda x:x[1], reverse=True)\n",
        "  plt.scatter(range(1, len(words_sorted) + 1),\n",
        "              [words_sorted[i][1] for i in range(len(words_sorted))],\n",
        "              s=10\n",
        "              )\n",
        "  plt.xlim(1, len(words_sorted))\n",
        "  plt.xscale('log')\n",
        "  plt.yscale('log')\n",
        "  plt.title(\"単語の出現頻度順位と出現頻度の関係\", fontsize=18)\n",
        "  plt.xlabel(\"単語の出現頻度順位\", fontsize=16)\n",
        "  plt.ylabel(\"単語の出現頻度\", fontsize=16)\n",
        "  plt.tick_params(labelsize=14)\n",
        "  plt.grid(which='both')\n",
        "  \n",
        "q39()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhcFZcbG2cpa",
        "colab_type": "text"
      },
      "source": [
        "## 第5章: 係り受け解析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD5zDA8CpNPr",
        "colab_type": "text"
      },
      "source": [
        "### 事前準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p8FJw6dpPCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 第5章\n",
        "\n",
        "# neko.txt download\n",
        "!wget -P /content/5/ https://nlp100.github.io/data/neko.txt\n",
        "\n",
        "# graphviz (command-line) installation\n",
        "!apt install -y graphviz\n",
        "\n",
        "# Japanese font installation\n",
        "!apt install -y fonts-ipafont-gothic\n",
        "\n",
        "# graphviz installation\n",
        "!pip install graphviz\n",
        "\n",
        "# mecab installation\n",
        "!pip install mecab-python3\n",
        "!pip install japanize_matplotlib\n",
        "!apt install -y mecab\n",
        "!apt install -y libmecab-dev\n",
        "!apt install -y mecab-ipadic-utf8\n",
        "\n",
        "# CRF++ installation\n",
        "%cd /content/5/\n",
        "filename_crfpp='crfpp.tar.gz'\n",
        "!wget 'https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7QVR6VXJ5dWExSTQ' -O $filename_crfpp\n",
        "!tar zxvf $filename_crfpp\n",
        "%cd CRF++-0.58\n",
        "!./configure\n",
        "!make\n",
        "!make install\n",
        "\n",
        "# CaboCha (command-line) installation\n",
        "import os\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/local/lib'\n",
        "%cd /content/5/\n",
        "filename_cabocha = 'cabocha.tar.bz2'\n",
        "!wget -O $filename_cabocha --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=0B4y35FiV1wh7SDd1Q1dUQkZQaUU' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=0B4y35FiV1wh7SDd1Q1dUQkZQaUU\" -O $filename_cabocha && rm -rf /tmp/cookies.txt\n",
        "!bzip2 -dc $filename_cabocha | tar xvf -\n",
        "%cd cabocha-0.69\n",
        "!./configure --with-mecab-config=`which mecab-config` --with-charset=UTF8\n",
        "!make\n",
        "!make check\n",
        "!make install\n",
        "%cd ..\n",
        "\n",
        "# CaboCha (python) installation\n",
        "%cd cabocha-0.69/python\n",
        "!python setup.py build_ext\n",
        "!python setup.py install\n",
        "!ldconfig\n",
        "%cd ../..\n",
        "\n",
        "# make neko.txt.cabocha (from command)\n",
        "#!cabocha -f1 < neko.txt > neko.txt.cabocha\n",
        "#!head -n 40 neko.txt.cabocha\n",
        "\n",
        "# make neko.txt.cabocha (from python)\n",
        "import CaboCha\n",
        "c = CaboCha.Parser()\n",
        "\n",
        "f = open('neko.txt', 'r')\n",
        "g = open('neko.txt.cabocha', 'w')\n",
        "\n",
        "lines = f.readlines()\n",
        "for line in lines:\n",
        "  g.write(c.parse(line).toString(CaboCha.FORMAT_LATTICE))\n",
        "\n",
        "f.close()\n",
        "g.close()\n",
        "\n",
        "!head -n 30 neko.txt.cabocha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5czXL_o5JmM",
        "colab_type": "text"
      },
      "source": [
        "### `40.` 係り受け解析結果の読み込み（形態素）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAVZojTHIX0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 40. 係り受け解析結果の読み込み（形態素）\n",
        "\n",
        "class Morph:\n",
        "  '''\n",
        "  self.surface (str): 形態素そのもの\n",
        "  self.base (str): 形態素の原型\n",
        "  self.pos (str): 形態素の品詞\n",
        "  self.pos1 (str): 品詞細分類1\n",
        "  '''\n",
        "  def __init__(self, surface, base, pos, pos1):\n",
        "    self.surface = surface\n",
        "    self.base = base \n",
        "    self.pos = pos\n",
        "    self.pos1 = pos1\n",
        "\n",
        "def q40():\n",
        "  f = open('neko.txt.cabocha', 'r')\n",
        "  lines = f.readlines()\n",
        "  \n",
        "  morph_parsed_texts = []\n",
        "  morph_parsed_text = []\n",
        "  for line in lines:\n",
        "    # '*'から始まる行は係り受け解析の行\n",
        "    if line[0] == '*':\n",
        "      continue\n",
        "    \n",
        "    if line == 'EOS\\n':\n",
        "      morph_parsed_texts.append(morph_parsed_text)\n",
        "      morph_parsed_text = []\n",
        "      continue\n",
        "    \n",
        "    surface, feature_part = line.split('\\t')\n",
        "    features = feature_part.split(',')\n",
        "    morph_parsed_text.append(Morph(surface, features[6], features[0], features[1]))\n",
        "\n",
        "  for morph in morph_parsed_texts[2]:\n",
        "    print('surface: \\'{}\\', base: \\'{}\\', pos1: \\'{}\\', pos2: \\'{}\\''.format(morph.surface, morph.base, morph.pos, morph.pos1))\n",
        "\n",
        "q40()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWcqCsmo5NBI",
        "colab_type": "text"
      },
      "source": [
        "### `41.` 係り受け解析結果の読み込み（文節・係り受け）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewkb0_UNPNEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
        "class Morph:\n",
        "  '''\n",
        "  self.surface (str): 形態素そのもの\n",
        "  self.base (str): 形態素の原型\n",
        "  self.pos (str): 形態素の品詞\n",
        "  self.pos1 (str): 品詞細分類1\n",
        "  '''\n",
        "  def __init__(self, surface, base, pos, pos1):\n",
        "    self.surface = surface\n",
        "    self.base = base\n",
        "    self.pos = pos\n",
        "    self.pos1 = pos1\n",
        "\n",
        "class Chunk:\n",
        "  '''\n",
        "  self.morphs (Morph[]): 文節に含まれる形態素(Morphインスタンス)のリスト\n",
        "  self.dst (int): 係り先文節インデックス番号\n",
        "  self.srcs (int[]): 係り元文節インデックス番号のリスト\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    self.morphs = []\n",
        "    self.dst = 0\n",
        "    self.srcs = []\n",
        "  \n",
        "  def chunk_content(self, without_symbols=False):\n",
        "    chunk_str = ''\n",
        "    for i in range(len(self.morphs)):\n",
        "      if without_symbols and self.morphs[i].pos == '記号':\n",
        "        continue\n",
        "      chunk_str += self.morphs[i].surface\n",
        "    \n",
        "    return chunk_str\n",
        "\n",
        "def q41():\n",
        "  f = open('neko.txt.cabocha', 'r')\n",
        "  lines = f.readlines()\n",
        "  \n",
        "  chunk_index = -1\n",
        "  chunk_src_dict = {}\n",
        "  chunk_parsed_text = []\n",
        "  chunks_in_sentence = []\n",
        "\n",
        "  for line in lines:\n",
        "    # '*'から始まる行は係り受け解析の行\n",
        "    if line[0] == '*':\n",
        "      chunk = Chunk()\n",
        "      chunk_index += 1\n",
        "      chunks_in_sentence.extend([chunk])\n",
        "      chunk_data = line.split()\n",
        "      dst = int(chunk_data[2][:-1])\n",
        "      chunks_in_sentence[chunk_index].dst = dst\n",
        "\n",
        "      # 係り元チャンクの辞書に追加\n",
        "      if dst != -1:\n",
        "        if dst not in chunk_src_dict:\n",
        "          chunk_src_dict[dst] = [chunk_index]\n",
        "        else:\n",
        "          chunk_src_dict[dst].append(chunk_index)\n",
        "      continue\n",
        "\n",
        "    # 文末\n",
        "    if line == 'EOS\\n':\n",
        "\n",
        "      # srcsを追加\n",
        "      for chunk_index, srcs in chunk_src_dict.items():\n",
        "        chunks_in_sentence[chunk_index].srcs = srcs\n",
        "      \n",
        "      chunk_parsed_text.append(chunks_in_sentence)\n",
        "      chunk_index = -1\n",
        "      chunk_src_dict = {}\n",
        "      chunks_in_sentence = []\n",
        "      continue\n",
        "    \n",
        "    # 文節の形態素リストに形態素を追加\n",
        "    surface, feature_part = line.split('\\t')\n",
        "    features = feature_part.split(',')\n",
        "    morph = Morph(surface, features[6], features[0], features[1])\n",
        "    chunks_in_sentence[chunk_index].morphs.extend([morph])\n",
        "  \n",
        "  return chunk_parsed_text\n",
        "  \n",
        "chunk_parsed_text = q41()\n",
        "for chunk_index, chunk in enumerate(chunk_parsed_text[7]):\n",
        "    print('[chunk id: {}]{} : srcs={}, dst={}'.format(chunk_index, chunk.chunk_content(), chunk.srcs, chunk.dst))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cwbx8MN5Quu",
        "colab_type": "text"
      },
      "source": [
        "### `42.` 係り元と係り先の文節の表示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMRkeHDGNizb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 42. 係り元と係り先の文節の表示\n",
        "#「41. 係り受け解析結果の読み込み（文節・係り受け）」のコードをはじめに実行してください。\n",
        "\n",
        "def q42():\n",
        "  chunk_parsed_text = q41()\n",
        "  for chunks_in_sentence in chunk_parsed_text:\n",
        "    print('-' * 50)\n",
        "    for chunk_index in range(len(chunks_in_sentence) - 1):\n",
        "      chunk_src_content = chunks_in_sentence[chunk_index].chunk_content(without_symbols=True)\n",
        "      chunk_dst_content = chunks_in_sentence[chunks_in_sentence[chunk_index].dst].chunk_content(without_symbols=True)\n",
        "      print('{}\\t{}'.format(chunk_src_content, chunk_dst_content))\n",
        "\n",
        "q42()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seL3nAC65Waf",
        "colab_type": "text"
      },
      "source": [
        "### `43.` 名詞を含む文節が動詞を含む文節に係るものを抽出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLnBBKKvhs_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
        "#「41. 係り受け解析結果の読み込み（文節・係り受け）」のコードをはじめに実行してください。\n",
        "\n",
        "def q43():\n",
        "  chunk_parsed_text = q41()\n",
        "  for chunks_in_sentence in chunk_parsed_text:\n",
        "    print('-' * 50)\n",
        "    for chunk_index in range(len(chunks_in_sentence) - 1):\n",
        "      src_chunk_has_noun = False\n",
        "      dst_chunk_has_verb = False\n",
        "      chunk_src_index = chunk_index\n",
        "      chunk_dst_index = chunks_in_sentence[chunk_index].dst\n",
        "      \n",
        "      # 係り元チャンクが名詞の形態素を持っているか\n",
        "      for morph in chunks_in_sentence[chunk_src_index].morphs:\n",
        "        if morph.pos == '名詞':\n",
        "          src_chunk_has_noun = True\n",
        "          break\n",
        "      \n",
        "      # 係り先チャンクが動詞の形態素を持っているか\n",
        "      for morph in chunks_in_sentence[chunk_dst_index].morphs:\n",
        "        if morph.pos == '動詞':\n",
        "          dst_chunk_has_verb = True\n",
        "          break\n",
        "\n",
        "      if src_chunk_has_noun and dst_chunk_has_verb:\n",
        "        chunk_src_content = chunks_in_sentence[chunk_index].chunk_content(without_symbols=True)\n",
        "        chunk_dst_content = chunks_in_sentence[chunks_in_sentence[chunk_index].dst].chunk_content(without_symbols=True)\n",
        "        print('{}\\t{}'.format(chunk_src_content, chunk_dst_content))\n",
        "\n",
        "q43()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIzVSUuH5Z-R",
        "colab_type": "text"
      },
      "source": [
        "### `44.` 係り受け木の可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz7UVgQv0ul4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 44. 係り受け木の可視化\n",
        "#「41. 係り受け解析結果の読み込み（文節・係り受け）」のコードをはじめに実行してください。\n",
        "\n",
        "def q44():\n",
        "  chunk_parsed_text = q41()\n",
        "  chunks_in_sentence = chunk_parsed_text[6]\n",
        "  g = open('/content/5/44.dot', 'w')\n",
        "  g.write('digraph 44 {')\n",
        "  \n",
        "  # node definition\n",
        "  for chunk_index in range(len(chunks_in_sentence)):\n",
        "    g.write(u'\\t{} [label=\\\"{}\\\"];\\n'.format(chunk_index, chunks_in_sentence[chunk_index].chunk_content(without_symbols=True)))\n",
        "    pass\n",
        "  \n",
        "  # node definition\n",
        "  for chunk_index in range(len(chunks_in_sentence) - 1):\n",
        "    g.write(u'\\t{} -> {};\\n'.format(chunk_index, chunks_in_sentence[chunk_index].dst))\n",
        "    pass\n",
        "\n",
        "  g.write(u'}')\n",
        "  g.close()\n",
        "\n",
        "q44()\n",
        "\n",
        "!dot -Tpng 44.dot -o 44.png\n",
        "\n",
        "from IPython.display import Image,display_png\n",
        "display_png(Image('44.png'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR-POANk5qRl",
        "colab_type": "text"
      },
      "source": [
        "### `45.` 動詞の格パターンの抽出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG-F-_TaNwML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 45. 動詞の格パターンの抽出\n",
        "def q45():\n",
        "  chunk_parsed_text = q41()\n",
        "  g = open('45.txt', 'w')\n",
        "  kaku_pattern_dict = {}\n",
        "  for chunks_in_sentence in chunk_parsed_text:\n",
        "    for chunk_index in range(len(chunks_in_sentence)):\n",
        "      \n",
        "      # チャンクの中で動詞を含む形態素を探す\n",
        "      contains_verb = False\n",
        "      for morph in chunks_in_sentence[chunk_index].morphs:\n",
        "        if morph.pos == '動詞':\n",
        "          target_verb = morph.base\n",
        "          contains_verb = True\n",
        "          break\n",
        "      \n",
        "      # 動詞を含んでいなければ次のチャンクでの探索に移る\n",
        "      if not contains_verb:\n",
        "        continue\n",
        "      \n",
        "      # 格パターン辞書に対象の動詞がなかったら追加\n",
        "      if target_verb not in kaku_pattern_dict.keys():\n",
        "        kaku_pattern_dict[target_verb] = []\n",
        "      \n",
        "      # 係り元チャンクの助詞を検索して格パターン辞書に追加する\n",
        "      for chunk_src_index in chunks_in_sentence[chunk_index].srcs:\n",
        "        for morph in chunks_in_sentence[chunk_src_index].morphs:\n",
        "          # 形態素が助詞で、格パターン辞書になかったら追加する\n",
        "          if morph.pos == '助詞' and morph.surface not in kaku_pattern_dict[target_verb]:\n",
        "            kaku_pattern_dict[target_verb].append(morph.surface)\n",
        "      \n",
        "      # ファイルに書き込み\n",
        "      for verb, jyoshis in kaku_pattern_dict.items():\n",
        "        if len(jyoshis) >= 1:\n",
        "          g.write('{}\\t{}\\n'.format(verb, ' '.join(sorted(jyoshis))))\n",
        "      kaku_pattern_dict = {}\n",
        "  \n",
        "  g.close()\n",
        "\n",
        "q45()\n",
        "\n",
        "!python3 -c \"print('-' * 50)\"\n",
        "!cat 45.txt | sort | uniq -c | sort -n -r -k 1 | head -n 10\n",
        "!python3 -c \"print('-' * 50)\"\n",
        "!cat 45.txt | awk -F \"\\t\" '$1 == \"する\" { print; }' | sort | uniq -c | sort -n -r -k 1 | head -n 10\n",
        "!python3 -c \"print('-' * 50)\"\n",
        "!cat 45.txt | awk -F \"\\t\" '$1 == \"見る\" { print; }' | sort | uniq -c | sort -n -r -k 1 | head -n 10\n",
        "!python3 -c \"print('-' * 50)\"\n",
        "!cat 45.txt | awk -F \"\\t\" '$1 == \"与える\" { print; }' | sort | uniq -c | sort -n -r -k 1 | head -n 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f5namvqP2d-",
        "colab_type": "text"
      },
      "source": [
        "### `46.` 動詞の格フレーム情報の抽出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sesKtE9P1hQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 46. 動詞の格フレーム情報の抽出\n",
        "\n",
        "def q46():\n",
        "  chunk_parsed_text = q41()\n",
        "  g = open('46.txt', 'w')\n",
        "  kaku_pattern_dict = {}\n",
        "  for chunks_in_sentence in chunk_parsed_text:\n",
        "    for chunk_index in range(len(chunks_in_sentence)):\n",
        "      \n",
        "      # チャンクの中で動詞を含む形態素を探す\n",
        "      contains_verb = False\n",
        "      for morph in chunks_in_sentence[chunk_index].morphs:\n",
        "        if morph.pos == '動詞':\n",
        "          target_verb = morph.base\n",
        "          contains_verb = True\n",
        "          break\n",
        "      \n",
        "      # 動詞を含んでいなければ次のチャンクでの探索に移る\n",
        "      if not contains_verb:\n",
        "        continue\n",
        "      \n",
        "      # 格パターン辞書に対象の動詞がなかったら追加\n",
        "      if target_verb not in kaku_pattern_dict.keys():\n",
        "        kaku_pattern_dict[target_verb] = []\n",
        "      \n",
        "      # 係り元チャンクの助詞を検索して格パターン辞書に追加する\n",
        "      for chunk_src_index in chunks_in_sentence[chunk_index].srcs:\n",
        "        chunk_includes_jyoshi = False\n",
        "        for morph in chunks_in_sentence[chunk_src_index].morphs:\n",
        "          # 形態素が助詞で、格パターン辞書になかったら助詞のリストに追加する\n",
        "          if morph.pos == '助詞':\n",
        "            jyoshi = morph.surface\n",
        "            chunk_includes_jyoshi = True\n",
        "            \n",
        "        # チャンクに助詞が含まれていたならそのチャンクをチャンクのリストに追加する\n",
        "        if chunk_includes_jyoshi:\n",
        "          kaku_pattern_dict[target_verb].append({'jyoshi': jyoshi, 'chunk': chunks_in_sentence[chunk_src_index].chunk_content()})\n",
        "      \n",
        "      # ファイルに書き込み\n",
        "      for verb, jyoshis_and_chunks in kaku_pattern_dict.items():\n",
        "        if len(jyoshis_and_chunks) >= 1:\n",
        "          jyoshis_and_chunks.sort(key=lambda x:x['jyoshi'])\n",
        "          g.write('{}\\t{}\\t{}\\n'.format(verb, ' '.join([x['jyoshi'] for x in jyoshis_and_chunks]), ' '.join([x['chunk'] for x in jyoshis_and_chunks])))\n",
        "      kaku_pattern_dict = {}\n",
        "  \n",
        "  g.close()\n",
        "\n",
        "q46()\n",
        "\n",
        "!head -n 10 46.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeVbUMsbzjXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  !head -n 70 neko.txt.cabocha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLSGKTiWxdln",
        "colab_type": "text"
      },
      "source": [
        "### `47.` 機能動詞構文のマイニング"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDKm4DO0yFIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 47. 機能動詞構文のマイニング\n",
        "\n",
        "def q47():\n",
        "  chunk_parsed_text = q41()\n",
        "  g = open('47.txt', 'w')\n",
        "  kaku_pattern_dict = {}\n",
        "  for chunks_in_sentence in chunk_parsed_text:\n",
        "    for chunk_index in range(len(chunks_in_sentence)):\n",
        "      \n",
        "      # チャンクの中で動詞を含む形態素を探す\n",
        "      check = False\n",
        "      if len(chunks_in_sentence[chunk_index].morphs) == 2 \\\n",
        "      and chunks_in_sentence[chunk_index].morphs[0].pos1 == 'サ変接続' \\\n",
        "      and chunks_in_sentence[chunk_index].morphs[1].surface == 'を':\n",
        "        for morph in chunks_in_sentence[chunk_index + 1].morphs:\n",
        "          if morph.pos == '動詞':\n",
        "            target_verb = morph.base\n",
        "            check = True\n",
        "            break\n",
        "      \n",
        "      # 条件を満たせていなければ次のチャンクでの探索に移る\n",
        "      if not check:\n",
        "        continue\n",
        "\n",
        "      # 述語部分の組み立て\n",
        "      jyutugo = chunks_in_sentence[chunk_index].chunk_content() + target_verb\n",
        "      \n",
        "      # 係り元チャンクを検索する\n",
        "      src_chunk_info_list = []\n",
        "      for chunk_src_index in chunks_in_sentence[chunk_index].srcs:\n",
        "        \n",
        "        chunk_includes_jyoshi = False\n",
        "        for morph in chunks_in_sentence[chunk_src_index].morphs:\n",
        "          # 形態素が助詞で、格パターン辞書になかったら助詞のリストに追加する\n",
        "          if morph.pos == '助詞':\n",
        "            jyoshi = morph.surface\n",
        "            chunk_includes_jyoshi = True\n",
        "            \n",
        "        # チャンクに助詞が含まれていたならそのチャンクをチャンクのリストに追加する\n",
        "        if chunk_includes_jyoshi:\n",
        "          src_chunk_info_list.append({'jyoshi': jyoshi, 'chunk': chunks_in_sentence[chunk_src_index].chunk_content()})\n",
        "      \n",
        "      # ファイルに書き込み\n",
        "      if len(src_chunk_info_list) >= 1:\n",
        "        src_chunk_info_list.sort(key=lambda x:x['jyoshi'])\n",
        "        g.write('{}\\t{}\\t{}\\n'.format(jyutugo, ' '.join([x['jyoshi'] for x in src_chunk_info_list]), ' '.join([x['chunk'] for x in src_chunk_info_list])))\n",
        "  \n",
        "  g.close()\n",
        "\n",
        "q47()\n",
        "\n",
        "!cat 47.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OMtMB9mf4U2",
        "colab_type": "text"
      },
      "source": [
        "### `48.` 名詞から根へのパスの抽出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmIzJAq6f3V-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 48. 名詞から根へのパスの抽出\n",
        "\n",
        "def q48():\n",
        "  chunk_parsed_text = q41()\n",
        "  g = open('48.txt', 'w')\n",
        "  for chunks_in_sentence in chunk_parsed_text:\n",
        "    graph = {}\n",
        "    for chunk_index in range(len(chunks_in_sentence)):\n",
        "      # 名詞を含む文節であれば、構文木の開始地点として登録する\n",
        "      includes_noun = False\n",
        "      for morph in chunks_in_sentence[chunk_index].morphs:\n",
        "        if morph.pos == '名詞':\n",
        "          graph[chunk_index] = [chunk_index]\n",
        "\n",
        "      # 構文木を登録\n",
        "      for chunk_src_index in chunks_in_sentence[chunk_index].srcs:\n",
        "        for key in graph.keys():\n",
        "          if graph[key][-1] == chunk_src_index:\n",
        "            graph[key].append(chunk_index)\n",
        "    \n",
        "    # 構文木の表示\n",
        "    g.write('-' * 50 + '\\n')\n",
        "    for chunk_index_begin, chunk_flow in graph.items():\n",
        "      output = ' -> '.join([chunks_in_sentence[chunk_index].chunk_content() for chunk_index in chunk_flow])\n",
        "      g.write(output + '\\n')\n",
        "  \n",
        "  g.close()\n",
        "      \n",
        "q48()\n",
        "\n",
        "!head -n 55 48.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_ZP3ZyhIKJg",
        "colab_type": "text"
      },
      "source": [
        "### `49.` 名詞間の係り受けパスの抽出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvNenOyKIJQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 49. 名詞間の係り受けパスの抽出\n",
        "\n",
        "def q49():\n",
        "  chunk_parsed_text = q41()\n",
        "  g = open('49.txt', 'w')\n",
        "  for chunks_in_sentence in chunk_parsed_text:\n",
        "    g.write('-' * 50 + '\\n')\n",
        "    graph = {}\n",
        "    for chunk_index in range(len(chunks_in_sentence)):\n",
        "      # 名詞を含む文節であれば、構文木の開始地点として登録する\n",
        "      includes_noun = False\n",
        "      for morph in chunks_in_sentence[chunk_index].morphs:\n",
        "        if morph.pos == '名詞':\n",
        "          graph[chunk_index] = [chunk_index]\n",
        "\n",
        "      # 構文木を登録\n",
        "      for chunk_src_index in chunks_in_sentence[chunk_index].srcs:\n",
        "        for key in graph.keys():\n",
        "          if graph[key][-1] == chunk_src_index:\n",
        "            graph[key].append(chunk_index)\n",
        "    \n",
        "    # 構文木の表示\n",
        "    for chunk_index_begin in graph.keys():\n",
        "      for chunk_index_end in graph.keys():\n",
        "\n",
        "        # i < jである場合のみ先まで処理を続ける\n",
        "        if not chunk_index_begin < chunk_index_end:\n",
        "          continue\n",
        "        \n",
        "        # 文節iの文字列を作成\n",
        "        chunk_begin_content = ''\n",
        "        for morph in chunks_in_sentence[chunk_index_begin].morphs:\n",
        "          if morph.pos == '名詞':\n",
        "            chunk_begin_content += 'X'\n",
        "          else:\n",
        "            chunk_begin_content += morph.surface\n",
        "        \n",
        "        # 文節jの文字列を作成\n",
        "        chunk_end_content = ''\n",
        "        for morph in chunks_in_sentence[chunk_index_end].morphs:\n",
        "          if morph.pos == '名詞':\n",
        "            chunk_end_content += 'Y'\n",
        "          else:\n",
        "            chunk_end_content += morph.surface\n",
        "        \n",
        "        output = ''\n",
        "\n",
        "        # 文節i -> 文節jへのパスがある場合\n",
        "        if chunk_index_end in graph[chunk_index_begin]:\n",
        "          output += chunk_begin_content\n",
        "          for chunk_index in graph[chunk_index_begin]:\n",
        "            if chunk_index == chunk_index_end:\n",
        "              output += ' -> ' + chunk_end_content\n",
        "            else:\n",
        "              output += ' -> ' + chunks_in_sentence[chunk_index].chunk_content()\n",
        "        \n",
        "        # 文節i -> 文節jへのパスがない場合\n",
        "        else:\n",
        "          # 共通インデックスを探す\n",
        "          common_index = -1\n",
        "          for index_x in graph[chunk_index_begin]:\n",
        "            for index_y in graph[chunk_index_end]:\n",
        "              if index_x == index_y:\n",
        "                common_index = index_x\n",
        "                break\n",
        "            if common_index != -1:\n",
        "              break\n",
        "          \n",
        "          # 共通するインデックスが存在する場合のみ先まで処理を続ける\n",
        "          if common_index == -1:\n",
        "            continue\n",
        "          \n",
        "          output += chunk_begin_content\n",
        "          for chunk_index in graph[chunk_index_begin][1:]:\n",
        "            if chunk_index == common_index:\n",
        "              break\n",
        "            output += ' -> ' + chunks_in_sentence[chunk_index].chunk_content()\n",
        "          \n",
        "          output += ' | '\n",
        "          output += chunk_end_content\n",
        "          for chunk_index in graph[chunk_index_end][1:]:\n",
        "            if chunk_index == common_index:\n",
        "              break\n",
        "            output += ' -> ' + chunks_in_sentence[chunk_index].chunk_content()\n",
        "          \n",
        "          output += ' | '\n",
        "          output += chunks_in_sentence[common_index].chunk_content()\n",
        "            \n",
        "          g.write(output + '\\n')\n",
        "\n",
        "  g.close()\n",
        "\n",
        "q49()\n",
        "\n",
        "!head -n 55 49.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57YD2I-ZO5I5",
        "colab_type": "text"
      },
      "source": [
        "## 第6章: 機械学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk00ANmepdok",
        "colab_type": "text"
      },
      "source": [
        "### 事前準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNwQeBXZpeFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 第6章\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tqdm import tqdm\n",
        "!pip install stop-words\n",
        "from stop_words import get_stop_words\n",
        "!pip install nltk\n",
        "from nltk import stem\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "!pip install japanize_matplotlib\n",
        "import japanize_matplotlib\n",
        "\n",
        "# pd.DataFrameで省略せず表示する最大カラム数を100とする\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "# 乱数のシード値を0に設定\n",
        "random.seed(0)\n",
        "\n",
        "!mkdir /content/6\n",
        "%cd /content/6/\n",
        "\n",
        "# dataset download\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
        "!unzip NewsAggregatorDataset.zip; rm -r NewsAggregatorDataset.zip\n",
        "!cat newsCorpora.csv | awk -F \"\\t\" '$4==\"Reuters\" {print;}' | head -n 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkuXbzMC5wNI",
        "colab_type": "text"
      },
      "source": [
        "### `50.` データの入手・整形"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8NkatZK3xzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 50. データの入手・整形\n",
        "\n",
        "def q50():\n",
        "  # extract train.txt, valid.txt, test.txt\n",
        "  f = open('newsCorpora.csv', 'r')\n",
        "  article_info_list = []\n",
        "\n",
        "  # 記事の情報を集める\n",
        "  while True:\n",
        "    line = f.readline()[:-1] # exclude '\\n'\n",
        "    \n",
        "    # 終端になったらbreak\n",
        "    if len(line) == 0:\n",
        "      break\n",
        "    \n",
        "    # 記事の情報を'\\t'区切りで取得\n",
        "    article_info = line.split('\\t')\n",
        "\n",
        "    # 記事の出版社と記事のタイトルとカテゴリを取得\n",
        "    publisher, title, category = itemgetter(3, 1, 4)(article_info)\n",
        "    if publisher in ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']:\n",
        "      article_info_list.append({'category': category, 'title': title})\n",
        "\n",
        "  # 記事をシャッフル  \n",
        "  random.shuffle(article_info_list)\n",
        "\n",
        "  number_of_filtered_article = len(article_info_list)\n",
        "  g1 = open('train.txt', 'w')\n",
        "  g2 = open('valid.txt', 'w')\n",
        "  g3 = open('test.txt', 'w')\n",
        "  for index, article_info in enumerate(article_info_list):\n",
        "    content = '{}\\t{}\\n'.format(article_info['category'], article_info['title'])\n",
        "    if index <= number_of_filtered_article * 0.8:\n",
        "      g1.write(content)\n",
        "    elif index <= number_of_filtered_article * 0.9:\n",
        "      g2.write(content)\n",
        "    else:\n",
        "      g3.write(content)\n",
        "  \n",
        "  f.close()\n",
        "  g1.close()\n",
        "  g2.close()\n",
        "  g3.close()\n",
        "\n",
        "q50()\n",
        "\n",
        "# 内容確認\n",
        "!echo '--- # of lines in train.txt---'\n",
        "!wc -l train.txt\n",
        "!echo '--- # of lines in valid.txt---'\n",
        "!wc -l valid.txt\n",
        "!echo '--- # of lines in test.txt---'\n",
        "!wc -l test.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeCXNktI5z6S",
        "colab_type": "text"
      },
      "source": [
        "### `51.` 特徴量抽出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nRgXtH-NmS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 51. 特徴量抽出\n",
        "# 単語列によるBag-of-Wordsのほかに特徴量が思い浮かばぬ…\n",
        "# TF-IDFについては、テキストが短すぎて文書中での単語の出現回数の多さが期待できないと思ったため使用せず\n",
        "\n",
        "def q51():\n",
        "  stemmer = stem.PorterStemmer()\n",
        "  stop_words = get_stop_words('en')\n",
        "  pattern_only_symbols = re.compile('[^0-9a-zA-Z]+')\n",
        "  pattern_start_with_symbol = re.compile('^[^0-9a-zA-Z].*')\n",
        "  pattern_end_with_symbol = re.compile('.*[^0-9a-zA-Z]$')\n",
        "\n",
        "  files_input = ['train.txt', 'valid.txt', 'test.txt']\n",
        "  files_output = ['train.feature.txt', 'valid.feature.txt', 'test.feature.txt']\n",
        "  assert len(files_input) == len(files_output)\n",
        "  \n",
        "  # Bag-of-Wordsをつくる\n",
        "  word_and_freq = {}\n",
        "  idiom_and_freq = {}\n",
        "  file_input = open('train.txt', 'r')\n",
        "  lines = file_input.read().split('\\n')[:-1]\n",
        "  \n",
        "  # 出現する単語/熟語とその頻度、出現する単語/熟語とその頻度を求める\n",
        "  print('[+] 出現する単語/熟語とその頻度、出現する単語/熟語とその頻度を求める')\n",
        "  for line in tqdm(lines):\n",
        "    prev_word = 'XXXXX'\n",
        "    words = line.split('\\t')[1].split()\n",
        "    for word in words:\n",
        "      \n",
        "      # 記号のみの場合記録しない\n",
        "      if pattern_only_symbols.match(word):\n",
        "        continue\n",
        "      \n",
        "      # 小文字の文字列にする\n",
        "      word = word.lower()\n",
        "      \n",
        "      # 記号から始まる場合はその記号をスキップした文字列をとる\n",
        "      if pattern_start_with_symbol.match(word):\n",
        "        word = word[1:]\n",
        "      \n",
        "      # 記号で終わる場合はそれまでの文字列をとる\n",
        "      if pattern_end_with_symbol.match(word):\n",
        "        word = word[:-1]\n",
        "      \n",
        "      # ストップワードにある場合は記録しない\n",
        "      if word in stop_words:\n",
        "        continue\n",
        "      \n",
        "      # 語幹を取り出す\n",
        "      word = stemmer.stem(word)\n",
        "      \n",
        "      # 単語と出現頻度の辞書に加える\n",
        "      if word not in word_and_freq.keys():\n",
        "        word_and_freq[word] = 1\n",
        "      else:\n",
        "        word_and_freq[word] += 1\n",
        "      \n",
        "      # 熟語と出現頻度の辞書に加える\n",
        "      idiom = prev_word + ' ' + word\n",
        "      if idiom not in idiom_and_freq.keys():\n",
        "        idiom_and_freq[idiom] = 1\n",
        "      else:\n",
        "        idiom_and_freq[idiom] += 1\n",
        "      prev_word = word\n",
        "\n",
        "  file_input.close()\n",
        "\n",
        "  # 今後のためにid順の特徴量名のリストを作る\n",
        "  feature_names = []\n",
        "\n",
        "  # 単語に単語idを付与\n",
        "  word2id = {}\n",
        "  word_id = 0\n",
        "  for word, freq in word_and_freq.items():\n",
        "    if 2 < freq < 300:\n",
        "      word2id[word] = word_id\n",
        "      feature_names.append(word)\n",
        "      word_id += 1\n",
        "  \n",
        "  # 熟語に熟語idを付与\n",
        "  idiom2id = {}\n",
        "  idiom_id = 0\n",
        "  for idiom, freq in idiom_and_freq.items():\n",
        "    if 3 < freq:\n",
        "      idiom2id[idiom] = idiom_id\n",
        "      feature_names.append(idiom)\n",
        "      idiom_id += 1\n",
        "\n",
        "  print('[+] number of word: {}'.format(len(word2id)))\n",
        "  print('[+] number of idiom: {}'.format(len(idiom2id)))\n",
        "\n",
        "  # id順に並んだ特徴量名のリストをファイルに保存\n",
        "  f = open('feature_names.txt', 'w')\n",
        "  f.write('\\t'.join(feature_names))\n",
        "  f.close()\n",
        "  \n",
        "  feature_length = len(word2id.keys()) + len(idiom2id.keys())\n",
        "  idiom_id_offset = len(word2id.keys())\n",
        "\n",
        "  # カテゴリと、idの付けられた単語、熟語を抽出し、特徴量ファイルに書き込み\n",
        "  for file_index in range(len(files_input)):\n",
        "    file_input = open(files_input[file_index], 'r')\n",
        "    file_output = open(files_output[file_index], 'w')\n",
        "    lines = file_input.read().split('\\n')[:-1]\n",
        "    \n",
        "    print('[+]カテゴリと、idの付けられた単語、熟語を抽出し、特徴量ファイルに書き込み ({}/3)'.format(file_index + 1))\n",
        "    prev_word = 'XXXXX'\n",
        "    \n",
        "    for line in tqdm(lines):\n",
        "      feature = [0 for _ in range(feature_length)]\n",
        "      category, title = line.split('\\t')\n",
        "      words = title.split(' ')\n",
        "      \n",
        "      for word in words:\n",
        "        \n",
        "        # 記号のみの場合記録しない\n",
        "        if pattern_only_symbols.match(word):\n",
        "          continue\n",
        "        \n",
        "        # 小文字の文字列にする\n",
        "        word = word.lower()\n",
        "        \n",
        "        # 記号から始まる場合はその記号をスキップした文字列をとる\n",
        "        if pattern_start_with_symbol.match(word):\n",
        "          word = word[1:]\n",
        "        \n",
        "        # 記号で終わる場合はそれまでの文字列をとる\n",
        "        if pattern_end_with_symbol.match(word):\n",
        "          word = word[:-1]\n",
        "        \n",
        "        # ストップワードにある場合は記録しない\n",
        "        if word in stop_words:\n",
        "          continue\n",
        "        \n",
        "        # 語幹を取り出す\n",
        "        word = stemmer.stem(word)\n",
        "        idiom = prev_word + ' ' + word\n",
        "        \n",
        "        # 出現単語/熟語がidがあてられたものであれば1とする\n",
        "        if word in word2id.keys():\n",
        "          feature[word2id[word]] = 1\n",
        "        if idiom in idiom2id.keys():\n",
        "          feature[idiom_id_offset + idiom2id[idiom]] = 1\n",
        "        \n",
        "        prev_word = word\n",
        "\n",
        "      # ファイルに書き込み(特徴量ファイル)\n",
        "      output = category\n",
        "      for i in range(len(feature)):\n",
        "        output += (' %d' % feature[i])\n",
        "      output += '\\n'\n",
        "      file_output.write(output)\n",
        "\n",
        "    file_input.close()\n",
        "    file_output.close()\n",
        "    \n",
        "q51()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3IbcWuz59zx",
        "colab_type": "text"
      },
      "source": [
        "### `52.` 学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQiFX5mD6AKk",
        "colab_type": "text"
      },
      "source": [
        "### `53.` 予測"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtDCzLQv6FOT",
        "colab_type": "text"
      },
      "source": [
        "### `54.` 正解率の計測"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mq2vPcs6E3G",
        "colab_type": "text"
      },
      "source": [
        "### `55.` 混同行列の作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhq4L4LR6Era",
        "colab_type": "text"
      },
      "source": [
        "### `56.` 適合率，再現率，F1スコアの計測"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BjRm1Qp6EfF",
        "colab_type": "text"
      },
      "source": [
        "### `57.` 特徴量の重みの確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDTqnCAH--Mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 52. 学習\n",
        "### 53. 予測\n",
        "### 54. 正解率の計測\n",
        "### 55. 混同行列の作成\n",
        "### 56. 適合率，再現率，F1スコアの計測\n",
        "### 57. 特徴量の重みの確認\n",
        "\n",
        "# 先にq51()を実行してください\n",
        "\n",
        "def q52_58():\n",
        "  # 訓練、検証、テストデータから特徴量を抽出\n",
        "  files_input = ['train.feature.txt', 'valid.feature.txt', 'test.feature.txt']\n",
        "  \n",
        "  # 説明変数と目的変数\n",
        "  Xs = []\n",
        "  ys = []\n",
        "\n",
        "  # カテゴリからカテゴリidへのマップ\n",
        "  category2id = {'b': 0, 't': 1, 'm': 2, 'e': 3}\n",
        "  \n",
        "  # 各ファイルから説明変数とラベル(カテゴリ)を読み込む\n",
        "  for file_index in range(len(files_input)):\n",
        "    Xs.append([])\n",
        "    ys.append([])\n",
        "    file_input = open(files_input[file_index], 'r')\n",
        "    lines = file_input.read().split('\\n')[:-1]\n",
        "\n",
        "    # 特徴量ファイルから特徴量を読み込み、説明変数と目的変数を読み込む\n",
        "    print('[+]特徴量ファイルから特徴量を読み込み、説明変数と目的変数を読み込む ({}/3)'.format(file_index + 1))\n",
        "    for line in tqdm(lines):\n",
        "      X_line = []\n",
        "      split = line.split(' ')\n",
        "      category_str = split[0]\n",
        "      features_str = split[1:]\n",
        "      y = category2id[category_str]\n",
        "      for feature_str in features_str:\n",
        "        X_line.append(int(feature_str))\n",
        "      Xs[file_index].append(X_line)\n",
        "      ys[file_index].append(y)\n",
        "    file_input.close()\n",
        "  \n",
        "  X_train = Xs[0]\n",
        "  X_valid = Xs[1]\n",
        "  X_test = Xs[2]\n",
        "  y_train = ys[0]\n",
        "  y_valid = ys[1]\n",
        "  y_test = ys[2]\n",
        "  \n",
        "  ### 52. 学習\n",
        "  print('--- 52. 学習 ---')\n",
        "  print('[+] 学習開始')\n",
        "  lr = LogisticRegression(random_state=0, max_iter=1000)\n",
        "  lr.fit(X_train, y_train)\n",
        "\n",
        "  #pred = lr.predict(X_train)\n",
        "  print('[+] 学習終了、予測開始')\n",
        "  probs = lr.predict_proba(X_train)\n",
        "  y_train_pred = probs.argmax(axis=1)\n",
        "  y_train_prob = probs.max(axis=1)\n",
        "\n",
        "  ### 53. 予測\n",
        "  print('--- 53. 予測 ---')\n",
        "  for i in range(10):\n",
        "    print('[{}] 正解カテゴリ:{}, 予測カテゴリ:{}, 予測確率:{}'.format(i, y_train[i], y_train_pred[i], y_train_prob[i]))\n",
        "\n",
        "  ### 54. 正解率の計測\n",
        "  print('--- 54. 正解率の計測 ---')\n",
        "  \n",
        "  # trainの正解率\n",
        "  accuracy_score_train = accuracy_score(y_train_pred, y_train)\n",
        "  print('[+] 訓練データでの正解率: {}'.format(accuracy_score_train))\n",
        "\n",
        "  # testの正解率\n",
        "  probs = lr.predict_proba(X_test)\n",
        "  y_test_pred = probs.argmax(axis=1)\n",
        "  accuracy_score_test = accuracy_score(y_test_pred, y_test)\n",
        "  print('[+] テストデータでの正解率: {}'.format(accuracy_score_test))\n",
        "\n",
        "  ### 55. 混同行列の作成\n",
        "  print('--- 55. 混同行列の作成 ---')\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "\n",
        "  # 訓練データでのconfusion_matrix\n",
        "  print('[+]訓練データのconfusion_matrix')\n",
        "  cm_train = confusion_matrix(y_train_pred, y_train)\n",
        "  print(cm_train)\n",
        "  sns.heatmap(cm_train, annot=True, ax=ax1)\n",
        "\n",
        "  # テストデータでのconfusion_matrix\n",
        "  print('[+]テストデータのconfusion_matrix')\n",
        "  cm_test = confusion_matrix(y_test_pred, y_test)\n",
        "  print(cm_test)\n",
        "  sns.heatmap(cm_test, annot=True, ax=ax2)\n",
        "\n",
        "  ### 56. 適合率，再現率，F1スコアの計測\n",
        "  print('--- 56. 適合率，再現率，F1スコアの計測 ---')\n",
        "  # カテゴリごとにprecision, recall, F1を求める\n",
        "  ps = [cm_test[i][i] / np.sum(cm_test, axis=0)[i] for i in range(len(category2id))]\n",
        "  rs = [cm_test[i][i] / np.sum(cm_test, axis=1)[i] for i in range(len(category2id))]\n",
        "  fs = [2 * ps[i] * rs[i] / (ps[i] + rs[i]) for i in range(len(category2id))]\n",
        "\n",
        "  # マイクロ平均を求める\n",
        "  tp = np.sum(np.diag(cm_test))\n",
        "  fp = np.sum(np.tril(cm_test, k=-1))\n",
        "  fn = np.sum(np.triu(cm_test, k=1))\n",
        "  micro_average = {}\n",
        "  micro_average['precision'] = tp / (tp + fp)\n",
        "  micro_average['recall'] = tp / (tp + fn)\n",
        "  micro_average['f1'] = 2 * micro_average['precision'] * micro_average['recall'] / (micro_average['precision'] + micro_average['recall'])\n",
        "\n",
        "  # マクロ平均を求める\n",
        "  macro_average = {}\n",
        "  macro_average['precision'] = np.mean(ps)\n",
        "  macro_average['recall'] = np.mean(rs)\n",
        "  macro_average['f1'] = np.mean(fs)\n",
        "  \n",
        "  # 表を作る - データの用意\n",
        "  data = [[ps[i], rs[i], fs[i]] for i in range(len(category2id))]\n",
        "  data.append([micro_average['precision'], micro_average['recall'], micro_average['f1']])\n",
        "  data.append([macro_average['precision'], macro_average['recall'], macro_average['f1']])\n",
        "\n",
        "  # 表を作る - 項目の用意\n",
        "  index = ['b', 't', 'm', 'e', 'マイクロ平均', 'マクロ平均']\n",
        "  columns = ['適合率(precision)', '再現率(recall)', 'F1値(F1-score)']\n",
        "  df = pd.DataFrame(data, index=index, columns=columns)\n",
        "  print(df)\n",
        "\n",
        "  ### 57. 特徴量の重みの確認\n",
        "  f = open('feature_names.txt', 'r')\n",
        "  feature_names = f.read().split('\\t')\n",
        "  print('--- 57. 特徴量の重みの確認 ---')\n",
        "  \n",
        "  # カテゴリごとに特徴量の重みを表示\n",
        "  for category_index, category_name in enumerate(['business', 'science and technology', 'health', 'entertainment']):\n",
        "    \n",
        "    print('[{}]'.format(category_name))\n",
        "\n",
        "    # 特徴量と係数の対応辞書を作成\n",
        "    feature_and_coef = {}\n",
        "    for i in range(len(lr.coef_[category_index])):\n",
        "      feature_and_coef[feature_names[i]] = lr.coef_[category_index][i]\n",
        "    \n",
        "    # 特徴量と係数の対応辞書をソート\n",
        "    feature_and_coef_sorted = sorted(feature_and_coef.items(), key=lambda x:x[1], reverse=True)\n",
        "\n",
        "    # 係数の大きい特徴量から10個表示\n",
        "    print('[+]特徴量の大きい順')\n",
        "    data = [[feature_and_coef_sorted[i][0] for i in range(10)], [feature_and_coef_sorted[i][1] for i in range(10)]]\n",
        "    df = pd.DataFrame(data, index=['特徴量', '係数'], columns=[i + 1 for i in range(10)])\n",
        "    print(df)\n",
        "\n",
        "    # 係数の大きい特徴量から10個表示\n",
        "    print('[+]特徴量の小さい順')\n",
        "    data = [[feature_and_coef_sorted[-10:][::-1][i][0] for i in range(10)], [feature_and_coef_sorted[-10:][::-1][i][1] for i in range(10)]]\n",
        "    df = pd.DataFrame(data, index=['特徴量', '係数'], columns=[i + 1 for i in range(10)])\n",
        "    print(df)\n",
        "\n",
        "q52_58()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr_-cNYl6ibI",
        "colab_type": "text"
      },
      "source": [
        "### `58.` 正則化パラメータの変更"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN67qNvb3Ona",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 58. 正則化パラメータの変更\n",
        "\n",
        "# 先にq50(), q51()を実行してください\n",
        "\n",
        "def q58():\n",
        "  # 訓練、検証、テストデータから特徴量を抽出\n",
        "  files_input = ['train.feature.txt', 'valid.feature.txt', 'test.feature.txt']\n",
        "  \n",
        "  # 説明変数と目的変数\n",
        "  Xs = []\n",
        "  ys = []\n",
        "\n",
        "  # カテゴリからカテゴリidへのマップ\n",
        "  category2id = {'b': 0, 't': 1, 'm': 2, 'e': 3}\n",
        "  \n",
        "  # 各ファイルから説明変数とラベル(カテゴリ)を読み込む\n",
        "  for file_index in range(len(files_input)):\n",
        "    Xs.append([])\n",
        "    ys.append([])\n",
        "    file_input = open(files_input[file_index], 'r')\n",
        "    lines = file_input.read().split('\\n')[:-1]\n",
        "\n",
        "    # 特徴量ファイルから特徴量を読み込み、説明変数と目的変数を読み込む\n",
        "    print('[+]特徴量ファイルから特徴量を読み込み、説明変数と目的変数を読み込む ({}/3)'.format(file_index + 1))\n",
        "    for line in tqdm(lines):\n",
        "      X_line = []\n",
        "      split = line.split(' ')\n",
        "      category_str = split[0]\n",
        "      features_str = split[1:]\n",
        "      y = category2id[category_str]\n",
        "      for feature_str in features_str:\n",
        "        X_line.append(int(feature_str))\n",
        "      Xs[file_index].append(X_line)\n",
        "      ys[file_index].append(y)\n",
        "    file_input.close()\n",
        "  \n",
        "  X_train = Xs[0]\n",
        "  X_valid = Xs[1]\n",
        "  X_test = Xs[2]\n",
        "  y_train = ys[0]\n",
        "  y_valid = ys[1]\n",
        "  y_test = ys[2]\n",
        "  \n",
        "  Cs = []\n",
        "  Cs.extend([0.1 + 0.05 * i for i in range(18)])\n",
        "  Cs.extend([1 + 0.5 * i for i in range(18)])\n",
        "  Cs.extend([10 + 5 * i for i in range(18)])\n",
        "  \n",
        "  accuracies = []\n",
        "\n",
        "  # 正則化パラメータCごとに学習\n",
        "  for C in Cs:\n",
        "    print('[+]学習開始(C={})'.format(C))\n",
        "    lr = LogisticRegression(C=C, random_state=0, max_iter=1000)\n",
        "    lr.fit(X_train, y_train)\n",
        "\n",
        "    print('[+]予測開始(C={})'.format(C))\n",
        "    probs = lr.predict_proba(X_test)\n",
        "    y_test_pred = probs.argmax(axis=1)\n",
        "    accuracy = accuracy_score(y_test_pred, y_test)\n",
        "\n",
        "    print('[+]正解率: {}'.format(accuracy))\n",
        "    accuracies.append(accuracy)\n",
        "  \n",
        "  plt.xscale('log')\n",
        "  plt.title(\"正則化パラメータCの値と正解率の関係\", fontsize=18)\n",
        "  plt.xlabel(\"正則化パラメータCの値\", fontsize=16)\n",
        "  plt.ylabel(\"正解率\", fontsize=16)\n",
        "  plt.tick_params(labelsize=14)\n",
        "  plt.plot(Cs, accuracies)\n",
        "\n",
        "q58()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y17LwsVY6oby",
        "colab_type": "text"
      },
      "source": [
        "### `59.` ハイパーパラメータの探索"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SjOgne6Rfrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 59. ハイパーパラメータの探索\n",
        "\n",
        "def q59():\n",
        "  # 訓練、検証、テストデータから特徴量を抽出\n",
        "  files_input = ['train.feature.txt', 'valid.feature.txt', 'test.feature.txt']\n",
        "  \n",
        "  # 説明変数と目的変数\n",
        "  Xs = []\n",
        "  ys = []\n",
        "\n",
        "  # カテゴリからカテゴリidへのマップ\n",
        "  category2id = {'b': 0, 't': 1, 'm': 2, 'e': 3}\n",
        "  \n",
        "  # 各ファイルから説明変数とラベル(カテゴリ)を読み込む\n",
        "  for file_index in range(len(files_input)):\n",
        "    Xs.append([])\n",
        "    ys.append([])\n",
        "    file_input = open(files_input[file_index], 'r')\n",
        "    lines = file_input.read().split('\\n')[:-1]\n",
        "\n",
        "    # 特徴量ファイルから特徴量を読み込み、説明変数と目的変数を読み込む\n",
        "    print('[+]特徴量ファイルから特徴量を読み込み、説明変数と目的変数を読み込む ({}/3)'.format(file_index + 1))\n",
        "    for line in tqdm(lines):\n",
        "      X_line = []\n",
        "      split = line.split(' ')\n",
        "      category_str = split[0]\n",
        "      features_str = split[1:]\n",
        "      y = category2id[category_str]\n",
        "      for feature_str in features_str:\n",
        "        X_line.append(int(feature_str))\n",
        "      Xs[file_index].append(X_line)\n",
        "      ys[file_index].append(y)\n",
        "    file_input.close()\n",
        "  \n",
        "  X_train = Xs[0]\n",
        "  X_valid = Xs[1]\n",
        "  X_test = Xs[2]\n",
        "  y_train = ys[0]\n",
        "  y_valid = ys[1]\n",
        "  y_test = ys[2]\n",
        "  \n",
        "q59()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etM6TO-2A11M",
        "colab_type": "text"
      },
      "source": [
        "## 第7章: 単語ベクトル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FWFVgF-pov2",
        "colab_type": "text"
      },
      "source": [
        "### 事前準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtyyUevdppJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 第7章\n",
        "from tqdm import tqdm\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from sklearn.manifold import TSNE\n",
        "import gensim\n",
        "!pip install japanize_matplotlib\n",
        "import japanize_matplotlib\n",
        "\n",
        "filename_dataset='GoogleNews-vectors-negative300.bin.gz'\n",
        "!mkdir /content/7/\n",
        "%cd /content/7/\n",
        "!wget -O $filename_dataset https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "!gzip -d $filename_dataset\n",
        "!rm -r $filename_dataset\n",
        "!wget http://download.tensorflow.org/data/questions-words.txt\n",
        "!wget http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.zip\n",
        "!unzip wordsim353.zip\n",
        "\n",
        "# 単語ベクトルの読み込み\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubeYodtr6xva",
        "colab_type": "text"
      },
      "source": [
        "### `60.` 単語ベクトルの読み込みと表示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOZjp2hUBg3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 60. 単語ベクトルの読み込みと表示\n",
        "def q60():\n",
        "  print(word2vec_model['United_States'])\n",
        "\n",
        "q60()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk0Pozj-60u8",
        "colab_type": "text"
      },
      "source": [
        "### `61.` 単語の類似度"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtHS7jUCEbLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 61. 単語の類似度\n",
        "import numpy as np\n",
        "\n",
        "def q61():\n",
        "  # メソッドから\n",
        "  similarity_1 = word2vec_model.similarity('United_States', 'U.S.')\n",
        "  # コサイン類似度の計算\n",
        "  similarity_2 = np.dot(word2vec_model['United_States'], word2vec_model['U.S.']) / (np.linalg.norm(word2vec_model['United_States']) * np.linalg.norm(word2vec_model['U.S.']))\n",
        "  \n",
        "  # 確認\n",
        "  print(similarity_1)\n",
        "  print(similarity_2)\n",
        "\n",
        "q61()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg-VviLQ659z",
        "colab_type": "text"
      },
      "source": [
        "### `62.` 類似度の高い単語10件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuFyx0y2HdhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 62. 類似度の高い単語10件\n",
        "def q62():\n",
        "  similarities = word2vec_model.most_similar('United_States')\n",
        "  for i in range(10):\n",
        "    print('[{}] 単語: \\\"{}\\\", 類似度: {}'.format(i + 1, similarities[i][0], similarities[i][1]))\n",
        "\n",
        "q62()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSJUW3Mb68yX",
        "colab_type": "text"
      },
      "source": [
        "### `63.` 加法構成性によるアナロジー"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUfYjsGSLM-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 63. 加法構成性によるアナロジー\n",
        "def q63():\n",
        "  similarities = word2vec_model.most_similar(positive=['Spain', 'Athens'], negative=['Madrid'])\n",
        "  for i in range(10):\n",
        "    print('[{}] 単語: \\\"{}\\\", 類似度: {}'.format(i + 1, similarities[i][0], similarities[i][1]))\n",
        "\n",
        "q63()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgEW3Gyw6_nq",
        "colab_type": "text"
      },
      "source": [
        "### `64.` アナロジーデータでの実験"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrW6ZVCoM8mK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 64. アナロジーデータでの実験\n",
        "def q64():\n",
        "  f = open('questions-words.txt', 'r')\n",
        "  g = open('questions-words-added.txt', 'w')\n",
        "\n",
        "  lines = f.read().split('\\n')[:-1]\n",
        "  for line in tqdm(lines):\n",
        "    if line[0] == ':':\n",
        "      g.write(line + '\\n')\n",
        "      continue\n",
        "    words = line.split(' ')\n",
        "    result_word, result_pred = word2vec_model.most_similar(positive=[words[1], words[2]], negative=[words[0]], topn=1)[0]\n",
        "    g.write(line + ' ' + result_word + ' ' + str(result_pred) + '\\n')\n",
        "  \n",
        "  f.close()\n",
        "  g.close()\n",
        "\n",
        "q64()\n",
        "\n",
        "!head -n 5 questions-words-added.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc14rULd7CxK",
        "colab_type": "text"
      },
      "source": [
        "### `65.` アナロジータスクでの正解率"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhmOGaQqnAB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 65. アナロジータスクでの正解率\n",
        "def q65():\n",
        "  f = open('questions-words-added.txt', 'r')\n",
        "  lines = f.read().split('\\n')[:-1]\n",
        "  \n",
        "  # アナロジーのカテゴリと、全体数および正解数\n",
        "  analogy_category = 'syntactic'\n",
        "  number_syntactic = 0\n",
        "  number_syntactic_acc = 0\n",
        "  number_semantic = 0\n",
        "  number_semantic_acc = 0\n",
        "  \n",
        "  for line in lines:\n",
        "    words = line.split()\n",
        "\n",
        "    # アナロジーのカテゴリの変更\n",
        "    if line[0] == ':':\n",
        "      if ': gram' in line:\n",
        "        analogy_category = 'syntactic'\n",
        "      else:\n",
        "        analogy_category = 'semantic'\n",
        "      continue\n",
        "\n",
        "    # 意味的アナロジー、文法的アナロジーごとに個数と正解数を計算\n",
        "    print(split)\n",
        "    if analogy_category == 'syntactic':\n",
        "      number_syntactic += 1\n",
        "      if split[3] == split[4]:\n",
        "        number_syntactic_acc += 1\n",
        "    elif analogy_category == 'semantic':\n",
        "      number_semantic += 1\n",
        "      if split[3] == split[4]:\n",
        "        number_semantic_acc += 1\n",
        "  \n",
        "  # 正解率の計算\n",
        "  print('意味的アナロジー正解率: {}'.format(number_syntactic / number_syntactic_acc))\n",
        "  print('文法的アナロジー正解率: {}'.format(number_semantic / number_semantic_acc))\n",
        "\n",
        "q65()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE7SIa2W7Hdj",
        "colab_type": "text"
      },
      "source": [
        "### `66.` WordSimilarity-353での評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR-YGVC1q1xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 66. WordSimilarity-353での評価\n",
        "def q66():\n",
        "  f = open('combined.csv', 'r')\n",
        "  similarities_human = []\n",
        "  similarities_wordvec = []\n",
        "  lines = f.read().split('\\n')[1:-1]\n",
        "  for line in lines:\n",
        "    data_line = line.split(',')\n",
        "    similarities_human.append([data_line[0], data_line[1], float(data_line[2])])\n",
        "    similarities_wordvec.append([data_line[0], data_line[1], word2vec_model.similarity(data_line[0], data_line[1])])\n",
        "  f.close()\n",
        "  \n",
        "  similarities_human.sort(key=lambda x: x[2])\n",
        "  similarities_wordvec.sort(key=lambda x: x[2])\n",
        "\n",
        "  sum_d2 = 0\n",
        "  n = len(similarities_human)\n",
        "  assert len(similarities_human) == len(similarities_wordvec)\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      if similarities_human[i][0] == similarities_wordvec[j][0] and similarities_human[i][1] == similarities_wordvec[j][1]:\n",
        "        sum_d2 += (i - j) ** 2\n",
        "        continue\n",
        "  \n",
        "  rs = 1 - (6 * sum_d2) / (n ** 3 - n)\n",
        "  print('スピアマン相関係数: {}'.format(rs))\n",
        "  \n",
        "q66()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeytEGC37Ksg",
        "colab_type": "text"
      },
      "source": [
        "### `67.` k-meansクラスタリング"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUZ9PMHq7iR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 67. k-meansクラスタリング\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def q67():\n",
        "  '''\n",
        "  0: インデックス1と3を取得\n",
        "  1: インデックス0と2を取得\n",
        "  2: 取得しない\n",
        "  '''\n",
        "  fetchmode_dict = {\n",
        "      ': capital-common-countries': 0,\n",
        "      ': capital-world': 0,\n",
        "      ': currency': 1,\n",
        "      ': city-in-state': 2,\n",
        "      ': family': 2,\n",
        "      ': gram1-adjective-to-adverb': 2,\n",
        "      ': gram2-opposite': 2,\n",
        "      ': gram3-comparative': 2,\n",
        "      ': gram4-superlative': 2,\n",
        "      ': gram5-present-participle' : 2,\n",
        "      ': gram6-nationality-adjective': 1,\n",
        "      ': gram7-past-tense': 2,\n",
        "      ': gram8-plural': 2,\n",
        "      ': gram9-plural-verbs': 2\n",
        "  }\n",
        "\n",
        "  # 国名の集合を作成\n",
        "  fetchmode = 2\n",
        "  f = open('questions-words.txt', 'r')\n",
        "  countries = set()\n",
        "  lines = f.read().split('\\n')[:-1]\n",
        "  for line in lines:\n",
        "    if line[0] == ':':\n",
        "      fetchmode = fetchmode_dict[line]\n",
        "      continue\n",
        "    \n",
        "    split = line.split(' ')\n",
        "    if fetchmode == 0:\n",
        "      countries.add(split[1])\n",
        "      countries.add(split[3])\n",
        "    elif fetchmode == 1:\n",
        "      countries.add(split[0])\n",
        "      countries.add(split[2])\n",
        "  \n",
        "  countries = list(countries)\n",
        "  countries_vec = [word2vec_model[country] for country in countries]\n",
        "  f.close()\n",
        "\n",
        "  # k-meansクラスタリング\n",
        "  k = 5\n",
        "  kmeans_model = KMeans(n_clusters=k, random_state=0).fit(countries_vec)\n",
        "  kmeans_result = [[] for i in range(k)]\n",
        "  for i, label in enumerate(kmeans_model.labels_):\n",
        "    kmeans_result[label].append(countries[i])\n",
        "  \n",
        "  for i in range(k):\n",
        "    print('----- class {} -----'.format(i))\n",
        "    print(kmeans_result[i])\n",
        "  \n",
        "  # PCAで可視化(参考: https://qiita.com/maskot1977/items/34158d044711231c4292)\n",
        "  pca = PCA()\n",
        "  pca.fit(countries_vec)\n",
        "\n",
        "  countries_vec_2d = pca.transform(countries_vec)\n",
        "  color_codes = {0:'#ff0000', 1:'#ffff00', 2:'#ff00ff', 3:'#00ffff', 4:'#0000ff'}\n",
        "  colors = colors = [color_codes[x] for x in kmeans_model.labels_]\n",
        "\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  for x, y, name in zip(countries_vec_2d[:, 0], countries_vec_2d[:, 1], countries):\n",
        "    plt.text(x, y, name, alpha=0.8, size=10)\n",
        "  plt.scatter(countries_vec_2d[:, 0], countries_vec_2d[:, 1], alpha=0.8, color=colors)\n",
        "  plt.title('k-means(k=5)によるクラスタリング結果のPCAによる可視化結果', fontsize=18)\n",
        "  plt.xlabel('第一主成分', fontsize=16)\n",
        "  plt.ylabel('第二主成分', fontsize=16)\n",
        "  plt.tick_params(labelsize=14)\n",
        "  plt.show()\n",
        "  \n",
        "q67()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inO1JR8a7Nk_",
        "colab_type": "text"
      },
      "source": [
        "### `68.` Ward法によるクラスタリング"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM0ymdnKcB7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 68. Ward法によるクラスタリング\n",
        "def q68():\n",
        "  '''\n",
        "  0: インデックス1と3を取得\n",
        "  1: インデックス0と2を取得\n",
        "  2: 取得しない\n",
        "  '''\n",
        "  fetchmode_dict = {\n",
        "      ': capital-common-countries': 0,\n",
        "      ': capital-world': 0,\n",
        "      ': currency': 1,\n",
        "      ': city-in-state': 2,\n",
        "      ': family': 2,\n",
        "      ': gram1-adjective-to-adverb': 2,\n",
        "      ': gram2-opposite': 2,\n",
        "      ': gram3-comparative': 2,\n",
        "      ': gram4-superlative': 2,\n",
        "      ': gram5-present-participle' : 2,\n",
        "      ': gram6-nationality-adjective': 1,\n",
        "      ': gram7-past-tense': 2,\n",
        "      ': gram8-plural': 2,\n",
        "      ': gram9-plural-verbs': 2\n",
        "  }\n",
        "\n",
        "  # 国名の集合を作成\n",
        "  fetchmode = 2\n",
        "  f = open('questions-words.txt', 'r')\n",
        "  countries = set()\n",
        "  lines = f.read().split('\\n')[:-1]\n",
        "  for line in lines:\n",
        "    if line[0] == ':':\n",
        "      fetchmode = fetchmode_dict[line]\n",
        "      continue\n",
        "    \n",
        "    split = line.split(' ')\n",
        "    if fetchmode == 0:\n",
        "      countries.add(split[1])\n",
        "      countries.add(split[3])\n",
        "    elif fetchmode == 1:\n",
        "      countries.add(split[0])\n",
        "      countries.add(split[2])\n",
        "  \n",
        "  countries = list(countries)  \n",
        "  country_vectors = [word2vec_model[country] for country in countries]\n",
        "  f.close()\n",
        "  \n",
        "  # 参考: https://analytics-note.xyz/machine-learning/scipy-cluster-hierarchy/\n",
        "  # ユークリッド距離とウォード法を使用してクラスタリング\n",
        "  z = linkage(country_vectors, metric='euclidean', method='ward')\n",
        "\n",
        "  # 結果を可視化\n",
        "  plt.figure(figsize=(30, 6))\n",
        "  dendrogram(z, labels=countries)\n",
        "  plt.title('ユークリッド距離を用いたウォード法による階層型クラスタリングのデンドログラム', fontsize=18)\n",
        "  plt.xlabel('国名', fontsize=16)\n",
        "  plt.tick_params(labelsize=14)\n",
        "  plt.show()\n",
        "\n",
        "q68()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5T8kDfJ7QnL",
        "colab_type": "text"
      },
      "source": [
        "### `69.` t-SNEによる可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlR-m3FaiYhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 69. t-SNEによる可視化\n",
        "  '''\n",
        "  0: インデックス1と3を取得\n",
        "  1: インデックス0と2を取得\n",
        "  2: 取得しない\n",
        "  '''\n",
        "  fetchmode_dict = {\n",
        "      ': capital-common-countries': 0,\n",
        "      ': capital-world': 0,\n",
        "      ': currency': 1,\n",
        "      ': city-in-state': 2,\n",
        "      ': family': 2,\n",
        "      ': gram1-adjective-to-adverb': 2,\n",
        "      ': gram2-opposite': 2,\n",
        "      ': gram3-comparative': 2,\n",
        "      ': gram4-superlative': 2,\n",
        "      ': gram5-present-participle' : 2,\n",
        "      ': gram6-nationality-adjective': 1,\n",
        "      ': gram7-past-tense': 2,\n",
        "      ': gram8-plural': 2,\n",
        "      ': gram9-plural-verbs': 2\n",
        "  }\n",
        "\n",
        "  # 国名の集合を作成\n",
        "  fetchmode = 2\n",
        "  f = open('questions-words.txt', 'r')\n",
        "  countries = set()\n",
        "  lines = f.read().split('\\n')[:-1]\n",
        "  for line in lines:\n",
        "    if line[0] == ':':\n",
        "      fetchmode = fetchmode_dict[line]\n",
        "      continue\n",
        "    \n",
        "    split = line.split(' ')\n",
        "    if fetchmode == 0:\n",
        "      countries.add(split[1])\n",
        "      countries.add(split[3])\n",
        "    elif fetchmode == 1:\n",
        "      countries.add(split[0])\n",
        "      countries.add(split[2])\n",
        "  \n",
        "  countries = list(countries)\n",
        "  country_vecs = [word2vec_model[country] for country in countries]\n",
        "  f.close()\n",
        "\n",
        "  # k-meansクラスタリング\n",
        "  k = 5\n",
        "  kmeans_model = KMeans(n_clusters=k, random_state=0).fit(country_vecs)\n",
        "  \n",
        "  color_codes = {0:'#ff0000', 1:'#ffff00', 2:'#ff00ff', 3:'#00ffff', 4:'#0000ff'}\n",
        "  colors = colors = [color_codes[x] for x in kmeans_model.labels_]\n",
        "\n",
        "  # t-SNEによる次元削減\n",
        "  tsne_result = TSNE(n_components=2, random_state=0).fit_transform(country_vecs)\n",
        "\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  for x, y, name in zip(tsne_result[:, 0], tsne_result[:, 1], countries):\n",
        "    plt.text(x, y, name, alpha=0.8, size=10)\n",
        "  plt.scatter(tsne_result[:, 0], tsne_result[:, 1], alpha=0.8, color=colors)\n",
        "  plt.title('t-SNEによる次元削減後の国名に関する単語ベクトルのベクトル空間の可視化結果', fontsize=18)\n",
        "  plt.xlabel('第一主成分', fontsize=16)\n",
        "  plt.ylabel('第二主成分', fontsize=16)\n",
        "  plt.tick_params(labelsize=14)\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmBmnH553neH",
        "colab_type": "text"
      },
      "source": [
        "## 第8章: ニューラルネット"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIA1tiqdpyYv",
        "colab_type": "text"
      },
      "source": [
        "### 事前準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7DsjZhzpy1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 第8章\n",
        "!pip install stop-words\n",
        "from stop_words import get_stop_words\n",
        "from tqdm import tqdm\n",
        "!pip install torch==1.3.1+cu100 torchvision==0.4.2+cu100 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "import gensim\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install japanize_matplotlib\n",
        "import japanize_matplotlib\n",
        "\n",
        "filename_dataset='GoogleNews-vectors-negative300.bin.gz'\n",
        "!mkdir /content/8/\n",
        "%cd /content/8/\n",
        "!wget -O $filename_dataset https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "!gzip -d $filename_dataset\n",
        "\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfv8xNbN7Wia",
        "colab_type": "text"
      },
      "source": [
        "### `70.` 単語ベクトルの和による特徴量"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo44W6GV8QTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 70. 単語ベクトルの和による特徴量\n",
        "\n",
        "# はじめにq50()を実行してください。\n",
        "\n",
        "!cp /content/6/train.txt /content/8/train.txt\n",
        "!cp /content/6/valid.txt /content/8/valid.txt\n",
        "!cp /content/6/test.txt /content/8/test.txt\n",
        "\n",
        "def q70():\n",
        "  stop_words = get_stop_words('en')\n",
        "  category2id = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
        "  pattern_only_symbols = re.compile('[^0-9a-zA-Z]+')\n",
        "  pattern_start_with_symbol = re.compile('^[^0-9a-zA-Z].*')\n",
        "  pattern_end_with_symbol = re.compile('.*[^0-9a-zA-Z]$')\n",
        "  files_input = ['train.txt', 'valid.txt', 'test.txt']\n",
        "  files_output = ['train.words.txt', 'valid.words.txt', 'test.words.txt']\n",
        "\n",
        "  # 単語と熟語を抽出し、単語列ファイルに書き込み\n",
        "  for file_index in range(len(files_input)):\n",
        "    file_input = open(files_input[file_index], 'r')\n",
        "    file_output = open(files_output[file_index], 'w')\n",
        "    lines = file_input.read().split('\\n')[:-1]\n",
        "    \n",
        "    print('[+]単語を抽出し、単語列ファイルに書き込む ({}/3)'.format(file_index + 1))\n",
        "    for line in lines:\n",
        "      prev_word = 'XXXXX'\n",
        "      words_and_idioms = []\n",
        "      category, title = line.split('\\t')\n",
        "      words = title.split(' ')\n",
        "      \n",
        "      for word in words:\n",
        "        # 記号のみの場合記録しない\n",
        "        if pattern_only_symbols.match(word):\n",
        "          continue\n",
        "\n",
        "        # 記号から始まる場合はその記号をスキップした文字列をとる\n",
        "        if pattern_start_with_symbol.match(word):\n",
        "          word = word[1:]\n",
        "        \n",
        "        # 記号で終わる場合はそれまでの文字列をとる\n",
        "        if pattern_end_with_symbol.match(word):\n",
        "          word = word[:-1]\n",
        "        \n",
        "        # ストップワードにある場合は記録しない\n",
        "        if word in stop_words:\n",
        "          continue\n",
        "        \n",
        "        words_and_idioms.append(word)\n",
        "        words_and_idioms.append(prev_word + '_' + word)\n",
        "        prev_word = word\n",
        "\n",
        "      # ファイルに書き込み(単語羅列ファイル)\n",
        "      output = category\n",
        "      for word_and_idiom in words_and_idioms:\n",
        "        if 'XXXXX' not in word_and_idiom:\n",
        "          output += (' ' + word_and_idiom)\n",
        "      output += '\\n'\n",
        "      file_output.write(output)\n",
        "\n",
        "    file_input.close()\n",
        "    file_output.close()\n",
        "\n",
        "  # 単語列ファイルから単語を読み込み、特徴量行列とラベルベクトルをファイルに保存\n",
        "  files_input = ['train.words.txt', 'valid.words.txt', 'test.words.txt']\n",
        "  files_output_X = ['X_train.pickle', 'X_valid.pickle', 'X_test.pickle']\n",
        "  files_output_y = ['y_train.pickle', 'y_valid.pickle', 'y_test.pickle']\n",
        "  \n",
        "  for file_index in range(len(files_input)):\n",
        "    X = []\n",
        "    y = []\n",
        "    file_input = open(files_input[file_index], 'r')\n",
        "    file_output_X = open(files_output_X[file_index], 'wb')\n",
        "    file_output_y = open(files_output_y[file_index], 'wb')\n",
        "\n",
        "    lines = file_input.read().split('\\n')[:-1]\n",
        "    for line in lines:\n",
        "      split = line.split(' ')\n",
        "      category = split[0]\n",
        "      words = split[1:]\n",
        "\n",
        "      wordvecs = []\n",
        "      for word in words:\n",
        "        if word in word2vec_model:\n",
        "          wordvecs.append(word2vec_model[word])\n",
        "      titlevec = np.sum(np.array(wordvecs), axis=0)\n",
        "      X.append(titlevec)\n",
        "      y.append(category2id[category])\n",
        "\n",
        "    # torchで扱う行列形式にする\n",
        "    X = torch.from_numpy(np.array(X))\n",
        "    y = torch.from_numpy(np.array(y))\n",
        "\n",
        "    pickle.dump(X, file_output_X)\n",
        "    pickle.dump(y, file_output_y)\n",
        "\n",
        "q70()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riTznpDp7gle",
        "colab_type": "text"
      },
      "source": [
        "### `71.` 単層ニューラルネットワークによる予測"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbxbuAWGOi_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 71. 単層ニューラルネットワークによる予測\n",
        "def q71():\n",
        "\n",
        "  # シードの固定\n",
        "  np.random.seed(0)\n",
        "  random.seed(0)\n",
        "  torch.manual_seed(0)\n",
        "  torch.cuda.manual_seed(0)\n",
        "\n",
        "  # 特徴量行列を取り出す\n",
        "  with open('X_train.pickle', 'rb') as f:\n",
        "    X_train = pickle.load(f)\n",
        "  with open('y_train.pickle', 'rb') as f:\n",
        "    y_train = pickle.load(f)\n",
        "\n",
        "  # 乱数のウェイト\n",
        "  W = torch.rand(4, 300)\n",
        "  \n",
        "  # 線形変換\n",
        "  x = F.linear(X_train[0:4], W)\n",
        "  # softmax\n",
        "  x = F.softmax(x, dim=-1)\n",
        "  print(x)\n",
        "\n",
        "  # 確率分布の確認：総和は1\n",
        "  print(sum(x[0]))\n",
        "\n",
        "q71()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV4jCj9S7kOT",
        "colab_type": "text"
      },
      "source": [
        "### `72.` 損失と勾配の計算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2WUq6YUVanT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 72. 損失と勾配の計算\n",
        "def q72():\n",
        "  class q72_Net(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "      super(q72_Net, self).__init__()\n",
        "      self.fc1 = nn.Linear(in_features, out_features, bias=False)\n",
        "      nn.init.xavier_normal_(self.fc1.weight)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x = self.fc1(x)\n",
        "      x = F.softmax(x)\n",
        "      return x\n",
        "\n",
        "  # シードの固定\n",
        "  np.random.seed(0)\n",
        "  random.seed(0)\n",
        "  torch.manual_seed(0)\n",
        "  torch.cuda.manual_seed(0)\n",
        "\n",
        "  # 特徴量行列を取り出す\n",
        "  with open('X_train.pickle', 'rb') as f:\n",
        "    X_train = pickle.load(f)\n",
        "  with open('y_train.pickle', 'rb') as f:\n",
        "    y_train = pickle.load(f)\n",
        "  \n",
        "  # 出力の4項目は、softmaxによるニュースの4カテゴリの確率\n",
        "  net = q72_Net(300, 4)\n",
        "\n",
        "  # クロスエントロピー損失関数のインスタンス\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  output = net(X_train[:4])\n",
        "  target = y_train[:4]\n",
        "  loss = criterion(output, target)\n",
        "\n",
        "  # 損失の表示\n",
        "  print('--- 損失 ---')\n",
        "  print(loss)\n",
        "  \n",
        "  # backward()により勾配が計算される\n",
        "  net.zero_grad()\n",
        "  loss.backward()\n",
        "\n",
        "  # fc1のウェイトの表示\n",
        "  print('--- 勾配 ---')\n",
        "  print(net.fc1.weight.grad)\n",
        "\n",
        "q72()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du-sGzYy7obQ",
        "colab_type": "text"
      },
      "source": [
        "### `73.` 確率的勾配降下法による学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5DsZCo77oQA",
        "colab_type": "text"
      },
      "source": [
        "### `74.` 正解率の計測"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHpC5XN27oFb",
        "colab_type": "text"
      },
      "source": [
        "### `75.` 損失と正解率のプロット"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_wZ1LAv7n5b",
        "colab_type": "text"
      },
      "source": [
        "### `76.` チェックポイント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIa-jkyAwosL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 73. 確率的勾配降下法による学習\n",
        "### 74. 正解率の計測\n",
        "### 75. 損失と正解率のプロット\n",
        "### 76. チェックポイント\n",
        "\n",
        "def q73_76():\n",
        "  class q73_76_Net(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "      super().__init__()\n",
        "      self.fc1 = nn.Linear(in_features, out_features, bias=False)\n",
        "      nn.init.xavier_normal_(self.fc1.weight)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x = self.fc1(x)\n",
        "      x = F.softmax(x)\n",
        "      return x\n",
        "  \n",
        "  # 学習過程でのパラメータや最適化アルゴリズムを保存するディレクトリを作成\n",
        "  dir_name = 'model_states'\n",
        "  if not os.path.exists(dir_name):\n",
        "    os.mkdir(dir_name)\n",
        "\n",
        "  # 定数\n",
        "  EPOCHS = 100\n",
        "  LEARNING_RATE_SGD = 0.05\n",
        "\n",
        "  # シードの固定\n",
        "  np.random.seed(0)\n",
        "  random.seed(0)\n",
        "  torch.manual_seed(0)\n",
        "  torch.cuda.manual_seed(0)\n",
        "\n",
        "  # CUDAを使用可能であればCUDAを、そうでなければCPUを使う\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  print(device)\n",
        "\n",
        "  # 訓練データから特徴量行列とラベルベクトルを取り出す\n",
        "  with open('X_train.pickle', 'rb') as f:\n",
        "    X_train = pickle.load(f).to(device)\n",
        "  with open('y_train.pickle', 'rb') as f:\n",
        "    y_train = pickle.load(f).to(device)\n",
        "\n",
        "  # 訓練データから特徴量行列とラベルベクトルを取り出す\n",
        "  with open('X_valid.pickle', 'rb') as f:\n",
        "    X_valid = pickle.load(f).to(device)\n",
        "  with open('y_valid.pickle', 'rb') as f:\n",
        "    y_valid = pickle.load(f).to(device)\n",
        "  \n",
        "  # 事例数を求める\n",
        "  num_train = len(y_train)\n",
        "  num_valid = len(y_valid)\n",
        "\n",
        "  net = q73_76_Net(300, 4)\n",
        "  net.to(device)\n",
        "  \n",
        "  # 最適化手法として確率的勾配降下法(SGD)を指定\n",
        "  optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE_SGD)\n",
        "\n",
        "  # ロス関数としてクロスエントロピーを指定\n",
        "  # 実は内部でsoftmaxによる確率分布化も実行してくれるらしい-> https://qiita.com/mathlive/items/8e1f9a8467fff8dfd03c\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # 学習過程の可視化のために正解率と損失の表示\n",
        "  history_train_loss = []\n",
        "  history_train_accuracy = []\n",
        "  history_valid_loss = []\n",
        "  history_valid_accuracy = []\n",
        "  \n",
        "  for epoch in range(EPOCHS):\n",
        "\n",
        "    # 訓練データで学習\n",
        "    # 学習を行う設定にする\n",
        "    net.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    # 勾配が累積されないようgradientを0にする\n",
        "    optimizer.zero_grad()\n",
        "    # モデルの出力をとる\n",
        "    output = net(X_train)\n",
        "    # ロスを求める\n",
        "    train_loss = criterion(output, y_train)\n",
        "    history_train_loss.append(train_loss)\n",
        "    # backprop\n",
        "    train_loss.backward()\n",
        "    # パラメータの更新\n",
        "    optimizer.step()\n",
        "\n",
        "    # モデルやoptimizerの状態を保存\n",
        "    # いったんモデルのデバイスをcpuに変えて保存する\n",
        "    # 読み込み時: model.load_state_dict(torch.load('xxx.pth'))\n",
        "    net = net.to('cpu')\n",
        "    state = {\n",
        "      'epoch': epoch,\n",
        "      'state_dict': net.state_dict(),\n",
        "      'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(state, './{}/model_and_optimizer_state_{:0>3}.pth'.format(dir_name, epoch))\n",
        "    net = net.to(device)\n",
        "\n",
        "    # 訓練データでの正解率を求める\n",
        "    y_train_pred = output.argmax(dim=1, keepdim=True)\n",
        "    y_train_pred = [y[0] for y in y_train_pred]\n",
        "    correct_count = 0\n",
        "    for i in range(num_train):\n",
        "      if torch.eq(y_train[i], y_train_pred[i]):\n",
        "        correct_count += 1\n",
        "    train_accuracy = correct_count / num_train\n",
        "    history_train_accuracy.append(train_accuracy)\n",
        "\n",
        "    # 検証データで検証\n",
        "    # 学習を行わないようにする(検証データで検証を行うため)\n",
        "    net.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    # モデルの出力をとる\n",
        "    output = net(X_valid)\n",
        "    # 損失を求める\n",
        "    valid_loss = criterion(output, y_valid)\n",
        "    history_valid_loss.append(valid_loss)\n",
        "\n",
        "    # 検証データでの正解率を求める\n",
        "    y_valid_pred = output.argmax(dim=1, keepdim=True)\n",
        "    y_valid_pred = [y[0] for y in y_valid_pred]\n",
        "    correct_count = 0\n",
        "    for i in range(num_valid):\n",
        "      if torch.eq(y_valid[i], y_valid_pred[i]):\n",
        "        correct_count += 1\n",
        "    valid_accuracy = correct_count / num_valid\n",
        "    history_valid_accuracy.append(valid_accuracy)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      print('[{}] 損失(訓練データ): {:.4f}, 正解率(訓練データ): {:.4f}, 損失(検証データ): {:.4f}, 正解率(検証データ): {:.4f}'.format(epoch, train_loss, train_accuracy, valid_loss, valid_accuracy))\n",
        "  \n",
        "  # 学習の進行の様子をグラフで表示\n",
        "  fig = plt.figure(figsize=(10, 4))\n",
        "  #fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
        "\n",
        "  # 損失について\n",
        "  axL = fig.add_subplot(1, 2, 1)\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_train_loss, label='訓練データ')\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_valid_loss, label='検証データ')\n",
        "  axL.set_title('epochごとの損失の変化')\n",
        "  axL.set_xlabel('epoch')\n",
        "  axL.set_ylabel('損失')\n",
        "  axL.legend()\n",
        "  \n",
        "  # 正解率の変化\n",
        "  axR = fig.add_subplot(1, 2, 2)\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_train_accuracy, label='訓練データ')\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_valid_accuracy, label='検証データ')\n",
        "  axR.set_title('epochごとの正解率の変化')\n",
        "  axR.set_xlabel('epoch')\n",
        "  axR.set_ylabel('正解率')\n",
        "  axR.set_ylim(0, 1)\n",
        "  axR.legend()\n",
        "  \n",
        "  fig.show()\n",
        "\n",
        "q73_76()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72U0bvVt74dD",
        "colab_type": "text"
      },
      "source": [
        "### `77.` ミニバッチ化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjedVCQk74QZ",
        "colab_type": "text"
      },
      "source": [
        "### `78.` GPU上での学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ui25VD2avTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 77. ミニバッチ化\n",
        "### 78. GPU上での学習\n",
        "\n",
        "import time\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "def q77_78():\n",
        "  class q77_78_Net(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "      super().__init__()\n",
        "      self.fc1 = nn.Linear(in_features, out_features, bias=False)\n",
        "      nn.init.xavier_normal_(self.fc1.weight)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x = self.fc1(x)\n",
        "      x = F.softmax(x)\n",
        "      return x\n",
        "  \n",
        "  # 学習過程でのパラメータや最適化アルゴリズムを保存するディレクトリを作成\n",
        "  dir_name = 'model_states'\n",
        "  if not os.path.exists(dir_name):\n",
        "    os.mkdir(dir_name)\n",
        "\n",
        "  # 定数\n",
        "  BATCH_SIZE_LIST = [2 ** i for i in range(13)]\n",
        "  EPOCHS = 5\n",
        "  LEARNING_RATE_SGD = 0.01\n",
        "\n",
        "  # CUDAを使用可能であればCUDAを、そうでなければCPUを使う\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  print(device)\n",
        "\n",
        "  # 訓練データから特徴量行列とラベルベクトルを取り出す\n",
        "  with open('X_train.pickle', 'rb') as f:\n",
        "    X_train = pickle.load(f).to(device)\n",
        "  with open('y_train.pickle', 'rb') as f:\n",
        "    y_train = pickle.load(f).to(device)\n",
        "\n",
        "  # 訓練データから特徴量行列とラベルベクトルを取り出す\n",
        "  with open('X_valid.pickle', 'rb') as f:\n",
        "    X_valid = pickle.load(f).to(device)\n",
        "  with open('y_valid.pickle', 'rb') as f:\n",
        "    y_valid = pickle.load(f).to(device)\n",
        "  \n",
        "  # 事例数を求める\n",
        "  num_train = len(y_train)\n",
        "  num_valid = len(y_valid)\n",
        "\n",
        "  # グラフを作成するための設定\n",
        "  fig, ((fig_train_loss, fig_train_accuracy),\n",
        "        (fig_valid_loss, fig_valid_accuracy),\n",
        "        (fig_elapsed_time,_)) = plt.subplots(nrows=3, ncols=2, figsize=(10, 30))\n",
        "  \n",
        "  fig_train_loss.set_title('訓練データにおけるepochごとの損失の変化')\n",
        "  fig_train_accuracy.set_title('訓練データにおけるepochごとの正解率の変化')\n",
        "  fig_valid_loss.set_title('検証データにおけるepochごとの損失の変化')\n",
        "  fig_valid_accuracy.set_title('検証データにおけるepochごとの正解率の変化')\n",
        "  fig_elapsed_time.set_title('バッチサイズと1epochあたりの学習所要時間の関係')\n",
        "  \n",
        "  fig_train_loss.set_xlabel('epoch')\n",
        "  fig_train_accuracy.set_xlabel('epoch')\n",
        "  fig_valid_loss.set_xlabel('epoch')\n",
        "  fig_valid_accuracy.set_xlabel('epoch')\n",
        "  fig_elapsed_time.set_xlabel('バッチサイズ')\n",
        "\n",
        "  fig_train_loss.set_ylabel('損失')\n",
        "  fig_train_accuracy.set_ylabel('正解率')\n",
        "  fig_valid_loss.set_ylabel('損失')\n",
        "  fig_valid_accuracy.set_ylabel('正解率')\n",
        "  fig_elapsed_time.set_ylabel('1epochあたりの所要時間(秒)')\n",
        "\n",
        "  # x軸方向目盛り\n",
        "  fig_xarray_epoch = range(EPOCHS)\n",
        "  \n",
        "  # 経過時間の保存\n",
        "  elapsed_times = []\n",
        "\n",
        "  for batch_size_index, batch_size in enumerate(BATCH_SIZE_LIST):\n",
        "    print('[batch_size: {}]'.format(batch_size))\n",
        "\n",
        "    # データローダを作成\n",
        "    dataset_train = TensorDataset(X_train, y_train)  # 訓練用\n",
        "    dataset_valid = TensorDataset(X_valid, y_valid)  # 精度検証用\n",
        "    loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "    loader_valid = DataLoader(dataset_valid, batch_size=batch_size)\n",
        "    \n",
        "    net = q77_78_Net(300, 4)\n",
        "    net.to(device)\n",
        "    \n",
        "    # 最適化手法として確率的勾配降下法(SGD)を指定\n",
        "    # 実は内部でsoftmaxによる確率分布化も実行してくれるらしい-> https://qiita.com/mathlive/items/8e1f9a8467fff8dfd03c\n",
        "    optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE_SGD)\n",
        "\n",
        "    # ロス関数としてクロスエントロピーを指定\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 学習過程の可視化のために正解率と損失の表示\n",
        "    history_train_loss = []\n",
        "    history_train_accuracy = []\n",
        "    history_valid_loss = []\n",
        "    history_valid_accuracy = []\n",
        "    \n",
        "    # シードの固定\n",
        "    np.random.seed(0)\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    torch.cuda.manual_seed(0)\n",
        "\n",
        "    # 時間計測開始\n",
        "    time_start = time.perf_counter()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "      # バッチごとの損失や正解率の総和\n",
        "      total_train_loss = 0\n",
        "      total_train_accuracy = 0\n",
        "      total_valid_loss = 0\n",
        "      total_valid_accuracy = 0\n",
        "\n",
        "      ########################\n",
        "      ### 訓練データで学習 ###\n",
        "      ########################\n",
        "\n",
        "      # 学習を行う設定にする(validationでは学習を行わない)\n",
        "      net.train()\n",
        "      torch.set_grad_enabled(True)\n",
        "      \n",
        "      # バッチサイズによる訓練データと検証データの分割数\n",
        "      split_count = 0\n",
        "\n",
        "      for X_train_batch, y_train_batch in loader_train:\n",
        "        # 勾配が累積されないようgradientを0にする\n",
        "        optimizer.zero_grad()\n",
        "        # モデルの出力をとる\n",
        "        output = net(X_train_batch)\n",
        "        # ロスを求める\n",
        "        train_loss_batch = criterion(output, y_train_batch)\n",
        "        total_train_loss += train_loss_batch\n",
        "        # backprop\n",
        "        train_loss_batch.backward()\n",
        "        # パラメータの更新\n",
        "        optimizer.step()\n",
        "\n",
        "        # 訓練データのバッチの正解率を求め、total_train_accuracyに足す\n",
        "        y_train_pred = output.argmax(dim=1, keepdim=True)\n",
        "        y_train_pred = [y[0] for y in y_train_pred]\n",
        "        correct_count = 0\n",
        "        for i in range(len(y_train_batch)):\n",
        "          if torch.eq(y_train_batch[i], y_train_pred[i]):\n",
        "            correct_count += 1\n",
        "        total_train_accuracy += correct_count / len(y_train_batch)\n",
        "\n",
        "        split_count += 1\n",
        "      \n",
        "      # このepochでの損失と正解率を求める(バッチごとの損失の平均値と正解率の平均値)\n",
        "      train_accuracy = total_train_accuracy / split_count\n",
        "      train_loss = total_train_loss / split_count\n",
        "      \n",
        "      # 学習の過程を可視化するためのリストに追加\n",
        "      history_train_loss.append(train_loss)\n",
        "      history_train_accuracy.append(train_accuracy)\n",
        "\n",
        "      ########################\n",
        "      ### 検証データで検証 ###\n",
        "      ########################\n",
        "\n",
        "      # 学習を行わないようにする(検証データで検証を行うため)\n",
        "      net.eval()\n",
        "      \n",
        "      # バッチサイズによる訓練データと検証データの分割数\n",
        "      split_count = 0\n",
        "\n",
        "      for X_valid_batch, y_valid_batch in loader_valid:\n",
        "        # モデルの出力をとる\n",
        "        output = net(X_valid_batch)\n",
        "        # 損失を求める\n",
        "        valid_loss_batch = criterion(output, y_valid_batch)\n",
        "        total_valid_loss += valid_loss_batch\n",
        "        # 検証データでの正解率を求める\n",
        "        y_valid_pred = output.argmax(dim=1, keepdim=True)\n",
        "        y_valid_pred = [y[0] for y in y_valid_pred]\n",
        "        correct_count = 0\n",
        "        for i in range(len(y_valid_batch)):\n",
        "          if torch.eq(y_valid_batch[i], y_valid_pred[i]):\n",
        "            correct_count += 1\n",
        "        total_valid_accuracy += correct_count / len(y_valid_batch)\n",
        "        split_count += 1\n",
        "\n",
        "      # このepochでの損失と正解率を求める(バッチごとの損失の平均値と正解率の平均値)\n",
        "      valid_loss = total_valid_loss / split_count\n",
        "      valid_accuracy = total_valid_accuracy / split_count\n",
        "      \n",
        "      # 学習の過程を可視化するためのリストに追加\n",
        "      history_valid_loss.append(valid_loss)\n",
        "      history_valid_accuracy.append(valid_accuracy)\n",
        "      \n",
        "      print('[{}] 損失(訓練データ): {:.4f}, 正解率(訓練データ): {:.4f}, 損失(検証データ): {:.4f}, 正解率(検証データ): {:.4f}'.format(epoch, train_loss, train_accuracy, valid_loss, valid_accuracy))\n",
        "\n",
        "      # グラフに描画\n",
        "      fig_train_loss.plot()\n",
        "  \n",
        "    # 時間計測終了\n",
        "    time_end = time.perf_counter()\n",
        "    elapsed_times.append((time_end - time_start) / EPOCHS)\n",
        "\n",
        "    fig_train_loss.plot(\n",
        "        fig_xarray_epoch,\n",
        "        history_train_loss,\n",
        "        color=cm.cool(batch_size_index/len(BATCH_SIZE_LIST)),\n",
        "        label='バッチサイズ: {:>4}'.format(batch_size))\n",
        "    fig_train_accuracy.plot(\n",
        "        fig_xarray_epoch,\n",
        "        history_train_accuracy,\n",
        "        color=cm.cool(batch_size_index/len(BATCH_SIZE_LIST)),\n",
        "        label='バッチサイズ: {:>4}'.format(batch_size))\n",
        "    fig_valid_loss.plot(\n",
        "        fig_xarray_epoch,\n",
        "        history_valid_loss,\n",
        "        color=cm.cool(batch_size_index/len(BATCH_SIZE_LIST)),\n",
        "        label='バッチサイズ: {:>4}'.format(batch_size))\n",
        "    fig_valid_accuracy.plot(\n",
        "        fig_xarray_epoch,\n",
        "        history_valid_accuracy,\n",
        "        color=cm.cool(batch_size_index/len(BATCH_SIZE_LIST)),\n",
        "        label='バッチサイズ: {:>4}'.format(batch_size))\n",
        "    fig_train_loss.legend()\n",
        "    fig_train_accuracy.legend()\n",
        "    fig_valid_loss.legend()\n",
        "    fig_valid_accuracy.legend()\n",
        "\n",
        "\n",
        "  for i, elapsed_time in enumerate(elapsed_times):\n",
        "    print('batch_size: {}, elapsed_time: {:.4f}[sec]'.format(BATCH_SIZE_LIST[i], elapsed_time))\n",
        "  \n",
        "  fig_elapsed_time.plot(BATCH_SIZE_LIST, elapsed_times)\n",
        "  fig_elapsed_time.set_xscale('log')\n",
        "  \n",
        "  fig.show()\n",
        "\n",
        "q77_78()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9ylTZX77_mZ",
        "colab_type": "text"
      },
      "source": [
        "### `79.` 多層ニューラルネットワーク"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5htLPABhpRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 79. 多層ニューラルネットワーク\n",
        "\n",
        "import time\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "def q79():\n",
        "  class q79_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.fc1 = nn.Linear(300, 64, bias=True)\n",
        "      self.act1 = nn.ReLU()\n",
        "      self.dropout = nn.Dropout(p=0.5)\n",
        "      self.fc2 = nn.Linear(64, 4, bias=True)\n",
        "      self.act2 = nn.Softmax()\n",
        "      nn.init.kaiming_normal_(self.fc1.weight)\n",
        "      nn.init.xavier_normal_(self.fc2.weight)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x = self.fc1(x)\n",
        "      x = self.act1(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc2(x)\n",
        "      x = self.act2(x)\n",
        "      return x\n",
        "  \n",
        "  # 学習過程でのパラメータや最適化アルゴリズムを保存するディレクトリを作成\n",
        "  dir_name = 'model_states'\n",
        "  if not os.path.exists(dir_name):\n",
        "    os.mkdir(dir_name)\n",
        "\n",
        "  # 定数\n",
        "  BATCH_SIZE = 128\n",
        "  EPOCHS = 100\n",
        "  LEARNING_RATE_SGD = 0.05\n",
        "\n",
        "  # CUDAを使用可能であればCUDAを、そうでなければCPUを使う\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  print(device)\n",
        "\n",
        "  # 訓練データから特徴量行列とラベルベクトルを取り出す\n",
        "  with open('X_train.pickle', 'rb') as f:\n",
        "    X_train = pickle.load(f).to(device)\n",
        "  with open('y_train.pickle', 'rb') as f:\n",
        "    y_train = pickle.load(f).to(device)\n",
        "\n",
        "  # 訓練データから特徴量行列とラベルベクトルを取り出す\n",
        "  with open('X_valid.pickle', 'rb') as f:\n",
        "    X_valid = pickle.load(f).to(device)\n",
        "  with open('y_valid.pickle', 'rb') as f:\n",
        "    y_valid = pickle.load(f).to(device)\n",
        "  \n",
        "  # 事例数を求める\n",
        "  num_train = len(y_train)\n",
        "  num_valid = len(y_valid)\n",
        "    \n",
        "  # データローダを作成\n",
        "  dataset_train = TensorDataset(X_train, y_train)  # 訓練用\n",
        "  dataset_valid = TensorDataset(X_valid, y_valid)  # 精度検証用\n",
        "  loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  loader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)\n",
        "    \n",
        "  net = q79_Net()\n",
        "  net.to(device)\n",
        "    \n",
        "  # 最適化手法として確率的勾配降下法(SGD)を指定\n",
        "  # 実は内部でsoftmaxによる確率分布化も実行してくれるらしい-> https://qiita.com/mathlive/items/8e1f9a8467fff8dfd03c\n",
        "  optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE_SGD)\n",
        "\n",
        "  # ロス関数としてクロスエントロピーを指定\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # 学習過程の可視化のために正解率と損失の表示\n",
        "  history_train_loss = []\n",
        "  history_train_accuracy = []\n",
        "  history_valid_loss = []\n",
        "  history_valid_accuracy = []\n",
        "    \n",
        "  # シードの固定\n",
        "  np.random.seed(0)\n",
        "  random.seed(0)\n",
        "  torch.manual_seed(0)\n",
        "  torch.cuda.manual_seed(0)\n",
        "  \n",
        "  for epoch in range(EPOCHS):\n",
        "    # バッチごとの損失や正解率の総和\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    total_valid_loss = 0\n",
        "    total_valid_accuracy = 0\n",
        "\n",
        "    ########################\n",
        "    ### 訓練データで学習 ###\n",
        "    ########################\n",
        "\n",
        "    # 学習を行う設定にする(validationでは学習を行わない)\n",
        "    net.train()\n",
        "    #torch.set_grad_enabled(True)\n",
        "     \n",
        "    # バッチサイズによる訓練データと検証データの分割数\n",
        "    split_count = 0\n",
        "\n",
        "    for X_train_batch, y_train_batch in loader_train:\n",
        "      # 勾配が累積されないようgradientを0にする\n",
        "      optimizer.zero_grad()\n",
        "      # モデルの出力をとる\n",
        "      output = net(X_train_batch)\n",
        "      # ロスを求める\n",
        "      train_loss_batch = criterion(output, y_train_batch)\n",
        "      total_train_loss += train_loss_batch\n",
        "      # backprop\n",
        "      train_loss_batch.backward()\n",
        "      # パラメータの更新\n",
        "      optimizer.step()\n",
        "\n",
        "      # 訓練データのバッチの正解率を求め、total_train_accuracyに足す\n",
        "      y_train_pred = output.argmax(dim=1, keepdim=True)\n",
        "      y_train_pred = [y[0] for y in y_train_pred]\n",
        "      correct_count = 0\n",
        "      for i in range(len(y_train_batch)):\n",
        "        if torch.eq(y_train_batch[i], y_train_pred[i]):\n",
        "          correct_count += 1\n",
        "      total_train_accuracy += correct_count / len(y_train_batch)\n",
        "\n",
        "      split_count += 1\n",
        "      \n",
        "    # このepochでの損失と正解率を求める(バッチごとの損失の平均値と正解率の平均値)\n",
        "    train_accuracy = total_train_accuracy / split_count\n",
        "    train_loss = total_train_loss / split_count\n",
        "      \n",
        "    # 学習の過程を可視化するためのリストに追加\n",
        "    history_train_loss.append(train_loss)\n",
        "    history_train_accuracy.append(train_accuracy)\n",
        "\n",
        "    ########################\n",
        "    ### 検証データで検証 ###\n",
        "    ########################\n",
        "\n",
        "    # 学習を行わないようにする(検証データで検証を行うため)\n",
        "    net.eval()\n",
        "      \n",
        "    # バッチサイズによる訓練データと検証データの分割数\n",
        "    split_count = 0\n",
        "\n",
        "    for X_valid_batch, y_valid_batch in loader_valid:\n",
        "      # モデルの出力をとる\n",
        "      output = net(X_valid_batch)\n",
        "      # 損失を求める\n",
        "      valid_loss_batch = criterion(output, y_valid_batch)\n",
        "      total_valid_loss += valid_loss_batch\n",
        "      # 検証データでの正解率を求める\n",
        "      y_valid_pred = output.argmax(dim=1, keepdim=True)\n",
        "      y_valid_pred = [y[0] for y in y_valid_pred]\n",
        "      correct_count = 0\n",
        "      for i in range(len(y_valid_batch)):\n",
        "        if torch.eq(y_valid_batch[i], y_valid_pred[i]):\n",
        "          correct_count += 1\n",
        "      total_valid_accuracy += correct_count / len(y_valid_batch)\n",
        "      split_count += 1\n",
        "\n",
        "    # このepochでの損失と正解率を求める(バッチごとの損失の平均値と正解率の平均値)\n",
        "    valid_loss = total_valid_loss / split_count\n",
        "    valid_accuracy = total_valid_accuracy / split_count\n",
        "      \n",
        "    # 学習の過程を可視化するためのリストに追加\n",
        "    history_valid_loss.append(valid_loss)\n",
        "    history_valid_accuracy.append(valid_accuracy)\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      print('[{}] 損失(訓練データ): {:.4f}, 正解率(訓練データ): {:.4f}, 損失(検証データ): {:.4f}, 正解率(検証データ): {:.4f}'.format(epoch, train_loss, train_accuracy, valid_loss, valid_accuracy))\n",
        "\n",
        "  # 学習の進行の様子をグラフで表示\n",
        "  fig = plt.figure(figsize=(10, 4))\n",
        "  \n",
        "  # 損失について\n",
        "  axL = fig.add_subplot(1, 2, 1)\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_train_loss, label='訓練データ')\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_valid_loss, label='検証データ')\n",
        "  axL.set_title('epochごとの損失の変化')\n",
        "  axL.set_xlabel('epoch')\n",
        "  axL.set_ylabel('損失')\n",
        "  axL.legend()\n",
        "  \n",
        "  # 正解率の変化\n",
        "  axR = fig.add_subplot(1, 2, 2)\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_train_accuracy, label='訓練データ')\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_valid_accuracy, label='検証データ')\n",
        "  axR.set_title('epochごとの正解率の変化')\n",
        "  axR.set_xlabel('epoch')\n",
        "  axR.set_ylabel('正解率')\n",
        "  axR.legend()\n",
        "  \n",
        "  fig.show()  \n",
        "\n",
        "q79()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2OQPJK6r0vU",
        "colab_type": "text"
      },
      "source": [
        "## 第9章: RNN, CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DruhTivqAa7",
        "colab_type": "text"
      },
      "source": [
        "### 事前準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X72O-ss-qAxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 第9章\n",
        "!pip install stop-words\n",
        "from stop_words import get_stop_words\n",
        "from tqdm import tqdm\n",
        "!pip install torch==1.3.1+cu100 torchvision==0.4.2+cu100 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "import gensim\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install japanize_matplotlib\n",
        "import japanize_matplotlib\n",
        "\n",
        "!pip install transformers\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import transformers as ppb\n",
        "\n",
        "filename_dataset='GoogleNews-vectors-negative300.bin.gz'\n",
        "!mkdir /content/9/\n",
        "%cd /content/9/\n",
        "!wget -O $filename_dataset https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "!gzip -d $filename_dataset\n",
        "\n",
        "# 単語ベクトル列の長さが32になるまでゼロパディングする\n",
        "LENGTH_PAD_TO = 32\n",
        "# 利用する学習済み単語ベクトルの次元数\n",
        "GOOGLE_WORDVEC_DIM = 300\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho8ijpGC8E1V",
        "colab_type": "text"
      },
      "source": [
        "### `80.` ID番号への変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI28GPjir8b3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 80. ID番号への変換\n",
        "# 加えてq70\n",
        "# 頻度2以上のbi-gramがないことがわかったので単語のみでやっています。\n",
        "\n",
        "# 先にq50()を実行してください。\n",
        "\n",
        "!cp /content/6/train.txt /content/9/train.txt\n",
        "!cp /content/6/valid.txt /content/9/valid.txt\n",
        "!cp /content/6/test.txt /content/9/test.txt\n",
        "\n",
        "def words_to_ids(words, word2id_dict):\n",
        "  return [word2id_dict[word] if word in word2id.keys() else 0 for word in words]\n",
        "  \n",
        "def make_word2id_dict():\n",
        "  stop_words = get_stop_words('en')\n",
        "  category2id = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
        "  pattern_only_symbols = re.compile('[^0-9a-zA-Z]+')\n",
        "  pattern_start_with_symbol = re.compile('^[^0-9a-zA-Z].*')\n",
        "  pattern_end_with_symbol = re.compile('.*[^0-9a-zA-Z]$')\n",
        "  files_input = ['train.txt', 'valid.txt', 'test.txt']\n",
        "  files_output = ['train.words.txt', 'valid.words.txt', 'test.words.txt']\n",
        "\n",
        "  # 単語を抽出し、単語列ファイルに書き込み\n",
        "  for file_index in range(len(files_input)):\n",
        "    file_input = open(files_input[file_index], 'r')\n",
        "    file_output = open(files_output[file_index], 'w')\n",
        "    lines = file_input.read().split('\\n')[:-1]\n",
        "    \n",
        "    for line in lines:\n",
        "      record_words = []\n",
        "      category, title = line.split('\\t')\n",
        "      words = title.split(' ')\n",
        "      \n",
        "      for word in words:\n",
        "        # 記号のみの場合記録しない\n",
        "        if pattern_only_symbols.match(word):\n",
        "          continue\n",
        "\n",
        "        # 記号から始まる場合はその記号をスキップした文字列をとる\n",
        "        if pattern_start_with_symbol.match(word):\n",
        "          word = word[1:]\n",
        "        \n",
        "        # 記号で終わる場合はそれまでの文字列をとる\n",
        "        if pattern_end_with_symbol.match(word):\n",
        "          word = word[:-1]\n",
        "        \n",
        "        # ストップワードにある場合は記録しない\n",
        "        if word in stop_words:\n",
        "          continue\n",
        "        \n",
        "        record_words.append(word)\n",
        " \n",
        "      # ファイルに書き込み(単語羅列ファイル)\n",
        "      output = category\n",
        "      for record_word in record_words:\n",
        "        output += (' ' + record_word)\n",
        "      output += '\\n'\n",
        "      file_output.write(output)\n",
        "\n",
        "    file_input.close()\n",
        "    file_output.close()\n",
        "\n",
        "  ### 単語とその頻度のリストを作る\n",
        "  file_input = open('train.words.txt')\n",
        "  word_and_idiom_freq = {}\n",
        "  file_input = open(files_input[file_index], 'r')\n",
        "  lines = file_input.read().split('\\n')[:-1]\n",
        "  for line in lines:\n",
        "    words_and_idioms = line.split(' ')[1:]\n",
        "    for word_and_idiom in words_and_idioms:\n",
        "      if word_and_idiom in word2vec_model:\n",
        "        if word_and_idiom not in word_and_idiom_freq.keys():\n",
        "          word_and_idiom_freq[word_and_idiom] = 1\n",
        "        else:\n",
        "          word_and_idiom_freq[word_and_idiom] += 1\n",
        "  word_and_idiom_freq = sorted(word_and_idiom_freq.items(), key=lambda x:x[1], reverse=True)\n",
        "  file_input.close()\n",
        "  \n",
        "  # 頻度が2以上の単語ごとに単語idをふる\n",
        "  word2id = {}\n",
        "  word_id = 1\n",
        "  for word, freq in word_and_idiom_freq:\n",
        "    if freq > 1:\n",
        "      word2id[word] = word_id\n",
        "      word_id += 1\n",
        "  \n",
        "  return word2id\n",
        "\n",
        "# word2idをグローバル変数としていつでも使えるようにする\n",
        "word2id = make_word2id_dict()\n",
        "def q80():\n",
        "  # id振りのテスト\n",
        "  print('[+]単語とidの対応')\n",
        "  print(word2id)\n",
        "  f = open('train.words.txt', 'r')\n",
        "  words_sample = f.readline()[:-1].split(' ')[1:]\n",
        "  f.close()\n",
        "  print('[+]train.txtの1文目の単語列')\n",
        "  print(words_sample)\n",
        "  ids_sample = words_to_ids(words_sample, word2id)\n",
        "  print('[+]train.txtの1文目の単語列のid列への変換結果')\n",
        "  print(ids_sample)\n",
        "\n",
        "q80()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIEYhunn8H5k",
        "colab_type": "text"
      },
      "source": [
        "### `81.` RNNによる予測"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vB8RgDYD1PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 81. RNNによる予測\n",
        "\n",
        "# ここで4つの関数を定義する。\n",
        "# init_random_seed()  ... 各種乱数のシード値の固定\n",
        "# get_X_ids()         ... 単語列を単語idに変換した訓練データを取得\n",
        "# get_X_wordvecs()    ... 単語列を単語ベクトルに変換した訓練データを取得\n",
        "# get_y()             ... ラベル（正解のカテゴリの列）を取得\n",
        "\n",
        "# シード値の固定\n",
        "def init_random_seed():\n",
        "  np.random.seed(0)\n",
        "  random.seed(0)\n",
        "  torch.manual_seed(0)\n",
        "  torch.cuda.manual_seed(0)\n",
        "\n",
        "# 単語列を単語idに変換した訓練データを取得\n",
        "def get_X_ids(device):\n",
        "  files_input = ['train.words.txt', 'valid.words.txt', 'test.words.txt']\n",
        "  X = [[] for i in range(len(files_input))]\n",
        "  \n",
        "  for file_index in range(len(files_input)):\n",
        "    file_input = open(files_input[file_index], 'r')\n",
        "    lines = file_input.read().split('\\n')[:-1]\n",
        "    \n",
        "    # 単語列をid列に変換\n",
        "    for line in lines:\n",
        "      words = line.split(' ')[1:]\n",
        "      X[file_index].append([word2id[word] if word in word2id.keys() else 0 for word in words])\n",
        "    \n",
        "    # パディング\n",
        "    for line_index in range(len(X[file_index])):\n",
        "      X[file_index][line_index].extend([0 for _ in range(LENGTH_PAD_TO - len(X[file_index][line_index]))])\n",
        "    \n",
        "    # ファイルを閉じる\n",
        "    file_input.close()\n",
        "\n",
        "  # train, valid, testの順に返す\n",
        "  return torch.tensor(X[0], device=device), torch.tensor(X[1], device=device), torch.tensor(X[2], device=device)\n",
        "\n",
        "# 単語列を単語ベクトルに変換した訓練データを取得\n",
        "def get_X_wordvecs(device=torch.device('cpu')):\n",
        "  files_input = ['train.words.txt', 'valid.words.txt', 'test.words.txt']\n",
        "  X = [[] for i in range(len(files_input))]\n",
        "  \n",
        "  for file_index in range(len(files_input)):\n",
        "    file_input = open(files_input[file_index], 'r')\n",
        "    lines = file_input.read().split('\\n')[:-1]\n",
        "    \n",
        "    # 単語列を学習済み単語ベクトルに変換\n",
        "    for line in lines:\n",
        "      words = line.split(' ')[1:]\n",
        "      X[file_index].append([list(word2vec_model[word]) if word in word2vec_model else [0 for _ in range(GOOGLE_WORDVEC_DIM)] for word in words])\n",
        "      \n",
        "    # パディング\n",
        "    for line_index in range(len(X[file_index])):\n",
        "      X[file_index][line_index].extend([[0 for _ in range(GOOGLE_WORDVEC_DIM)] for j in range(LENGTH_PAD_TO - len(X[file_index][line_index]))])\n",
        "    file_input.close()\n",
        "    \n",
        "  # train, valid, testの順に返す\n",
        "  return torch.tensor(X[0], device=device), torch.tensor(X[1], device=device), torch.tensor(X[2], device=device)\n",
        "\n",
        "# ラベル（正解のカテゴリの列）を取得\n",
        "def get_y(device=torch.device('cpu')):\n",
        "  category2id = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
        "  files_input = ['train.words.txt', 'valid.words.txt', 'test.words.txt']\n",
        "  y = []\n",
        "  for file_index in range(len(files_input)):\n",
        "    file_input = open(files_input[file_index], 'r')\n",
        "    lines = file_input.readlines()\n",
        "    y.append([[category2id[line[0]]] for line in lines])\n",
        "  \n",
        "  # train, valid, testの順に返す\n",
        "  return torch.tensor(y[0], device=device), torch.tensor(y[1], device=device), torch.tensor(y[2], device=device)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "def q81():\n",
        "  class q81_RNN(nn.Module):\n",
        "    def __init__(self, dw, dh, dy):\n",
        "      super(q81_RNN, self).__init__()\n",
        "      self.dw = dw # 単語埋め込み(one-hotまたはword2vec)の次元数: 本問では単語の種類数\n",
        "      self.dh = dh # 隠れ状態の次元数(今回は50に設定)\n",
        "      self.dy = dy # 予測の次元数(今回は4カテゴリの確率分布が出力なので4)\n",
        "      self.embedding = nn.Embedding(dw, dh, padding_idx=0)\n",
        "      self.fc_i2h = nn.Linear(dh, dh, bias=True)\n",
        "      self.fc_h2h = nn.Linear(dh, dh, bias=True)\n",
        "      self.fc_h2y = nn.Linear(dh, dy, bias=True)\n",
        "      self.act_tanh = nn.Tanh()\n",
        "      self.act_softmax = nn.Softmax(dim=1)\n",
        "      self.hidden_state = torch.zeros(1, dh)\n",
        "      nn.init.xavier_normal_(self.fc_i2h.weight)\n",
        "      nn.init.xavier_normal_(self.fc_h2h.weight)\n",
        "    \n",
        "    # 隠れ状態ベクトルの初期化\n",
        "    def init_hidden_state(self):\n",
        "      self.hidden_state = torch.zeros(1, self.dh)\n",
        "\n",
        "    # 今の状態から次の状態へと1回だけ実行し、隠れ状態を返す\n",
        "    def forward_t(self, input_t, prev_hidden_state):\n",
        "      new_hidden_state = self.act_tanh(\n",
        "        self.fc_i2h(self.embedding(input_t))\n",
        "        + self.fc_h2h(prev_hidden_state)\n",
        "        )\n",
        "      return new_hidden_state\n",
        "    \n",
        "    # 単語埋め込みの列で一気に学習し、yとhiddenを返す\n",
        "    def forward(self, inputs):\n",
        "      self.init_hidden_state()\n",
        "      for input_t in inputs:\n",
        "        self.hidden_state = self.forward_t(input_t, self.hidden_state)\n",
        "      y = self.act_softmax(self.fc_h2y(self.hidden_state))\n",
        "      return y\n",
        "\n",
        "  # 定数\n",
        "  HIDDEN_SIZE = 50\n",
        "\n",
        "  # シード値の固定\n",
        "  init_random_seed()\n",
        "\n",
        "  # CPUかGPUの指定\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "  # 訓練データ、検証データ、評価データの取得\n",
        "  X_train, X_valid, X_test = get_X_ids(device)\n",
        "  y_train, y_valid, y_test = get_y(device)\n",
        "  \n",
        "  # ファイルの内容、単語id列の確認\n",
        "  f = open('train.words.txt', 'r')\n",
        "  words_sample = f.readline()[:-1].split(' ')[1:]\n",
        "  f.close()\n",
        "  print('[+]train.txtの1文目の単語列')\n",
        "  print(words_sample)\n",
        "  print('[+]train.txtの1文目の単語列のid列への変換結果')\n",
        "  print(X_train[0])\n",
        "\n",
        "  # RNNインスタンスの生成\n",
        "  rnn = q81_RNN(len(word2id), 50, 4)\n",
        "  # 一文目について確率分布を出力\n",
        "  print('[+] 1文目での確率分布の出力')\n",
        "  print(rnn.forward(X_train[0]))\n",
        "\n",
        "q81()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8vC28iz8Kgn",
        "colab_type": "text"
      },
      "source": [
        "### `82.` 確率的勾配降下法による学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-pbH7smEs6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 82. 確率的勾配降下法による学習\n",
        "\n",
        "def q82():\n",
        "  class MyRNN(nn.Module):\n",
        "    def __init__(self, dw, dh, dy):\n",
        "      super(MyRNN, self).__init__()\n",
        "      self.dw = dw # 単語埋め込み(one-hotまたはword2vec)の次元数: 本問では単語の種類数\n",
        "      self.dh = dh # 隠れ状態の次元数(今回は50に設定)\n",
        "      self.dy = dy # 予測の次元数(今回は4カテゴリの確率分布が出力なので4)\n",
        "      self.embedding = nn.Embedding(dw, dh, padding_idx=0)\n",
        "      self.fc_i2h = nn.Linear(dh, dh, bias=True)\n",
        "      self.fc_h2h = nn.Linear(dh, dh, bias=True)\n",
        "      self.fc_h2y = nn.Linear(dh, dy, bias=True)\n",
        "      self.act_tanh = nn.Tanh()\n",
        "      self.act_softmax = nn.Softmax(dim=1)\n",
        "      self.hidden_state = torch.zeros(1, dh)\n",
        "      nn.init.xavier_normal_(self.fc_i2h.weight)\n",
        "      nn.init.xavier_normal_(self.fc_h2h.weight)\n",
        "    \n",
        "    # 隠れ状態ベクトルの初期化\n",
        "    def init_hidden_state(self):\n",
        "      self.hidden_state = torch.zeros(1, self.dh)\n",
        "\n",
        "    # 今の状態から次の状態へと1回だけ実行し、隠れ状態を返す\n",
        "    def forward_t(self, input_t, prev_hidden_state):\n",
        "      new_hidden_state = self.act_tanh(\n",
        "        self.fc_i2h(self.embedding(input_t))\n",
        "        + self.fc_h2h(prev_hidden_state)\n",
        "        )\n",
        "      return new_hidden_state\n",
        "    \n",
        "    # 単語埋め込みの列で一気に学習し、yとhiddenを返す\n",
        "    def forward(self, inputs):\n",
        "      self.init_hidden_state()\n",
        "      for input_t in inputs:\n",
        "        if torch.eq(input_t, torch.tensor(0)):\n",
        "          continue\n",
        "        self.hidden_state = self.forward_t(input_t, self.hidden_state)\n",
        "      y = self.act_softmax(self.fc_h2y(self.hidden_state))\n",
        "      return y\n",
        "\n",
        "  # 定数\n",
        "  HIDDEN_SIZE = 50\n",
        "  EPOCHS = 10\n",
        "  LEARNING_RATE_SGD = 4\n",
        "\n",
        "  # シード値の固定\n",
        "  init_random_seed()\n",
        "\n",
        "  # CPUかGPUの指定\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "  # 訓練データ、検証データ、評価データの取得\n",
        "  X_train, X_valid, X_test = get_X_ids(device=device)\n",
        "  y_train, y_valid, y_test = get_y(device=device)\n",
        "\n",
        "  # 事例数の取得\n",
        "  num_train = len(X_train)\n",
        "  num_valid = len(X_valid)\n",
        "  num_test = len(X_test)\n",
        "\n",
        "  rnn = MyRNN(len(word2id) + 1, 50, 4) # idが降られた語に加えidが0(idを振らなかった)の分も必要\n",
        "  rnn.to(device)\n",
        "\n",
        "  # 最適化手法として確率的勾配降下法(SGD)を指定\n",
        "  optimizer = optim.SGD(rnn.parameters(), lr=LEARNING_RATE_SGD)\n",
        "\n",
        "  # ロス関数としてクロスエントロピーを指定\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # 学習過程の可視化のために正解率と損失の表示\n",
        "  history_train_loss = []\n",
        "  history_train_accuracy = []\n",
        "  history_valid_loss = []\n",
        "  history_valid_accuracy = []\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    \n",
        "    ########################\n",
        "    ### 訓練データで学習 ###\n",
        "    ########################\n",
        "\n",
        "    # 学習モードにする\n",
        "    rnn.train()\n",
        "\n",
        "    # 全ての文について予測とロスを求める\n",
        "    total_loss = 0\n",
        "    correct_count = 0\n",
        "    outputs = []\n",
        "    for sentence_index in range(num_train):\n",
        "      # 出力（4カテゴリの確率分布）をとる\n",
        "      output = rnn.forward(X_train[sentence_index])\n",
        "      # 損失を計算する\n",
        "      loss = criterion(output, y_train[sentence_index])\n",
        "      total_loss += loss\n",
        "      # 予測が正解か確認する\n",
        "      if torch.eq(output.argmax(dim=1, keepdim=False), y_train[sentence_index]):\n",
        "        correct_count += 1\n",
        "    \n",
        "    # このエポック全体としての正解率と損失を求める\n",
        "    train_loss = total_loss / num_train\n",
        "    history_train_loss.append(train_loss)\n",
        "    train_accuracy = correct_count / num_train\n",
        "    history_train_accuracy.append(train_accuracy)\n",
        "     \n",
        "    # backprop\n",
        "    train_loss.backward()\n",
        "\n",
        "    ########################\n",
        "    ### 検証データで検証 ###\n",
        "    ########################\n",
        "\n",
        "    # 評価モードにする\n",
        "    rnn.eval()\n",
        "\n",
        "    # 全ての文について予測とロスを求める\n",
        "    total_loss = 0\n",
        "    correct_count = 0\n",
        "    outputs = []\n",
        "    for sentence_index in range(num_valid):\n",
        "      # 出力（4カテゴリの確率分布）をとる\n",
        "      output = rnn.forward(X_valid[sentence_index])\n",
        "      # 損失を計算する\n",
        "      loss = criterion(output, y_valid[sentence_index])\n",
        "      total_loss += loss\n",
        "      # 予測が正解か確認する\n",
        "      if torch.eq(output.argmax(dim=1, keepdim=False), y_valid[sentence_index]):\n",
        "        correct_count += 1\n",
        "    \n",
        "    # このエポック全体としての正解率と損失を求める\n",
        "    valid_loss = total_loss / num_valid\n",
        "    history_valid_loss.append(valid_loss)\n",
        "    valid_accuracy = correct_count / num_valid\n",
        "    history_valid_accuracy.append(valid_accuracy)\n",
        "\n",
        "    # epochごとの損失と正解率を表示\n",
        "    print('[{}] 損失(訓練データ): {:.4f}, 正解率(訓練データ): {:.4f}, 損失(検証データ): {:.4f}, 正解率(検証データ): {:.4f}'.format(epoch, train_loss, train_accuracy, valid_loss, valid_accuracy))\n",
        "\n",
        "    # パラメータの更新\n",
        "    optimizer.step()\n",
        "\n",
        "  ####################\n",
        "  ### 結果の可視化 ###\n",
        "  ####################\n",
        "\n",
        "  # 学習の進行の様子をグラフで表示\n",
        "  fig = plt.figure(figsize=(10, 4))\n",
        "  \n",
        "  # 損失について\n",
        "  axL = fig.add_subplot(1, 2, 1)\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_train_loss, label='訓練データ')\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_valid_loss, label='検証データ')\n",
        "  axL.set_title('epochごとの損失の変化')\n",
        "  axL.set_xlabel('epoch')\n",
        "  axL.set_ylabel('損失')\n",
        "  axL.legend()\n",
        "  \n",
        "  # 正解率の変化\n",
        "  axR = fig.add_subplot(1, 2, 2)\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_train_accuracy, label='訓練データ')\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_valid_accuracy, label='検証データ')\n",
        "  axR.set_title('epochごとの正解率の変化')\n",
        "  axR.set_xlabel('epoch')\n",
        "  axR.set_ylabel('正解率')\n",
        "  axR.set_ylim(0, 1)\n",
        "  axR.legend()\n",
        "  \n",
        "  fig.show()\n",
        "    \n",
        "q82()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LTFd8SV8Nhh",
        "colab_type": "text"
      },
      "source": [
        "### `83.` ミニバッチ化・GPU上での学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzQJN_JrfJ6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 83. ミニバッチ化・GPU上での学習\n",
        "\n",
        "def q83():\n",
        "  class q83_RNN(nn.Module):\n",
        "    def __init__(self, dw, dh, dy, device='cpu'):\n",
        "      super(q83_RNN, self).__init__()\n",
        "      self.device = device\n",
        "      self.dw = dw # 単語埋め込み(one-hotまたはword2vec)の次元数: 本問では単語の種類だけ次元がある\n",
        "      self.dh = dh # 隠れ状態の次元数\n",
        "      self.dy = dy # 予測の次元数(今回は4カテゴリの確率分布が出力なので4)\n",
        "      self.embedding = nn.Embedding(dw, dh, padding_idx=0)\n",
        "      self.fc_i2h = nn.Linear(dh, dh, bias=True)\n",
        "      self.fc_h2h = nn.Linear(dh, dh, bias=True)\n",
        "      self.fc_h2y = nn.Linear(dh, dy, bias=True)\n",
        "      self.act_tanh = nn.Tanh()\n",
        "      self.act_softmax = nn.Softmax(dim=1)\n",
        "      self.hidden_state = torch.zeros(1, dh)\n",
        "      nn.init.xavier_normal_(self.fc_i2h.weight)\n",
        "      nn.init.xavier_normal_(self.fc_h2h.weight)\n",
        "      nn.init.xavier_normal_(self.fc_h2y.weight)\n",
        "    \n",
        "    # 隠れ状態ベクトルの初期化\n",
        "    def init_hidden_state(self):\n",
        "      self.hidden_state = torch.zeros(1, self.dh, device=self.device)\n",
        "\n",
        "    # 今の状態から次の状態へと1回だけ実行し、隠れ状態を返す\n",
        "    def forward_t(self, input_t, prev_hidden_state):\n",
        "      new_hidden_state = self.act_tanh(\n",
        "        self.fc_i2h(self.embedding(input_t))\n",
        "        + self.fc_h2h(prev_hidden_state)\n",
        "        )\n",
        "      return new_hidden_state\n",
        "    \n",
        "    # 単語埋め込みの列で一気に学習し、yを返す\n",
        "    def forward(self, inputs):\n",
        "      self.init_hidden_state()\n",
        "      for input_t in inputs:\n",
        "        if torch.eq(input_t, torch.tensor(0, device=self.device)):\n",
        "          continue\n",
        "        self.hidden_state = self.forward_t(input_t, self.hidden_state)\n",
        "      y = self.act_softmax(self.fc_h2y(self.hidden_state))\n",
        "      return y\n",
        "\n",
        "  # 定数\n",
        "  BATCH_SIZE = 64\n",
        "  HIDDEN_SIZE = 50\n",
        "  EPOCHS = 10\n",
        "  LEARNING_RATE_SGD = 0.01\n",
        "\n",
        "  # CPUかGPUの指定\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  print(device)\n",
        "\n",
        "  # シード値の固定\n",
        "  init_random_seed()\n",
        "\n",
        "  # 訓練データ、検証データ、評価データの取得\n",
        "  X_train, X_valid, X_test = get_X_ids(device)\n",
        "  y_train, y_valid, y_test = get_y(device)\n",
        "\n",
        "  # データローダを作成\n",
        "  dataset_train = TensorDataset(X_train, y_train)  # 訓練用\n",
        "  dataset_valid = TensorDataset(X_valid, y_valid)  # 精度検証用\n",
        "  loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  loader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)\n",
        "\n",
        "  # 事例数の取得\n",
        "  num_train = len(X_train)\n",
        "  num_valid = len(X_valid)\n",
        "  num_test = len(X_test)\n",
        "\n",
        "  rnn = q83_RNN(len(word2id) + 1, 50, 4, device) # idが降られた語に加えidが0(idを振らなかった)の分も必要\n",
        "  rnn.to(device)\n",
        "\n",
        "  # 最適化手法として確率的勾配降下法(SGD)を指定\n",
        "  optimizer = optim.SGD(rnn.parameters(), lr=LEARNING_RATE_SGD)\n",
        "\n",
        "  # ロス関数としてクロスエントロピーを指定\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # 学習過程の可視化のために正解率と損失の表示\n",
        "  history_train_loss = []\n",
        "  history_train_accuracy = []\n",
        "  history_valid_loss = []\n",
        "  history_valid_accuracy = []\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    \n",
        "    ########################\n",
        "    ### 訓練データで学習 ###\n",
        "    ########################\n",
        "\n",
        "    # 学習モードにする\n",
        "    rnn.train()\n",
        "\n",
        "    total_loss = 0    # 正解数\n",
        "    split_count = 0   # バッチサイズによる訓練データと検証データの分割数\n",
        "    correct_count = 0 # 正解数\n",
        "    for X_train_batch, y_train_batch in loader_train:\n",
        "      # 勾配が累積されないようgradientを0にする\n",
        "      optimizer.zero_grad()\n",
        "      # バッチ内での損失\n",
        "      batch_train_loss = 0\n",
        "\n",
        "      # バッチ内での損失と正解数を求める\n",
        "      for sentence_index in range(len(X_train_batch)):\n",
        "        # 出力（4カテゴリの確率分布）をとる\n",
        "        output = rnn.forward(X_train_batch[sentence_index])\n",
        "        # 損失を計算する\n",
        "        loss = criterion(output, y_train_batch[sentence_index])\n",
        "        batch_train_loss += loss\n",
        "        # 予測が正解か確認する\n",
        "        if torch.eq(output.argmax(dim=1, keepdim=False), y_train_batch[sentence_index]):\n",
        "          correct_count += 1\n",
        "        split_count += 1\n",
        "      total_loss += batch_train_loss\n",
        "      \n",
        "      # backprop\n",
        "      batch_train_loss.backward()\n",
        "      # パラメータの更新\n",
        "      optimizer.step()\n",
        "\n",
        "    # 訓練データにおけるこのエポック全体での正解率と損失を求める\n",
        "    train_loss = total_loss / split_count\n",
        "    history_train_loss.append(train_loss)\n",
        "    train_accuracy = correct_count / num_train\n",
        "    history_train_accuracy.append(train_accuracy)\n",
        "\n",
        "    ########################\n",
        "    ### 検証データで検証 ###\n",
        "    ########################\n",
        "\n",
        "    # 評価モードにする\n",
        "    #rnn.eval()\n",
        "\n",
        "    total_loss = 0    # 正解数\n",
        "    split_count = 0   # バッチサイズによる訓練データと検証データの分割数\n",
        "    correct_count = 0 # 正解数\n",
        "    for X_valid_batch, y_valid_batch in loader_valid:\n",
        "      # バッチ内での損失\n",
        "      batch_valid_loss = 0\n",
        "      \n",
        "      # バッチ内での損失と正解数を求める\n",
        "      for sentence_index in range(len(X_valid_batch)):\n",
        "        # 出力（4カテゴリの確率分布）をとる\n",
        "        output = rnn.forward(X_valid_batch[sentence_index])\n",
        "        # 損失を計算する\n",
        "        loss = criterion(output, y_valid_batch[sentence_index])\n",
        "        batch_valid_loss += loss\n",
        "        # 予測が正解か確認する\n",
        "        if torch.eq(output.argmax(dim=1, keepdim=False), y_valid_batch[sentence_index]):\n",
        "          correct_count += 1\n",
        "        split_count += 1\n",
        "      total_loss += batch_valid_loss\n",
        "\n",
        "    # 訓練データにおけるこのエポック全体での正解率と損失を求める\n",
        "    valid_loss = total_loss / split_count\n",
        "    history_valid_loss.append(valid_loss)\n",
        "    valid_accuracy = correct_count / num_valid\n",
        "    history_valid_accuracy.append(valid_accuracy)\n",
        "\n",
        "    # epochごとの損失と正解率を表示\n",
        "    print('[{}] 損失(訓練データ): {:.4f}, 正解率(訓練データ): {:.4f}, 損失(検証データ): {:.4f}, 正解率(検証データ): {:.4f}'.format(epoch, train_loss, train_accuracy, valid_loss, valid_accuracy))\n",
        "\n",
        "  ####################\n",
        "  ### 結果の可視化 ###\n",
        "  ####################\n",
        "\n",
        "  # 学習の進行の様子をグラフで表示\n",
        "  fig = plt.figure(figsize=(10, 4))\n",
        "  \n",
        "  # 損失について\n",
        "  axL = fig.add_subplot(1, 2, 1)\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_train_loss, label='訓練データ')\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_valid_loss, label='検証データ')\n",
        "  axL.set_title('epochごとの損失の変化')\n",
        "  axL.set_xlabel('epoch')\n",
        "  axL.set_ylabel('損失')\n",
        "  axL.legend()\n",
        "  \n",
        "  # 正解率の変化\n",
        "  axR = fig.add_subplot(1, 2, 2)\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_train_accuracy, label='訓練データ')\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_valid_accuracy, label='検証データ')\n",
        "  axR.set_title('epochごとの正解率の変化')\n",
        "  axR.set_xlabel('epoch')\n",
        "  axR.set_ylabel('正解率')\n",
        "  axR.legend()\n",
        "  \n",
        "  fig.show()\n",
        "\n",
        "q83()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS51Otn98Rr5",
        "colab_type": "text"
      },
      "source": [
        "### `84.` 単語ベクトルの導入"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwNkTOLGql2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 84. 単語ベクトルの導入\n",
        "from tqdm import tqdm\n",
        "def q84():\n",
        "  class q84_RNN(nn.Module):\n",
        "    def __init__(self, dw, dh, dy, device='cpu'):\n",
        "      super(q84_RNN, self).__init__()\n",
        "      self.device = device\n",
        "      self.dw = dw # 単語埋め込み(one-hotまたはword2vec)の次元数: 本問では300\n",
        "      self.dh = dh # 隠れ状態の次元数(今回は50に設定)\n",
        "      self.dy = dy # 予測の次元数(今回は4カテゴリの確率分布が出力なので4)\n",
        "      self.fc_i2h = nn.Linear(dw, dh, bias=True)\n",
        "      self.fc_h2h = nn.Linear(dh, dh, bias=True)\n",
        "      self.fc_h2y = nn.Linear(dh, dy, bias=True)\n",
        "      self.act_tanh = nn.Tanh()\n",
        "      self.act_softmax = nn.Softmax(dim=1)\n",
        "      self.hidden_state = torch.zeros(1, dh, device=device)\n",
        "      nn.init.xavier_normal_(self.fc_i2h.weight)\n",
        "      nn.init.xavier_normal_(self.fc_h2h.weight)\n",
        "      nn.init.xavier_normal_(self.fc_h2y.weight)\n",
        "    \n",
        "    # 隠れ状態ベクトルの初期化\n",
        "    def init_hidden_state(self):\n",
        "      self.hidden_state = torch.zeros(1, self.dh, device=self.device)\n",
        "\n",
        "    # 今の状態から次の状態へと1回だけ実行し、隠れ状態を返す\n",
        "    def forward_t(self, input_t, prev_hidden_state):\n",
        "      # 学習済み単語ベクトルを使うためEmbedding層は必要ない\n",
        "      new_hidden_state = self.act_tanh(\n",
        "        self.fc_i2h(input_t)\n",
        "        + self.fc_h2h(prev_hidden_state)\n",
        "        )\n",
        "      return new_hidden_state\n",
        "    \n",
        "    # 単語埋め込みの列で一気に学習し、yとhiddenを返す\n",
        "    def forward(self, inputs):\n",
        "      self.init_hidden_state()\n",
        "      for input_t in inputs:\n",
        "        if torch.equal(input_t, torch.tensor([0. for _ in range(GOOGLE_WORDVEC_DIM)], device=self.device)):\n",
        "          continue\n",
        "        self.hidden_state = self.forward_t(input_t, self.hidden_state)\n",
        "      y = self.act_softmax(self.fc_h2y(self.hidden_state))\n",
        "      return y\n",
        "\n",
        "  # 定数\n",
        "  BATCH_SIZE = 64\n",
        "  HIDDEN_SIZE = 50\n",
        "  EPOCHS = 10\n",
        "  LEARNING_RATE_SGD = 0.002\n",
        "\n",
        "  # CPUかGPUの指定\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  print(device)\n",
        "\n",
        "  # シード値の固定\n",
        "  init_random_seed()\n",
        "\n",
        "  # 訓練データ、検証データ、評価データの取得\n",
        "  X_train, X_valid, X_test = get_X_wordvecs(device)\n",
        "  y_train, y_valid, y_test = get_y(device)\n",
        "\n",
        "  # データローダを作成\n",
        "  dataset_train = TensorDataset(X_train, y_train)  # 訓練用\n",
        "  dataset_valid = TensorDataset(X_valid, y_valid)  # 精度検証用\n",
        "  loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  loader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)\n",
        "\n",
        "  # 事例数の取得\n",
        "  print('[+]訓練データの作成中...')\n",
        "  num_train = len(X_train)\n",
        "  print('[+]検証データの作成中...')\n",
        "  num_valid = len(X_valid)\n",
        "  print('[+]評価データの作成中...')\n",
        "  num_test = len(X_test)\n",
        "  print('[+]データセット作成完了')\n",
        "\n",
        "  rnn = q84_RNN(GOOGLE_WORDVEC_DIM, 50, 4, device)\n",
        "  rnn.to(device)\n",
        "\n",
        "  # 最適化手法として確率的勾配降下法(SGD)を指定\n",
        "  optimizer = optim.SGD(rnn.parameters(), lr=LEARNING_RATE_SGD)\n",
        "\n",
        "  # ロス関数としてクロスエントロピーを指定\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # 学習過程の可視化のために正解率と損失の表示\n",
        "  history_train_loss = []\n",
        "  history_train_accuracy = []\n",
        "  history_valid_loss = []\n",
        "  history_valid_accuracy = []\n",
        "\n",
        "  print('[+]学習開始')\n",
        "  for epoch in range(EPOCHS):\n",
        "    \n",
        "    ########################\n",
        "    ### 訓練データで学習 ###\n",
        "    ########################\n",
        "    print('[{}]訓練データで学習中...'.format(epoch))\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 学習モードにする\n",
        "    rnn.train()\n",
        "\n",
        "    total_loss = 0    # 正解数\n",
        "    split_count = 0   # バッチサイズによる訓練データと検証データの分割数\n",
        "    correct_count = 0 # 正解数\n",
        "    for X_train_batch, y_train_batch in tqdm(loader_train):\n",
        "      # 勾配が累積されないようgradientを0にする\n",
        "      optimizer.zero_grad()\n",
        "      # バッチ内での損失\n",
        "      batch_train_loss = 0\n",
        "\n",
        "      # バッチ内での損失と正解数を求める\n",
        "      for sentence_index in range(len(X_train_batch)):\n",
        "        # 出力（4カテゴリの確率分布）をとる\n",
        "        output = rnn.forward(X_train_batch[sentence_index])\n",
        "        # 損失を計算する\n",
        "        loss = criterion(output, y_train_batch[sentence_index])\n",
        "        batch_train_loss += loss\n",
        "        # 予測が正解か確認する\n",
        "        if torch.eq(output.argmax(dim=1, keepdim=False), y_train_batch[sentence_index]):\n",
        "          correct_count += 1\n",
        "        split_count += 1\n",
        "      total_loss += batch_train_loss\n",
        "      \n",
        "      # backprop\n",
        "      batch_train_loss.backward()\n",
        "      # パラメータの更新\n",
        "      optimizer.step()\n",
        "\n",
        "    # 訓練データにおけるこのエポック全体での正解率と損失を求める\n",
        "    train_loss = total_loss / split_count\n",
        "    history_train_loss.append(train_loss)\n",
        "    train_accuracy = correct_count / num_train\n",
        "    history_train_accuracy.append(train_accuracy)\n",
        "    \n",
        "    ########################\n",
        "    ### 検証データで検証 ###\n",
        "    ########################\n",
        "    print('[{}]検証データで検証中...'.format(epoch))\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 評価モードにする\n",
        "    rnn.eval()\n",
        "\n",
        "    total_loss = 0    # 正解数\n",
        "    split_count = 0   # バッチサイズによる訓練データと検証データの分割数\n",
        "    correct_count = 0 # 正解数\n",
        "    for X_valid_batch, y_valid_batch in tqdm(loader_valid):\n",
        "      # バッチ内での損失\n",
        "      batch_valid_loss = 0\n",
        "\n",
        "      # バッチ内での損失と正解数を求める\n",
        "      for sentence_index in range(len(X_valid_batch)):\n",
        "        # 出力（4カテゴリの確率分布）をとる\n",
        "        output = rnn.forward(X_valid_batch[sentence_index])\n",
        "        # 損失を計算する\n",
        "        loss = criterion(output, y_valid_batch[sentence_index])\n",
        "        batch_valid_loss += loss\n",
        "        # 予測が正解か確認する\n",
        "        if torch.eq(output.argmax(dim=1, keepdim=False), y_valid_batch[sentence_index]):\n",
        "          correct_count += 1\n",
        "        split_count += 1\n",
        "      total_loss += batch_valid_loss\n",
        "\n",
        "    # 検証データにおけるこのエポック全体での正解率と損失を求める\n",
        "    valid_loss = total_loss / split_count\n",
        "    history_valid_loss.append(valid_loss)\n",
        "    valid_accuracy = correct_count / num_valid\n",
        "    history_valid_accuracy.append(valid_accuracy)\n",
        "\n",
        "    # epochごとの損失と正解率を表示\n",
        "    print('[{}] 損失(訓練データ): {:.4f}, 正解率(訓練データ): {:.4f}, 損失(検証データ): {:.4f}, 正解率(検証データ): {:.4f}'.format(epoch, train_loss, train_accuracy, valid_loss, valid_accuracy))\n",
        "\n",
        "  ####################\n",
        "  ### 結果の可視化 ###\n",
        "  ####################\n",
        "\n",
        "  # 学習の進行の様子をグラフで表示\n",
        "  fig = plt.figure(figsize=(10, 4))\n",
        "  \n",
        "  # 損失について\n",
        "  axL = fig.add_subplot(1, 2, 1)\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_train_loss, label='訓練データ')\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_valid_loss, label='検証データ')\n",
        "  axL.set_title('epochごとの損失の変化')\n",
        "  axL.set_xlabel('epoch')\n",
        "  axL.set_ylabel('損失')\n",
        "  axL.legend()\n",
        "  \n",
        "  # 正解率の変化\n",
        "  axR = fig.add_subplot(1, 2, 2)\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_train_accuracy, label='訓練データ')\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_valid_accuracy, label='検証データ')\n",
        "  axR.set_title('epochごとの正解率の変化')\n",
        "  axR.set_xlabel('epoch')\n",
        "  axR.set_ylabel('正解率')\n",
        "  axR.legend()\n",
        "  \n",
        "  fig.show()\n",
        "\n",
        "q84()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e66Kq3Lx8Vtj",
        "colab_type": "text"
      },
      "source": [
        "### `85.` 双方向RNN・多層化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cDLBJEu8Zvp",
        "colab_type": "text"
      },
      "source": [
        "#### 85-1. 双方向RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o633LfkYQE-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 85. 双方向RNN・多層化\n",
        "\n",
        "# 85_1. 双方向RNN\n",
        "\n",
        "from tqdm import tqdm\n",
        "def q85_1():\n",
        "  class q85_1_RNN(nn.Module):\n",
        "    def __init__(self, dw, dh, dy, device='cpu'):\n",
        "      super(q85_1_RNN, self).__init__()\n",
        "      self.device = device\n",
        "      self.dw = dw # 単語埋め込み(one-hotまたはword2vec)の次元数: 本問では300\n",
        "      self.dh = dh # 隠れ状態の次元数(今回は50に設定)\n",
        "      self.dy = dy # 予測の次元数(今回は4カテゴリの確率分布が出力なので4)\n",
        "      self.fc_i2h_forward = nn.Linear(dw, dh, bias=True)\n",
        "      self.fc_h2h_forward = nn.Linear(dh, dh, bias=True)\n",
        "      self.fc_i2h_backward = nn.Linear(dw, dh, bias=True)\n",
        "      self.fc_h2h_backward = nn.Linear(dh, dh, bias=True)\n",
        "      self.fc_h2y = nn.Linear(dh * 2, dy, bias=True)\n",
        "      self.act_tanh = nn.Tanh()\n",
        "      self.act_softmax = nn.Softmax()\n",
        "      self.hidden_state_forward = torch.zeros(1, dh, device=device)\n",
        "      self.hidden_state_backward = torch.zeros(1, dh, device=device)\n",
        "      nn.init.xavier_normal_(self.fc_i2h_forward.weight)\n",
        "      nn.init.xavier_normal_(self.fc_h2h_forward.weight)\n",
        "      nn.init.xavier_normal_(self.fc_i2h_backward.weight)\n",
        "      nn.init.xavier_normal_(self.fc_h2h_backward.weight)\n",
        "      nn.init.xavier_normal_(self.fc_h2y.weight)\n",
        "    \n",
        "    # 隠れ状態ベクトルの初期化\n",
        "    def init_hidden_state(self):\n",
        "      self.hidden_state_forward = torch.zeros(1, self.dh, device=self.device)\n",
        "      self.hidden_state_backward = torch.zeros(1, self.dh, device=self.device)\n",
        "\n",
        "    # 順方向へと1回だけ実行し、隠れ状態を返す\n",
        "    def forward_t(self, input_t, prev_hidden_state):\n",
        "      # 学習済み単語ベクトルを使うためEmbedding層は必要ない\n",
        "      new_hidden_state = self.act_tanh(\n",
        "        self.fc_i2h_forward(input_t)\n",
        "        + self.fc_h2h_forward(prev_hidden_state)\n",
        "        )\n",
        "      return new_hidden_state\n",
        "    \n",
        "    # 逆方向へと1回だけ実行し、隠れ状態を返す\n",
        "    def backward_t(self, input_t, prev_hidden_state):\n",
        "      new_hidden_state = self.act_tanh(\n",
        "        self.fc_i2h_backward(input_t)\n",
        "        + self.fc_h2h_backward(prev_hidden_state)\n",
        "        )\n",
        "      return new_hidden_state\n",
        "    \n",
        "    # 出力を行う\n",
        "    def output(self, hidden_state_forward, hidden_state_backward):\n",
        "      y = self.act_softmax(self.fc_h2y(torch.cat((hidden_state_forward, hidden_state_backward), dim=1)))\n",
        "      return y\n",
        "    \n",
        "    # 単語埋め込みの列で一気に学習し、隠れ状態の列を返す\n",
        "    def forward(self, inputs):\n",
        "      hidden_states_forward = []\n",
        "      hidden_states_backward = []\n",
        "      outputs = []\n",
        "      self.init_hidden_state()\n",
        "      T = len(inputs)\n",
        "\n",
        "      # 隠れ状態の系列を求める\n",
        "      for t in range(T):\n",
        "        # 順方向の隠れ状態の更新\n",
        "        if not torch.equal(inputs[t], torch.tensor([0. for _ in range(GOOGLE_WORDVEC_DIM)], device=self.device)):\n",
        "          self.hidden_state_forward = self.forward_t(inputs[t], self.hidden_state_forward)\n",
        "        hidden_states_forward.append(self.hidden_state_forward)\n",
        "        # 逆方向の隠れ状態の更新\n",
        "        if not torch.equal(inputs[(T-1) - t], torch.tensor([0. for _ in range(GOOGLE_WORDVEC_DIM)], device=self.device)):\n",
        "          self.hidden_state_backward = self.backward_t(inputs[(T-1) - t], self.hidden_state_backward)\n",
        "        hidden_states_backward.append(self.hidden_state_backward)\n",
        "      \n",
        "      # 出力の系列を求める\n",
        "      for t in range(T):\n",
        "        outputs.append(self.output(hidden_states_forward[t], hidden_states_backward[(T-1) - t]))\n",
        "      \n",
        "      return hidden_states_forward, hidden_states_backward, outputs\n",
        "  \n",
        "  class MultiLayerRNN(nn.Module):\n",
        "    def __init__(self, dw, dh, dy, device='cpu'):\n",
        "      self.rnn1 = RNN()\n",
        "\n",
        "  # 定数\n",
        "  BATCH_SIZE = 64\n",
        "  HIDDEN_SIZE = 50\n",
        "  EPOCHS = 10\n",
        "  LEARNING_RATE_SGD = 0.002\n",
        "\n",
        "  # CPUかGPUの指定\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  print(device)\n",
        "\n",
        "  # シード値の固定\n",
        "  init_random_seed()\n",
        "\n",
        "  # 訓練データ、検証データ、評価データの取得\n",
        "  X_train, X_valid, X_test = get_X_wordvecs(device)\n",
        "  y_train, y_valid, y_test = get_y(device)\n",
        "\n",
        "  # データローダを作成\n",
        "  dataset_train = TensorDataset(X_train, y_train)  # 訓練用\n",
        "  dataset_valid = TensorDataset(X_valid, y_valid)  # 精度検証用\n",
        "  loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  loader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)\n",
        "\n",
        "  # 事例数の取得\n",
        "  print('[+]訓練データの作成中...')\n",
        "  num_train = len(X_train)\n",
        "  print('[+]検証データの作成中...')\n",
        "  num_valid = len(X_valid)\n",
        "  print('[+]評価データの作成中...')\n",
        "  num_test = len(X_test)\n",
        "  print('[+]データセット作成完了')\n",
        "\n",
        "  rnn = q85_1_RNN(GOOGLE_WORDVEC_DIM, 50, 4, device)\n",
        "  rnn.to(device)\n",
        "\n",
        "  # 最適化手法として確率的勾配降下法(SGD)を指定\n",
        "  optimizer = optim.SGD(rnn.parameters(), lr=LEARNING_RATE_SGD)\n",
        "\n",
        "  # ロス関数としてクロスエントロピーを指定\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # 学習過程の可視化のために正解率と損失の表示\n",
        "  history_train_loss = []\n",
        "  history_train_accuracy = []\n",
        "  history_valid_loss = []\n",
        "  history_valid_accuracy = []\n",
        "\n",
        "  print('[+]学習開始')\n",
        "  for epoch in range(EPOCHS):\n",
        "    \n",
        "    ########################\n",
        "    ### 訓練データで学習 ###\n",
        "    ########################\n",
        "    print('[{}]訓練データで学習中...'.format(epoch))\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 学習モードにする\n",
        "    rnn.train()\n",
        "\n",
        "    total_loss = 0    # 正解数\n",
        "    split_count = 0   # バッチサイズによる訓練データと検証データの分割数\n",
        "    correct_count = 0 # 正解数\n",
        "    for X_train_batch, y_train_batch in tqdm(loader_train, position=0, leave=True):\n",
        "      # 勾配が累積されないようgradientを0にする\n",
        "      optimizer.zero_grad()\n",
        "      # バッチ内での損失\n",
        "      batch_train_loss = 0\n",
        "\n",
        "    \n",
        "      # バッチ内での損失と正解数を求める\n",
        "      for sentence_index in range(len(X_train_batch)):\n",
        "        # 出力（4カテゴリの確率分布）をとる\n",
        "        hidden_states_forward, hidden_states_backward, outputs = rnn.forward(X_train_batch[sentence_index])\n",
        "        # 損失を計算する\n",
        "        loss = criterion(outputs[-1], y_train_batch[sentence_index])\n",
        "        batch_train_loss += loss\n",
        "        # 予測が正解か確認する\n",
        "        if torch.eq(outputs[-1].argmax(dim=1, keepdim=False), y_train_batch[sentence_index]):\n",
        "          correct_count += 1\n",
        "        split_count += 1\n",
        "      total_loss += batch_train_loss\n",
        "      \n",
        "      # backprop\n",
        "      batch_train_loss.backward()\n",
        "      # パラメータの更新\n",
        "      optimizer.step()\n",
        "\n",
        "    # 訓練データにおけるこのエポック全体での正解率と損失を求める\n",
        "    train_loss = total_loss / split_count\n",
        "    history_train_loss.append(train_loss)\n",
        "    train_accuracy = correct_count / num_train\n",
        "    history_train_accuracy.append(train_accuracy)\n",
        "    \n",
        "    ########################\n",
        "    ### 検証データで検証 ###\n",
        "    ########################\n",
        "    print('[{}]検証データで検証中...'.format(epoch))\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 評価モードにする\n",
        "    rnn.eval()\n",
        "\n",
        "    total_loss = 0    # 正解数\n",
        "    split_count = 0   # バッチサイズによる訓練データと検証データの分割数\n",
        "    correct_count = 0 # 正解数\n",
        "    for X_valid_batch, y_valid_batch in tqdm(loader_valid, position=0, leave=True):\n",
        "      # バッチ内での損失\n",
        "      batch_valid_loss = 0\n",
        "\n",
        "      # バッチ内での損失と正解数を求める\n",
        "      for sentence_index in range(len(X_valid_batch)):\n",
        "        # 出力（4カテゴリの確率分布）をとる\n",
        "        hidden_states_forward, hidden_states_backward, outputs = rnn.forward(X_valid_batch[sentence_index])\n",
        "        # 損失を計算する\n",
        "        loss = criterion(outputs[-1], y_valid_batch[sentence_index])\n",
        "        batch_valid_loss += loss\n",
        "        # 予測が正解か確認する\n",
        "        if torch.eq(outputs[-1].argmax(dim=1, keepdim=False), y_valid_batch[sentence_index]):\n",
        "          correct_count += 1\n",
        "        split_count += 1\n",
        "      total_loss += batch_valid_loss\n",
        "\n",
        "    # 検証データにおけるこのエポック全体での正解率と損失を求める\n",
        "    valid_loss = total_loss / split_count\n",
        "    history_valid_loss.append(valid_loss)\n",
        "    valid_accuracy = correct_count / num_valid\n",
        "    history_valid_accuracy.append(valid_accuracy)\n",
        "\n",
        "    # epochごとの損失と正解率を表示\n",
        "    print('[{}] 損失(訓練データ): {:.4f}, 正解率(訓練データ): {:.4f}, 損失(検証データ): {:.4f}, 正解率(検証データ): {:.4f}'.format(epoch, train_loss, train_accuracy, valid_loss, valid_accuracy))\n",
        "\n",
        "  ####################\n",
        "  ### 結果の可視化 ###\n",
        "  ####################\n",
        "\n",
        "  # 学習の進行の様子をグラフで表示\n",
        "  fig = plt.figure(figsize=(10, 4))\n",
        "  \n",
        "  # 損失について\n",
        "  axL = fig.add_subplot(1, 2, 1)\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_train_loss, label='訓練データ')\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_valid_loss, label='検証データ')\n",
        "  axL.set_title('epochごとの損失の変化')\n",
        "  axL.set_xlabel('epoch')\n",
        "  axL.set_ylabel('損失')\n",
        "  axL.legend()\n",
        "  \n",
        "  # 正解率の変化\n",
        "  axR = fig.add_subplot(1, 2, 2)\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_train_accuracy, label='訓練データ')\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_valid_accuracy, label='検証データ')\n",
        "  axR.set_title('epochごとの正解率の変化')\n",
        "  axR.set_xlabel('epoch')\n",
        "  axR.set_ylabel('正解率')\n",
        "  axR.legend()\n",
        "  \n",
        "  fig.show()\n",
        "\n",
        "q85_1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0roZxei8hIg",
        "colab_type": "text"
      },
      "source": [
        "#### 85-2. 多層双方向RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4igt7xQG_2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 85. 双方向RNN・多層化\n",
        "\n",
        "# 85_2. 多層双方向RNN\n",
        "\n",
        "from tqdm import tqdm\n",
        "def q85_2():\n",
        "  # 1層分のRNN: 出力のactivation層は多層RNNのクラスで実装\n",
        "  class q85_2_RNN(nn.Module):\n",
        "    def __init__(self, dw, dh, dy, device='cpu'):\n",
        "      super(q85_2_RNN, self).__init__()\n",
        "      self.device = device\n",
        "      self.dw = dw # 単語埋め込み(one-hotまたはword2vec)の次元数: 本問では300\n",
        "      self.dh = dh # 隠れ状態の次元数\n",
        "      self.dy = dy # 予測の次元数(今回は4カテゴリの確率分布が出力なので4)\n",
        "      self.fc_i2h_forward = nn.Linear(dw, dh, bias=True)\n",
        "      self.fc_h2h_forward = nn.Linear(dh, dh, bias=True)\n",
        "      self.fc_i2h_backward = nn.Linear(dw, dh, bias=True)\n",
        "      self.fc_h2h_backward = nn.Linear(dh, dh, bias=True)\n",
        "      self.fc_h2y = nn.Linear(dh * 2, dy, bias=True)\n",
        "      self.act_tanh = nn.Tanh()\n",
        "      self.hidden_state_forward = torch.zeros(1, dh, device=device)\n",
        "      self.hidden_state_backward = torch.zeros(1, dh, device=device)\n",
        "      nn.init.xavier_normal_(self.fc_i2h_forward.weight)\n",
        "      nn.init.xavier_normal_(self.fc_h2h_forward.weight)\n",
        "      nn.init.xavier_normal_(self.fc_i2h_backward.weight)\n",
        "      nn.init.xavier_normal_(self.fc_h2h_backward.weight)\n",
        "      nn.init.xavier_normal_(self.fc_h2y.weight)\n",
        "    \n",
        "    # 隠れ状態ベクトルの初期化\n",
        "    def init_hidden_state(self):\n",
        "      self.hidden_state_forward = torch.zeros(1, self.dh, device=self.device)\n",
        "      self.hidden_state_backward = torch.zeros(1, self.dh, device=self.device)\n",
        "\n",
        "    # 順方向へと1回だけ実行し、隠れ状態を返す\n",
        "    def forward_t(self, input_t, prev_hidden_state):\n",
        "      # 学習済み単語ベクトルを使うためEmbedding層は必要ない\n",
        "      new_hidden_state = self.act_tanh(\n",
        "        self.fc_i2h_forward(input_t)\n",
        "        + self.fc_h2h_forward(prev_hidden_state)\n",
        "        )\n",
        "      return new_hidden_state\n",
        "    \n",
        "    # 逆方向へと1回だけ実行し、隠れ状態を返す\n",
        "    def backward_t(self, input_t, prev_hidden_state):\n",
        "      new_hidden_state = self.act_tanh(\n",
        "        self.fc_i2h_backward(input_t)\n",
        "        + self.fc_h2h_backward(prev_hidden_state)\n",
        "        )\n",
        "      return new_hidden_state\n",
        "    \n",
        "    # 出力を行う\n",
        "    def output(self, hidden_state_forward, hidden_state_backward):\n",
        "      y = self.fc_h2y(torch.cat((hidden_state_forward, hidden_state_backward))).unsqueeze(0)\n",
        "      return y\n",
        "    \n",
        "    # 単語埋め込みの列で一気に学習し、隠れ状態の列を返す\n",
        "    def forward(self, inputs):\n",
        "      #print(inputs)\n",
        "      T = len(inputs) # 入力系列の長さ\n",
        "      zero_tensor = torch.tensor([0. for _ in range(self.dw)], device=self.device)\n",
        "      hidden_states_forward = torch.zeros(T, self.dh, device=self.device)\n",
        "      hidden_states_backward = torch.zeros(T, self.dh, device=self.device)\n",
        "      outputs = torch.zeros(T, self.dy, device=self.device)\n",
        "      self.init_hidden_state()\n",
        "      \n",
        "      # 隠れ状態の系列を求める\n",
        "      for t in range(T):\n",
        "        # 順方向の隠れ状態の更新\n",
        "        if not torch.equal(inputs[t], zero_tensor):\n",
        "          self.hidden_state_forward = self.forward_t(inputs[t], self.hidden_state_forward)\n",
        "        hidden_states_forward[t] = self.hidden_state_forward\n",
        "        # 逆方向の隠れ状態の更新\n",
        "        if not torch.equal(inputs[(T-1) - t], zero_tensor):\n",
        "          self.hidden_state_backward = self.backward_t(inputs[(T-1) - t], self.hidden_state_backward)\n",
        "        hidden_states_backward[(T-1) - t] = self.hidden_state_backward\n",
        "      \n",
        "      # 出力の系列を求める\n",
        "      for t in range(T):\n",
        "        outputs[t] = self.output(hidden_states_forward[t], hidden_states_backward[(T-1) - t])\n",
        "      \n",
        "      return hidden_states_forward, hidden_states_backward, outputs\n",
        "  \n",
        "  # 2層の双方向RNNを定義\n",
        "  class q85_2_MultiLayerRNN(nn.Module):\n",
        "    def __init__(self, rnn1_params, rnn2_params, device='cpu'):\n",
        "      super(q85_2_MultiLayerRNN, self).__init__()\n",
        "      self.rnn1 = q85_2_RNN(rnn1_params[0], rnn1_params[1], rnn1_params[2], device)\n",
        "      self.rnn2 = q85_2_RNN(rnn2_params[0], rnn2_params[1], rnn2_params[2], device)\n",
        "      self.act1 = nn.ReLU()\n",
        "      self.act2 = nn.Softmax()\n",
        "      \n",
        "    # 最終的な出力は系列の最後のみ\n",
        "    def forward(self, inputs):\n",
        "      _, _, outputs_1 = self.rnn1(inputs)\n",
        "      outputs_1 = self.act1(outputs_1)\n",
        "      _, _, outputs_2 = self.rnn2(outputs_1)\n",
        "      outputs_2 = self.act2(outputs_2)\n",
        "      return outputs_2[-1]\n",
        "\n",
        "  # 定数\n",
        "  BATCH_SIZE = 32\n",
        "  EPOCHS = 10\n",
        "  LEARNING_RATE_SGD = 0.01\n",
        "\n",
        "  # CPUかGPUの指定\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  print(device)\n",
        "\n",
        "  # シード値の固定\n",
        "  init_random_seed()\n",
        "\n",
        "  # 訓練データ、検証データ、評価データの取得\n",
        "  X_train, X_valid, X_test = get_X_wordvecs(device)\n",
        "  y_train, y_valid, y_test = get_y(device)\n",
        "\n",
        "  # データローダを作成\n",
        "  dataset_train = TensorDataset(X_train, y_train)  # 訓練用\n",
        "  dataset_valid = TensorDataset(X_valid, y_valid)  # 精度検証用\n",
        "  loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  loader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)\n",
        "\n",
        "  # 事例数の取得\n",
        "  print('[+]訓練データの作成中...')\n",
        "  num_train = len(X_train)\n",
        "  print('[+]検証データの作成中...')\n",
        "  num_valid = len(X_valid)\n",
        "  print('[+]評価データの作成中...')\n",
        "  num_test = len(X_test)\n",
        "  print('[+]データセット作成完了')\n",
        "\n",
        "  multi_layer_rnn = q85_2_MultiLayerRNN((GOOGLE_WORDVEC_DIM, 100, 64), (64, 100, 4), device)\n",
        "  multi_layer_rnn.to(device)\n",
        "  \n",
        "  # 最適化手法として確率的勾配降下法(SGD)を指定\n",
        "  optimizer = optim.SGD(multi_layer_rnn.parameters(), lr=LEARNING_RATE_SGD)\n",
        "\n",
        "  # ロス関数としてクロスエントロピーを指定\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # 学習過程の可視化のために正解率と損失の表示\n",
        "  history_train_loss = []\n",
        "  history_train_accuracy = []\n",
        "  history_valid_loss = []\n",
        "  history_valid_accuracy = []\n",
        "\n",
        "  print('[+]学習開始')\n",
        "  for epoch in range(EPOCHS):\n",
        "    \n",
        "    ########################\n",
        "    ### 訓練データで学習 ###\n",
        "    ########################\n",
        "    print('[{}]訓練データで学習中...'.format(epoch))\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 学習モードにする\n",
        "    multi_layer_rnn.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    total_loss = 0    # 正解数\n",
        "    split_count = 0   # バッチサイズによる訓練データと検証データの分割数\n",
        "    correct_count = 0 # 正解数\n",
        "    for X_train_batch, y_train_batch in tqdm(loader_train, position=0, leave=True):\n",
        "      # 勾配が累積されないようgradientを0にする\n",
        "      optimizer.zero_grad()\n",
        "      # バッチ内での損失\n",
        "      batch_train_loss = 0\n",
        "    \n",
        "      # バッチ内での損失と正解数を求める\n",
        "      for sentence_index in range(len(X_train_batch)):\n",
        "        # 出力（4カテゴリの確率分布）をとる\n",
        "        output = multi_layer_rnn.forward(X_train_batch[sentence_index]).unsqueeze(0)\n",
        "        # 損失を計算する\n",
        "        loss = criterion(output, y_train_batch[sentence_index])\n",
        "        batch_train_loss += loss\n",
        "        # 予測が正解か確認する\n",
        "        if torch.eq(output.argmax(dim=1, keepdim=False), y_train_batch[sentence_index]):\n",
        "          correct_count += 1\n",
        "        split_count += 1\n",
        "      total_loss += float(batch_train_loss)\n",
        "      \n",
        "      # backprop\n",
        "      batch_train_loss.backward()\n",
        "      # パラメータの更新\n",
        "      optimizer.step()\n",
        "\n",
        "    # 訓練データにおけるこのエポック全体での正解率と損失を求める\n",
        "    train_loss = total_loss / split_count\n",
        "    history_train_loss.append(train_loss)\n",
        "    train_accuracy = correct_count / num_train\n",
        "    history_train_accuracy.append(train_accuracy)\n",
        "    \n",
        "    ########################\n",
        "    ### 検証データで検証 ###\n",
        "    ########################\n",
        "    print('[{}]検証データで検証中...'.format(epoch))\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 評価モードにする\n",
        "    multi_layer_rnn.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    total_loss = 0    # 正解数\n",
        "    split_count = 0   # バッチサイズによる訓練データと検証データの分割数\n",
        "    correct_count = 0 # 正解数\n",
        "    for X_valid_batch, y_valid_batch in tqdm(loader_valid, position=0, leave=True):\n",
        "      # バッチ内での損失\n",
        "      batch_valid_loss = 0\n",
        "\n",
        "      # バッチ内での損失と正解数を求める\n",
        "      for sentence_index in range(len(X_valid_batch)):\n",
        "        # 出力（4カテゴリの確率分布）をとる\n",
        "        output = multi_layer_rnn.forward(X_valid_batch[sentence_index]).unsqueeze(0)\n",
        "        # 損失を計算する\n",
        "        loss = criterion(output, y_valid_batch[sentence_index])\n",
        "        batch_valid_loss += loss\n",
        "        # 予測が正解か確認する\n",
        "        if torch.eq(output.argmax(dim=1, keepdim=False), y_valid_batch[sentence_index]):\n",
        "          correct_count += 1\n",
        "        split_count += 1\n",
        "      total_loss += float(batch_valid_loss)\n",
        "\n",
        "    # 検証データにおけるこのエポック全体での正解率と損失を求める\n",
        "    valid_loss = total_loss / split_count\n",
        "    history_valid_loss.append(valid_loss)\n",
        "    valid_accuracy = correct_count / num_valid\n",
        "    history_valid_accuracy.append(valid_accuracy)\n",
        "\n",
        "    # epochごとの損失と正解率を表示\n",
        "    print('[{}] 損失(訓練データ): {:.4f}, 正解率(訓練データ): {:.4f}, 損失(検証データ): {:.4f}, 正解率(検証データ): {:.4f}'.format(epoch, train_loss, train_accuracy, valid_loss, valid_accuracy))\n",
        "\n",
        "  ####################\n",
        "  ### 結果の可視化 ###\n",
        "  ####################\n",
        "\n",
        "  # 学習の進行の様子をグラフで表示\n",
        "  fig = plt.figure(figsize=(10, 4))\n",
        "  \n",
        "  # 損失について\n",
        "  axL = fig.add_subplot(1, 2, 1)\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_train_loss, label='訓練データ')\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_valid_loss, label='検証データ')\n",
        "  axL.set_title('epochごとの損失の変化')\n",
        "  axL.set_xlabel('epoch')\n",
        "  axL.set_ylabel('損失')\n",
        "  axL.legend()\n",
        "  \n",
        "  # 正解率の変化\n",
        "  axR = fig.add_subplot(1, 2, 2)\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_train_accuracy, label='訓練データ')\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_valid_accuracy, label='検証データ')\n",
        "  axR.set_title('epochごとの正解率の変化')\n",
        "  axR.set_xlabel('epoch')\n",
        "  axR.set_ylabel('正解率')\n",
        "  axR.legend()\n",
        "  \n",
        "  fig.show()\n",
        "\n",
        "q85_2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMTkozTsoseg",
        "colab_type": "text"
      },
      "source": [
        "### `86.` 畳み込みニューラルネットワーク (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zSaAmPz4kti",
        "colab_type": "text"
      },
      "source": [
        "### `87.` 確率的勾配降下法によるCNNの学習\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z4_OCWj8wEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 86. 畳み込みニューラルネットワーク (CNN)\n",
        "### 87. 確率的勾配降下法によるCNNの学習\n",
        "\n",
        "def q86():\n",
        "  class q86_CNN(nn.Module):\n",
        "    def __init__(self, dw, dh, dy, kernel_size, seq_length, device):\n",
        "      super(q86_CNN, self).__init__()\n",
        "      self.dw = dw\n",
        "      self.dh = dh\n",
        "      self.dy = dy\n",
        "      self.kernel_size = kernel_size\n",
        "      self.seq_length = seq_length\n",
        "      self.device = device\n",
        "      self.conv1d = nn.Conv1d(dw, dh, kernel_size, stride=1, bias=True)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.maxpool1d = nn.MaxPool1d(seq_length - (kernel_size - 1))\n",
        "      self.fc_y = nn.Linear(dh, dy, bias=True)\n",
        "      self.softmax = nn.Softmax()\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "      inputs_transposed = torch.transpose(inputs, 1, 2) # 縦方向の単語ベクトルが横方向に並ぶようにtranspose\n",
        "      x = self.conv1d(inputs_transposed) # 1d畳み込み\n",
        "      x = self.relu(x) # 活性化関数を適用\n",
        "      x = self.maxpool1d(x) # a行目には、各時刻における第n次元の値が並んでいる。その最大値をとる\n",
        "      x = x.squeeze(2) # [[[hoge], [fuga], [piyo]]] -> [[hoge, fuga, piyo]]\n",
        "      x = self.fc_y(x) # fully connected: 隠れ状態の次元数(dh)から出力の次元数(dy)へ\n",
        "      x = self.softmax(x) # dyカテゴリの確率分布\n",
        "      return x\n",
        "\n",
        "  # 定数\n",
        "  BATCH_SIZE = 32\n",
        "  EPOCHS = 10\n",
        "  LEARNING_RATE_SGD = 0.01\n",
        "\n",
        "  # CPUかGPUの指定\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  print(device)\n",
        "\n",
        "  # シード値の固定\n",
        "  init_random_seed()\n",
        "\n",
        "  # 訓練データ、検証データ、評価データの取得\n",
        "  X_train, X_valid, X_test = get_X_wordvecs(device)\n",
        "  y_train, y_valid, y_test = get_y(device)\n",
        "\n",
        "  # データローダを作成\n",
        "  dataset_train = TensorDataset(X_train, y_train)  # 訓練用\n",
        "  dataset_valid = TensorDataset(X_valid, y_valid)  # 精度検証用\n",
        "  loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  loader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)\n",
        "\n",
        "  # 事例数の取得\n",
        "  num_train = len(X_train)\n",
        "  num_valid = len(X_valid)\n",
        "  num_test = len(X_test)\n",
        "  \n",
        "  cnn = q86_CNN(GOOGLE_WORDVEC_DIM, 100, 4, 3, LENGTH_PAD_TO, device)\n",
        "  cnn.to(device)\n",
        "  \n",
        "  # 最適化手法として確率的勾配降下法(SGD)を指定\n",
        "  optimizer = optim.SGD(cnn.parameters(), lr=LEARNING_RATE_SGD)\n",
        "\n",
        "  # ロス関数としてクロスエントロピーを指定\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # 学習過程の可視化のために正解率と損失の表示\n",
        "  history_train_loss = []\n",
        "  history_train_accuracy = []\n",
        "  history_valid_loss = []\n",
        "  history_valid_accuracy = []\n",
        "\n",
        "  print('[+]学習開始')\n",
        "  for epoch in range(EPOCHS):\n",
        "    \n",
        "    ########################\n",
        "    ### 訓練データで学習 ###\n",
        "    ########################\n",
        "    print('[{}]訓練データで学習中...'.format(epoch))\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 学習モードにする\n",
        "    cnn.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    total_loss = 0    # 訓練データ全体の損失値\n",
        "    split_count = 0   # バッチサイズによる訓練データの分割数\n",
        "    correct_count = 0 # 正解数\n",
        "    for X_train_batch, y_train_batch in tqdm(loader_train, position=0, leave=True):\n",
        "      # [[1], [2], [3]] -> [1, 2, 3]\n",
        "      y_train_batch = y_train_batch.squeeze(1)\n",
        "      # 勾配が累積されないようgradientを0にする\n",
        "      optimizer.zero_grad()\n",
        "    \n",
        "      # 出力（4カテゴリの確率分布）をとる\n",
        "      output = cnn.forward(X_train_batch)\n",
        "      # 損失を計算する\n",
        "      batch_train_loss = criterion(output, y_train_batch)\n",
        "      # 予測が正解か確認する\n",
        "      y_train_batch_pred = output.argmax(dim=1, keepdim=True).squeeze(1)\n",
        "      for i in range(len(y_train_batch)):\n",
        "        if torch.eq(y_train_batch_pred[i], y_train_batch[i]):\n",
        "          correct_count += 1\n",
        "      split_count += 1\n",
        "      total_loss += float(batch_train_loss)\n",
        "      \n",
        "      # backprop\n",
        "      batch_train_loss.backward()\n",
        "      # パラメータの更新\n",
        "      optimizer.step()\n",
        "\n",
        "    # 訓練データにおけるこのエポック全体での正解率と損失を求める\n",
        "    train_loss = total_loss / split_count\n",
        "    history_train_loss.append(train_loss)\n",
        "    train_accuracy = correct_count / num_train\n",
        "    history_train_accuracy.append(train_accuracy)\n",
        "    \n",
        "    ########################\n",
        "    ### 検証データで検証 ###\n",
        "    ########################\n",
        "    print('[{}]検証データで検証中...'.format(epoch))\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 評価モードにする\n",
        "    cnn.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    total_loss = 0    # 検証データ全体の損失値\n",
        "    split_count = 0   # バッチサイズによる検証データの分割数\n",
        "    correct_count = 0 # 正解数\n",
        "    for X_valid_batch, y_valid_batch in tqdm(loader_valid, position=0, leave=True):\n",
        "      # [[1], [2], [3]] -> [1, 2, 3]\n",
        "      y_valid_batch = y_valid_batch.squeeze(1)\n",
        "\n",
        "      # 出力（4カテゴリの確率分布）をとる\n",
        "      output = cnn.forward(X_valid_batch)\n",
        "      # 損失を計算する\n",
        "      batch_valid_loss = criterion(output, y_valid_batch)\n",
        "      # 予測が正解か確認する\n",
        "      y_valid_batch_pred = output.argmax(dim=1, keepdim=True).squeeze(1)\n",
        "      for i in range(len(y_valid_batch)):\n",
        "        if torch.eq(y_valid_batch_pred[i], y_valid_batch[i]):\n",
        "          correct_count += 1\n",
        "      split_count += 1\n",
        "      total_loss += float(batch_valid_loss)\n",
        "      \n",
        "    # 検証データにおけるこのエポック全体での正解率と損失を求める\n",
        "    valid_loss = total_loss / split_count\n",
        "    history_valid_loss.append(valid_loss)\n",
        "    valid_accuracy = correct_count / num_valid\n",
        "    history_valid_accuracy.append(valid_accuracy)\n",
        "\n",
        "    # epochごとの損失と正解率を表示\n",
        "    print('[{}] 損失(訓練データ): {:.4f}, 正解率(訓練データ): {:.4f}, 損失(検証データ): {:.4f}, 正解率(検証データ): {:.4f}'.format(epoch, train_loss, train_accuracy, valid_loss, valid_accuracy))\n",
        "\n",
        "  ####################\n",
        "  ### 結果の可視化 ###\n",
        "  ####################\n",
        "\n",
        "  # 学習の進行の様子をグラフで表示\n",
        "  fig = plt.figure(figsize=(10, 4))\n",
        "  \n",
        "  # 損失について\n",
        "  axL = fig.add_subplot(1, 2, 1)\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_train_loss, label='訓練データ')\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_valid_loss, label='検証データ')\n",
        "  axL.set_title('epochごとの損失の変化')\n",
        "  axL.set_xlabel('epoch')\n",
        "  axL.set_ylabel('損失')\n",
        "  axL.legend()\n",
        "  \n",
        "  # 正解率の変化\n",
        "  axR = fig.add_subplot(1, 2, 2)\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_train_accuracy, label='訓練データ')\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_valid_accuracy, label='検証データ')\n",
        "  axR.set_title('epochごとの正解率の変化')\n",
        "  axR.set_xlabel('epoch')\n",
        "  axR.set_ylabel('正解率')\n",
        "  axR.legend()\n",
        "  \n",
        "  fig.show()\n",
        "\n",
        "q86()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAcSj65x4cni",
        "colab_type": "text"
      },
      "source": [
        "### `88.` パラメータチューニング"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9plYpWni5mWP",
        "colab_type": "text"
      },
      "source": [
        "#### 88-2. CNNのチューニング\n",
        "カーネルサイズnでn個の単語ベクトルを畳み込んだ結果はそのn-gramの特徴ベクトルといえそう。そこで、2-gram、4-gram、5-gramも入れてみた。  \n",
        "ほんの少し収束が速くなるだけでスコアの改善は見られなかった…"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLCvOICm5luF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 88_2. CNNのチューニング\n",
        "\n",
        "def q88_2():\n",
        "  class q88_2_CNN_n_gram(nn.Module):\n",
        "    def __init__(self, dw, dh, dy, kernel_size, seq_length, device):\n",
        "      super(q88_2_CNN_n_gram, self).__init__()\n",
        "      self.dw = dw\n",
        "      self.dh = dh\n",
        "      self.dy = dy\n",
        "      self.kernel_size = kernel_size\n",
        "      self.seq_length = seq_length\n",
        "      self.device = device\n",
        "      self.conv1d = nn.Conv1d(dw, dh, kernel_size, stride=1, bias=True)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.maxpool1d = nn.MaxPool1d(seq_length - (kernel_size - 1))\n",
        "      self.fc_y = nn.Linear(dh, dy, bias=True)\n",
        "      self.softmax = nn.Softmax()\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "      inputs_transposed = torch.transpose(inputs, 1, 2) # 縦方向の単語ベクトルが横方向に並ぶようにtranspose\n",
        "      x = self.conv1d(inputs_transposed) # 1d畳み込み\n",
        "      x = self.relu(x) # 活性化関数を適用\n",
        "      x = self.maxpool1d(x) # a行目には、各時刻における第n次元の値が並んでいる。その最大値をとる\n",
        "      x = x.squeeze(2) # [[[hoge], [fuga], [piyo]]] -> [[hoge, fuga, piyo]]\n",
        "      return x # 各文書の特徴ベクトルを返す\n",
        "    \n",
        "  class q88_2_CNN(nn.Module):\n",
        "    def __init__(self, dw, dh, dy, seq_length, device):\n",
        "      super(q88_2_CNN, self).__init__()\n",
        "      self.dw = dw\n",
        "      self.dh = dh\n",
        "      self.dy = dy\n",
        "      self.device = device\n",
        "      self.cnn_2_gram = q88_2_CNN_n_gram(dw, dh, dy, 2, seq_length, device)\n",
        "      self.cnn_3_gram = q88_2_CNN_n_gram(dw, dh, dy, 3, seq_length, device)\n",
        "      self.cnn_4_gram = q88_2_CNN_n_gram(dw, dh, dy, 4, seq_length, device)\n",
        "      self.cnn_5_gram = q88_2_CNN_n_gram(dw, dh, dy, 5, seq_length, device)\n",
        "      self.fc_y = nn.Linear(dh * 4, dy, bias=True)\n",
        "      self.softmax = nn.Softmax()\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "      output_2_gram = self.cnn_2_gram(inputs)\n",
        "      output_3_gram = self.cnn_3_gram(inputs)\n",
        "      output_4_gram = self.cnn_4_gram(inputs)\n",
        "      output_5_gram = self.cnn_5_gram(inputs)\n",
        "      feature_vector = torch.cat((output_2_gram, output_3_gram, output_4_gram, output_5_gram), dim=1)\n",
        "      y = self.softmax(self.fc_y(feature_vector))\n",
        "      return y\n",
        "\n",
        "  # 定数\n",
        "  BATCH_SIZE = 32\n",
        "  EPOCHS = 10\n",
        "  LEARNING_RATE_SGD = 0.01\n",
        "\n",
        "  # CPUかGPUの指定\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  print(device)\n",
        "\n",
        "  # シード値の固定\n",
        "  init_random_seed()\n",
        "\n",
        "  # 訓練データ、検証データ、評価データの取得\n",
        "  X_train, X_valid, X_test = get_X_wordvecs(device)\n",
        "  y_train, y_valid, y_test = get_y(device)\n",
        "\n",
        "  # データローダを作成\n",
        "  dataset_train = TensorDataset(X_train, y_train)  # 訓練用\n",
        "  dataset_valid = TensorDataset(X_valid, y_valid)  # 精度検証用\n",
        "  loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  loader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)\n",
        "\n",
        "  # 事例数の取得\n",
        "  num_train = len(X_train)\n",
        "  num_valid = len(X_valid)\n",
        "  num_test = len(X_test)\n",
        "  \n",
        "  cnn = q88_2_CNN(GOOGLE_WORDVEC_DIM, 100, 4, LENGTH_PAD_TO, device)\n",
        "  cnn.to(device)\n",
        "  \n",
        "  # 最適化手法として確率的勾配降下法(SGD)を指定\n",
        "  optimizer = optim.SGD(cnn.parameters(), lr=LEARNING_RATE_SGD)\n",
        "\n",
        "  # ロス関数としてクロスエントロピーを指定\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # 学習過程の可視化のために正解率と損失の表示\n",
        "  history_train_loss = []\n",
        "  history_train_accuracy = []\n",
        "  history_valid_loss = []\n",
        "  history_valid_accuracy = []\n",
        "\n",
        "  print('[+]学習開始')\n",
        "  for epoch in range(EPOCHS):\n",
        "    \n",
        "    ########################\n",
        "    ### 訓練データで学習 ###\n",
        "    ########################\n",
        "    print('[{}]訓練データで学習中...'.format(epoch))\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 学習モードにする\n",
        "    cnn.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    total_loss = 0    # 訓練データ全体の損失値\n",
        "    split_count = 0   # バッチサイズによる訓練データの分割数\n",
        "    correct_count = 0 # 正解数\n",
        "    for X_train_batch, y_train_batch in tqdm(loader_train, position=0, leave=True):\n",
        "      # [[1], [2], [3]] -> [1, 2, 3]\n",
        "      y_train_batch = y_train_batch.squeeze(1)\n",
        "      # 勾配が累積されないようgradientを0にする\n",
        "      optimizer.zero_grad()\n",
        "    \n",
        "      # 出力（4カテゴリの確率分布）をとる\n",
        "      output = cnn.forward(X_train_batch)\n",
        "      # 損失を計算する\n",
        "      batch_train_loss = criterion(output, y_train_batch)\n",
        "      # 予測が正解か確認する\n",
        "      y_train_batch_pred = output.argmax(dim=1, keepdim=True).squeeze(1)\n",
        "      for i in range(len(y_train_batch)):\n",
        "        if torch.eq(y_train_batch_pred[i], y_train_batch[i]):\n",
        "          correct_count += 1\n",
        "      split_count += 1\n",
        "      total_loss += float(batch_train_loss)\n",
        "      \n",
        "      # backprop\n",
        "      batch_train_loss.backward()\n",
        "      # パラメータの更新\n",
        "      optimizer.step()\n",
        "\n",
        "    # 訓練データにおけるこのエポック全体での正解率と損失を求める\n",
        "    train_loss = total_loss / split_count\n",
        "    history_train_loss.append(train_loss)\n",
        "    train_accuracy = correct_count / num_train\n",
        "    history_train_accuracy.append(train_accuracy)\n",
        "    \n",
        "    ########################\n",
        "    ### 検証データで検証 ###\n",
        "    ########################\n",
        "    print('[{}]検証データで検証中...'.format(epoch))\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 評価モードにする\n",
        "    cnn.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    total_loss = 0    # 検証データ全体の損失値\n",
        "    split_count = 0   # バッチサイズによる検証データの分割数\n",
        "    correct_count = 0 # 正解数\n",
        "    for X_valid_batch, y_valid_batch in tqdm(loader_valid, position=0, leave=True):\n",
        "      # [[1], [2], [3]] -> [1, 2, 3]\n",
        "      y_valid_batch = y_valid_batch.squeeze(1)\n",
        "\n",
        "      # 出力（4カテゴリの確率分布）をとる\n",
        "      output = cnn.forward(X_valid_batch)\n",
        "      # 損失を計算する\n",
        "      batch_valid_loss = criterion(output, y_valid_batch)\n",
        "      # 予測が正解か確認する\n",
        "      y_valid_batch_pred = output.argmax(dim=1, keepdim=True).squeeze(1)\n",
        "      for i in range(len(y_valid_batch)):\n",
        "        if torch.eq(y_valid_batch_pred[i], y_valid_batch[i]):\n",
        "          correct_count += 1\n",
        "      split_count += 1\n",
        "      total_loss += float(batch_valid_loss)\n",
        "      \n",
        "    # 検証データにおけるこのエポック全体での正解率と損失を求める\n",
        "    valid_loss = total_loss / split_count\n",
        "    history_valid_loss.append(valid_loss)\n",
        "    valid_accuracy = correct_count / num_valid\n",
        "    history_valid_accuracy.append(valid_accuracy)\n",
        "\n",
        "    # epochごとの損失と正解率を表示\n",
        "    print('[{}] 損失(訓練データ): {:.4f}, 正解率(訓練データ): {:.4f}, 損失(検証データ): {:.4f}, 正解率(検証データ): {:.4f}'.format(epoch, train_loss, train_accuracy, valid_loss, valid_accuracy))\n",
        "\n",
        "  ####################\n",
        "  ### 結果の可視化 ###\n",
        "  ####################\n",
        "\n",
        "  # 学習の進行の様子をグラフで表示\n",
        "  fig = plt.figure(figsize=(10, 4))\n",
        "  \n",
        "  # 損失について\n",
        "  axL = fig.add_subplot(1, 2, 1)\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_train_loss, label='訓練データ')\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_valid_loss, label='検証データ')\n",
        "  axL.set_title('epochごとの損失の変化')\n",
        "  axL.set_xlabel('epoch')\n",
        "  axL.set_ylabel('損失')\n",
        "  axL.legend()\n",
        "  \n",
        "  # 正解率の変化\n",
        "  axR = fig.add_subplot(1, 2, 2)\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_train_accuracy, label='訓練データ')\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_valid_accuracy, label='検証データ')\n",
        "  axR.set_title('epochごとの正解率の変化')\n",
        "  axR.set_xlabel('epoch')\n",
        "  axR.set_ylabel('正解率')\n",
        "  axR.legend()\n",
        "  \n",
        "  fig.show()\n",
        "\n",
        "q88_2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KE7c0qnEhLV",
        "colab_type": "text"
      },
      "source": [
        "### `89.` 事前学習済み言語モデルからの転移学習\n",
        "事前学習済みBERTモデルで各テキストの特徴ベクトルを作り、それを入力として4カテゴリに分類するMLPを学習する。  \n",
        "参考：https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWvOSHkvEo8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "def get_X_pretrained(device):\n",
        "\n",
        "  # Load pretrained model/tokenizer\n",
        "  model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "  tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "  model = model_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "  files_input = ['train.words.txt', 'valid.words.txt', 'test.words.txt']\n",
        "  X = []\n",
        "\n",
        "  # 特徴ベクトルの作成に時間がかかるので、初回実行時は特徴ベクトルの作成とともに保存、pickleが作成してあればロードする\n",
        "  save_path = '/content/9/X_bert_pretrained.pickle'\n",
        "  if os.path.exists(save_path):\n",
        "    print('[+]訓練データ、検証データ、評価データのロード中')\n",
        "    with open(save_path, 'rb') as f:\n",
        "      X = pickle.load(f)\n",
        "    return X\n",
        "\n",
        "  for file_index in range(len(files_input)):\n",
        "    print('[+]訓練データ、検証データ、評価データの作成中({}/3)'.format(file_index + 1))\n",
        "    file_input = open(files_input[file_index], 'r')\n",
        "    lines = file_input.read().split('\\n')[:-1]\n",
        "\n",
        "    # tokenize\n",
        "    tokenized = [tokenizer.encode(line, add_special_tokens=True) for line in lines]\n",
        "\n",
        "    # add padding\n",
        "    tokenized_padded = np.array([tokenized_i + [0] * (LENGTH_PAD_TO - len(tokenized_i)) for tokenized_i in tokenized])\n",
        "    attention_mask = np.where(tokenized_padded != 0, 1, 0)\n",
        "\n",
        "    # 学習済みBERTモデルにより各テキストの特徴ベクトルを767次元の特徴ベクトルで表現\n",
        "    input_ids = torch.tensor(tokenized_padded, device=device)\n",
        "    attention_mask = torch.tensor(attention_mask, device=device)\n",
        "    with torch.no_grad():\n",
        "      last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
        "    X.append(last_hidden_states[0][:,0,:])\n",
        "\n",
        "    file_input.close()\n",
        "  \n",
        "  # pickleの作成\n",
        "  with open(save_path, 'wb') as f:\n",
        "    pickle.dump(X, f)\n",
        "\n",
        "  # train, valid, testの順に返す\n",
        "  return X\n",
        "\n",
        "def q89():\n",
        "  class q89_MLP(nn.Module):\n",
        "    def __init__(self, dw, dh, dy):\n",
        "      super(q89_MLP, self).__init__()\n",
        "      self.fc1 = nn.Linear(dw, dh, bias=True)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.dropout = nn.Dropout(p=0.4)\n",
        "      self.fc2 = nn.Linear(dh, dy, bias=True)\n",
        "      self.softmax = nn.Softmax()\n",
        "      nn.init.kaiming_normal_(self.fc1.weight)\n",
        "      nn.init.xavier_normal_(self.fc2.weight)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "      x = self.fc1(inputs)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc2(x)\n",
        "      x = self.softmax(x)\n",
        "      return x\n",
        "      \n",
        "  # 定数\n",
        "  BATCH_SIZE = 32\n",
        "  EPOCHS = 50\n",
        "  LEARNING_RATE_SGD = 0.01\n",
        "\n",
        "  # CPUかGPUの指定\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  print(device)\n",
        "\n",
        "  # シード値の固定\n",
        "  init_random_seed()\n",
        "\n",
        "  # 訓練データ、検証データ、評価データの取得\n",
        "  print('[+]訓練データ、検証データ、評価データの取得中')\n",
        "  X_train, X_valid, X_test = get_X_pretrained(device)\n",
        "  y_train, y_valid, y_test = get_y(device)\n",
        "\n",
        "  # データローダを作成\n",
        "  dataset_train = TensorDataset(X_train, y_train)  # 訓練用\n",
        "  dataset_valid = TensorDataset(X_valid, y_valid)  # 精度検証用\n",
        "  loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  loader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)\n",
        "\n",
        "  # 事例数の取得\n",
        "  num_train = len(X_train)\n",
        "  num_valid = len(X_valid)\n",
        "  num_test = len(X_test)\n",
        "  \n",
        "  net = q89_MLP(768, 256, 4)\n",
        "  net.to(device)\n",
        "  \n",
        "  # 最適化手法として確率的勾配降下法(SGD)を指定\n",
        "  optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE_SGD)\n",
        "\n",
        "  # ロス関数としてクロスエントロピーを指定\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # 学習過程の可視化のために正解率と損失の表示\n",
        "  history_train_loss = []\n",
        "  history_train_accuracy = []\n",
        "  history_valid_loss = []\n",
        "  history_valid_accuracy = []\n",
        "\n",
        "  print('[+]学習開始')\n",
        "  for epoch in range(EPOCHS):\n",
        "    \n",
        "    ########################\n",
        "    ### 訓練データで学習 ###\n",
        "    ########################\n",
        "    print('[{}]訓練データで学習中...'.format(epoch))\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 学習モードにする\n",
        "    net.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    total_loss = 0    # 訓練データ全体の損失値\n",
        "    split_count = 0   # バッチサイズによる訓練データの分割数\n",
        "    correct_count = 0 # 正解数\n",
        "    for X_train_batch, y_train_batch in tqdm(loader_train, position=0, leave=True):\n",
        "      # [[1], [2], [3]] -> [1, 2, 3]\n",
        "      y_train_batch = y_train_batch.squeeze(1)\n",
        "      # 勾配が累積されないようgradientを0にする\n",
        "      optimizer.zero_grad()\n",
        "    \n",
        "      # 出力（4カテゴリの確率分布）をとる\n",
        "      output = net.forward(X_train_batch)\n",
        "      # 損失を計算する\n",
        "      batch_train_loss = criterion(output, y_train_batch)\n",
        "      # 予測が正解か確認する\n",
        "      y_train_batch_pred = output.argmax(dim=1, keepdim=True).squeeze(1)\n",
        "      for i in range(len(y_train_batch)):\n",
        "        if torch.eq(y_train_batch_pred[i], y_train_batch[i]):\n",
        "          correct_count += 1\n",
        "      split_count += 1\n",
        "      total_loss += float(batch_train_loss)\n",
        "      \n",
        "      # backprop\n",
        "      batch_train_loss.backward()\n",
        "      # パラメータの更新\n",
        "      optimizer.step()\n",
        "\n",
        "    # 訓練データにおけるこのエポック全体での正解率と損失を求める\n",
        "    train_loss = total_loss / split_count\n",
        "    history_train_loss.append(train_loss)\n",
        "    train_accuracy = correct_count / num_train\n",
        "    history_train_accuracy.append(train_accuracy)\n",
        "    \n",
        "    ########################\n",
        "    ### 検証データで検証 ###\n",
        "    ########################\n",
        "    print('[{}]検証データで検証中...'.format(epoch))\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 評価モードにする\n",
        "    net.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    total_loss = 0    # 検証データ全体の損失値\n",
        "    split_count = 0   # バッチサイズによる検証データの分割数\n",
        "    correct_count = 0 # 正解数\n",
        "    for X_valid_batch, y_valid_batch in tqdm(loader_valid, position=0, leave=True):\n",
        "      # [[1], [2], [3]] -> [1, 2, 3]\n",
        "      y_valid_batch = y_valid_batch.squeeze(1)\n",
        "\n",
        "      # 出力（4カテゴリの確率分布）をとる\n",
        "      output = net.forward(X_valid_batch)\n",
        "      # 損失を計算する\n",
        "      batch_valid_loss = criterion(output, y_valid_batch)\n",
        "      # 予測が正解か確認する\n",
        "      y_valid_batch_pred = output.argmax(dim=1, keepdim=True).squeeze(1)\n",
        "      for i in range(len(y_valid_batch)):\n",
        "        if torch.eq(y_valid_batch_pred[i], y_valid_batch[i]):\n",
        "          correct_count += 1\n",
        "      split_count += 1\n",
        "      total_loss += float(batch_valid_loss)\n",
        "      \n",
        "    # 検証データにおけるこのエポック全体での正解率と損失を求める\n",
        "    valid_loss = total_loss / split_count\n",
        "    history_valid_loss.append(valid_loss)\n",
        "    valid_accuracy = correct_count / num_valid\n",
        "    history_valid_accuracy.append(valid_accuracy)\n",
        "\n",
        "    # epochごとの損失と正解率を表示\n",
        "    print('[{}] 損失(訓練データ): {:.4f}, 正解率(訓練データ): {:.4f}, 損失(検証データ): {:.4f}, 正解率(検証データ): {:.4f}'.format(epoch, train_loss, train_accuracy, valid_loss, valid_accuracy))\n",
        "\n",
        "  ####################\n",
        "  ### 結果の可視化 ###\n",
        "  ####################\n",
        "\n",
        "  # 学習の進行の様子をグラフで表示\n",
        "  fig = plt.figure(figsize=(10, 4))\n",
        "  \n",
        "  # 損失について\n",
        "  axL = fig.add_subplot(1, 2, 1)\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_train_loss, label='訓練データ')\n",
        "  axL.plot([i + 1 for i in range(EPOCHS)], history_valid_loss, label='検証データ')\n",
        "  axL.set_title('epochごとの損失の変化')\n",
        "  axL.set_xlabel('epoch')\n",
        "  axL.set_ylabel('損失')\n",
        "  axL.legend()\n",
        "  \n",
        "  # 正解率の変化\n",
        "  axR = fig.add_subplot(1, 2, 2)\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_train_accuracy, label='訓練データ')\n",
        "  axR.plot([i + 1 for i in range(EPOCHS)], history_valid_accuracy, label='検証データ')\n",
        "  axR.set_title('epochごとの正解率の変化')\n",
        "  axR.set_xlabel('epoch')\n",
        "  axR.set_ylabel('正解率')\n",
        "  axR.legend()\n",
        "  \n",
        "  fig.show()\n",
        "\n",
        "q89()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjSo7Fz0otDf",
        "colab_type": "text"
      },
      "source": [
        "## 第10章: 機械翻訳 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZVkQRdEppe-",
        "colab_type": "text"
      },
      "source": [
        "### 事前準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q17SVkRksffE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir /content/10\n",
        "%cd /content/10\n",
        "\n",
        "# KFTTデータセットのダウンロードとchange directory\n",
        "!wget http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz\n",
        "!tar -xvf kftt-data-1.0.tar.gz\n",
        "\n",
        "# JParaCrawlのダウンロード(時間かかる)\n",
        "!wget http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/release/2.0/bitext/en-ja.tar.gz\n",
        "!tar -xvf en-ja.tar.gz\n",
        "\n",
        "# install JUMAN++\n",
        "!wget https://github.com/ku-nlp/jumanpp/releases/download/v2.0.0-rc3/jumanpp-2.0.0-rc3.tar.xz\n",
        "!tar xvf jumanpp-2.0.0-rc3.tar.xz\n",
        "%cd jumanpp-2.0.0-rc3/\n",
        "%mkdir build\n",
        "%cd build/\n",
        "!cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr/local\n",
        "!make -j2\n",
        "!sudo make install\n",
        "%cd /content/10\n",
        "\n",
        "# Download multi-bleu.perl to calculate BLEU score\n",
        "!wget https://github.com/moses-smt/mosesdecoder/raw/master/scripts/generic/multi-bleu.perl\n",
        "\n",
        "# build marian --- https://groups.google.com/forum/#!topic/marian-nmt/uQC6q1PIf3k\n",
        "!apt update\n",
        "!apt-get install -y git cmake build-essential libboost-all-dev libprotobuf10 protobuf-compiler libprotobuf-dev openssl libssl-dev libgoogle-perftools-dev doxygen\n",
        "!sudo apt remove cmake\n",
        "!sudo apt purge --auto-remove cmake\n",
        "!wget https://cmake.org/files/v3.12/cmake-3.12.3-Linux-x86_64.sh\n",
        "!sudo mkdir /opt/cmake\n",
        "!sudo sh cmake-3.12.3-Linux-x86_64.sh --prefix=/opt/cmake --skip-license\n",
        "!rm /usr/local/bin/cmake\n",
        "!sudo ln -s /opt/cmake/bin/cmake /usr/local/bin/cmake\n",
        "!git clone https://github.com/marian-nmt/marian\n",
        "%cd marian\n",
        "!git rev-parse --short HEAD\n",
        "%mkdir build\n",
        "%cd build\n",
        "!cmake .. -DCMAKE_BUILD_TYPE=Release -DUSE_STATIC_LIBS=on\n",
        "!make -j2 -C /content/10/marian/build\n",
        "%cd /content/10\n",
        "%mkdir models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emnz7OMfVv0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install spacy\n",
        "!pip install -U spacy\n",
        "import spacy\n",
        "\n",
        "!pip install tensorboardX\n",
        "!pip install torch==1.3.1+cu100 torchvision==0.4.2+cu100 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install subword-nmt\n",
        "from tqdm import tqdm\n",
        "import gensim\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import tensorboardX as tbx\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install japanize_matplotlib\n",
        "import japanize_matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFoCZwMMphSm",
        "colab_type": "text"
      },
      "source": [
        "### `90.` データの準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCNADLpYosU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 90. データの準備\n",
        "!mkdir /content/10/tokenized/\n",
        "\n",
        "#!jumanpp --help\n",
        "!echo \"[+] applying JUMAN++ v2.0 to kyoto-train.ja\"\n",
        "!jumanpp --output=/content/10/tokenized/kyoto-train.ja --segment /content/10/kftt-data-1.0/data/orig/kyoto-train.ja\n",
        "!echo \"[+] applying JUMAN++ v2.0 to kyoto-dev.ja\"\n",
        "!jumanpp --output=/content/10/tokenized/kyoto-dev.ja --segment /content/10/kftt-data-1.0/data/orig/kyoto-dev.ja\n",
        "!echo \"[+] applying JUMAN++ v2.0 to kyoto-test.ja\"\n",
        "!jumanpp --output=/content/10/tokenized/kyoto-test.ja --segment /content/10/kftt-data-1.0/data/orig/kyoto-test.ja\n",
        "\n",
        "def q90():\n",
        "  path_base_src = '/content/10/kftt-data-1.0/data/orig/'\n",
        "  path_base_dst = '/content/10/tokenized/'\n",
        "  path_srcs_ja = ['kyoto-train.ja', 'kyoto-dev.ja', 'kyoto-test.ja']\n",
        "  path_srcs_en = ['kyoto-train.en', 'kyoto-dev.en', 'kyoto-test.en']\n",
        "  \n",
        "  nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
        "\n",
        "  for path_index in range(3):\n",
        "    # 英語のトークナイズ\n",
        "    f_in_en = open(path_base_src + path_srcs_en[path_index], 'r')\n",
        "    f_out_en = open(path_base_dst + path_srcs_en[path_index], 'w')\n",
        "    print('[+] 英語データのトークナイズ中({}/3)...'.format(path_index + 1))\n",
        "    for line in tqdm(f_in_en.read().split('\\n')[:-1], position=0, leave=True):\n",
        "      f_out_en.write(' '.join([d.text for d in nlp.make_doc(line)]) + '\\n')\n",
        "    f_in_en.close()\n",
        "    f_out_en.close()\n",
        "\n",
        "q90()\n",
        "\n",
        "!head -n 5 /content/10/tokenized/kyoto-train.ja\n",
        "!head -n 5 /content/10/tokenized/kyoto-train.en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6ihTl--5EuG",
        "colab_type": "text"
      },
      "source": [
        "### `91.` 機械翻訳モデルの訓練\n",
        "1epochあたり12分程度かかり、5epochで1時間程度かかります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Ff46Qp7AGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 91. 機械翻訳モデルの訓練\n",
        "\n",
        "%%bash\n",
        "mkdir 91\n",
        "\n",
        "# vocaburaryファイル(単語とidの対応付けファイル)を生成\n",
        "./marian/build/marian-vocab < tokenized/kyoto-train.ja > tokenized/vocab-kyoto-train.ja.yml\n",
        "./marian/build/marian-vocab < tokenized/kyoto-train.en > tokenized/vocab-kyoto-train.en.yml\n",
        "\n",
        "# training\n",
        "./marian/build/marian \\\n",
        "  --model 91/model.npz \\\n",
        "  --type s2s \\\n",
        "  --train-sets tokenized/kyoto-train.ja tokenized/kyoto-train.en \\\n",
        "  --vocabs tokenized/vocab-kyoto-train.ja.yml tokenized/vocab-kyoto-train.en.yml \\\n",
        "  --dim-vocabs 32768 32768 \\\n",
        "  --mini-batch 128 \\\n",
        "  --learn-rate 0.0005 \\\n",
        "  --after-epochs 5 \\\n",
        "  --workspace 4096 \\\n",
        "  --valid-sets tokenized/kyoto-dev.ja tokenized/kyoto-dev.en \\\n",
        "  --valid-metrics cross-entropy bleu \\\n",
        "  --valid-freq 50 \\\n",
        "  --quiet-translation \\\n",
        "  --beam-size 6 --normalize 0.6 \\\n",
        "  --log 91/train.log --valid-log 91/valid.log \\\n",
        "  --seed 1 --exponential-smoothing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcGyr4U9djUl",
        "colab_type": "text"
      },
      "source": [
        "### `92.` 機械翻訳モデルの適用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBNypj6TcxET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 92. 機械翻訳モデルの適用\n",
        "\n",
        "input_str = input('日本語を入力 -> ')\n",
        "!echo -n \"翻訳結果 -> \"\n",
        "!echo $input_str | jumanpp --segment | ./marian/build/marian-decoder --quiet -m 91/model.npz --beam-size 6 -v tokenized/vocab-kyoto-train.ja.yml tokenized/vocab-kyoto-train.en.yml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9OFpspWmND8",
        "colab_type": "text"
      },
      "source": [
        "### `93.` BLEUスコアの計測"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leXcu1_eZsy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 93. BLEUスコアの計測\n",
        "\n",
        "%%bash\n",
        "\n",
        "./marian/build/marian-decoder \\\n",
        "  -m 91/model.npz \\\n",
        "  --vocabs tokenized/vocab-kyoto-train.ja.yml tokenized/vocab-kyoto-train.en.yml \\\n",
        "  --output 91/kyoto-test-result.en \\\n",
        "  --beam-size 6 \\\n",
        "  --quiet \\\n",
        "  < tokenized/kyoto-test.ja\n",
        "\n",
        "perl multi-bleu.perl tokenized/kyoto-test.en < 91/kyoto-test-result.en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9H03zej83Rk",
        "colab_type": "text"
      },
      "source": [
        "### `94.` ビーム探索"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VUdhYNr8286",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 94. ビーム探索 - BLUEスコアのロギング\n",
        "%%bash\n",
        "\n",
        "for beam_size in `seq 1 30`\n",
        "do\n",
        "  ./marian/build/marian-decoder \\\n",
        "    -m 91/model.npz \\\n",
        "    --vocabs tokenized/vocab-kyoto-train.ja.yml tokenized/vocab-kyoto-train.en.yml \\\n",
        "    --output 91/kyoto-test-result.en \\\n",
        "    --beam-size $beam_size \\\n",
        "    --quiet \\\n",
        "    < tokenized/kyoto-test.ja\n",
        "  \n",
        "  perl multi-bleu.perl tokenized/kyoto-test.en < 91/kyoto-test-result.en >> 94-bleu.txt\n",
        "done\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLsaxkDqFpbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 94. ビーム探索 - BLUEスコアのグラフ化 \n",
        "\n",
        "def q94():\n",
        "  with open('94-bleu.txt', 'r') as f:\n",
        "    bleu_multi_list = [float(x) for x in re.findall('BLEU = ([0-9\\.]+)', f.read())]\n",
        "  print(bleu_multi_list)\n",
        "  \n",
        "  # グラフで表示\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "  \n",
        "  # 損失について\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.plot([i + 1 for i in range(30)], bleu_multi_list)\n",
        "  ax.set_title('beam sizeごとのBLEUスコアの推移')\n",
        "  ax.set_xlabel('beam size')\n",
        "  ax.set_ylabel('BLEU score')\n",
        "  \n",
        "q94()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjTienbNSDVK",
        "colab_type": "text"
      },
      "source": [
        "### `95.` サブワード化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUxYDhrGSOM5",
        "colab_type": "text"
      },
      "source": [
        "#### `95-1.` 機械翻訳モデルの訓練"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPMclN4_UFQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### 95-1. 機械翻訳モデルの訓練\n",
        "%%bash\n",
        "\n",
        "mkdir tokenized-subword\n",
        "mkdir 95\n",
        "\n",
        "# 日本語のサブワード化\n",
        "subword-nmt learn-bpe -s 25000 < tokenized/kyoto-train.ja > subword-codes.ja\n",
        "subword-nmt apply-bpe -c subword-codes.ja < tokenized/kyoto-train.ja > tokenized-subword/kyoto-train.ja\n",
        "subword-nmt apply-bpe -c subword-codes.ja < tokenized/kyoto-dev.ja > tokenized-subword/kyoto-dev.ja\n",
        "subword-nmt apply-bpe -c subword-codes.ja < tokenized/kyoto-test.ja > tokenized-subword/kyoto-test.ja\n",
        "\n",
        "# 英語のサブワード化\n",
        "subword-nmt learn-bpe -s 30000 < tokenized/kyoto-train.en > subword-codes.en\n",
        "subword-nmt apply-bpe -c subword-codes.en < tokenized/kyoto-train.en > tokenized-subword/kyoto-train.en\n",
        "subword-nmt apply-bpe -c subword-codes.en < tokenized/kyoto-dev.en > tokenized-subword/kyoto-dev.en\n",
        "subword-nmt apply-bpe -c subword-codes.en < tokenized/kyoto-test.en > tokenized-subword/kyoto-test.en\n",
        "\n",
        "# vocaburaryファイル(単語とidの対応付けファイル)を生成\n",
        "./marian/build/marian-vocab < tokenized-subword/kyoto-train.ja > tokenized-subword/vocab-kyoto-train.ja.yml\n",
        "./marian/build/marian-vocab < tokenized-subword/kyoto-train.en > tokenized-subword/vocab-kyoto-train.en.yml\n",
        "\n",
        "# training\n",
        "./marian/build/marian \\\n",
        "  --model 95/model.npz \\\n",
        "  --type s2s \\\n",
        "  --train-sets tokenized-subword/kyoto-train.ja tokenized-subword/kyoto-train.en \\\n",
        "  --vocabs tokenized-subword/vocab-kyoto-train.ja.yml tokenized-subword/vocab-kyoto-train.en.yml \\\n",
        "  --dim-vocabs 32768 32768 \\\n",
        "  --mini-batch 128 \\\n",
        "  --learn-rate 0.0005 \\\n",
        "  --after-epochs 5 \\\n",
        "  --workspace 4096 \\\n",
        "  --valid-sets tokenized-subword/kyoto-dev.ja tokenized-subword/kyoto-dev.en \\\n",
        "  --valid-metrics cross-entropy bleu \\\n",
        "  --valid-freq 50 \\\n",
        "  --beam-size 6 --normalize 0.6 \\\n",
        "  --log 95/train.log --valid-log 95/valid.log \\\n",
        "  --seed 1 --exponential-smoothing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMCuUL8zDtn1",
        "colab_type": "text"
      },
      "source": [
        "#### `95-2.` 機械翻訳モデルの適用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cVp5vOKDr-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### 95-2. 機械翻訳モデルの適用\n",
        "\n",
        "input_str = input('日本語を入力 -> ')\n",
        "!echo -n \"翻訳結果 -> \"\n",
        "!echo $input_str | jumanpp --segment | subword-nmt apply-bpe -c subword-codes.ja | ./marian/build/marian-decoder --quiet -m 95/model.npz --beam-size 6 --normalize 0.6 -v tokenized-subword/vocab-kyoto-train.ja.yml tokenized-subword/vocab-kyoto-train.en.yml | sed -r 's/(@@ )|(@@ ?$)//g'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO1gHkT-GozH",
        "colab_type": "text"
      },
      "source": [
        "#### `95-3.` BLEUスコアの計測"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh6YRPWtGoZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### 95-3. BLEUスコアの計測\n",
        "\n",
        "%%bash\n",
        "\n",
        "./marian/build/marian-decoder \\\n",
        "  -m 95/model.npz \\\n",
        "  --vocabs tokenized-subword/vocab-kyoto-train.ja.yml tokenized-subword/vocab-kyoto-train.en.yml \\\n",
        "  --output 95/kyoto-test-result.en \\\n",
        "  --beam-size 6 \\\n",
        "  --quiet \\\n",
        "  --quiet-translation \\\n",
        "  < tokenized-subword/kyoto-test.ja\n",
        "\n",
        "sed -r 's/(@@ )|(@@ ?$)//g' 95/kyoto-test-result.en\n",
        "perl multi-bleu.perl tokenized-subword/kyoto-test.en < 95/kyoto-test-result.en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CAUDWNJYMbB",
        "colab_type": "text"
      },
      "source": [
        "#### 95-4. ビーム探索"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6RmZ5J4YW6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 95-4. ビーム探索 - BLUEスコアのロギング\n",
        "\n",
        "%%bash\n",
        "\n",
        "for beam_size in `seq 1 30`\n",
        "do\n",
        "  ./marian/build/marian-decoder \\\n",
        "    -m 95/model.npz \\\n",
        "    --vocabs tokenized-subword/vocab-kyoto-train.ja.yml tokenized-subword/vocab-kyoto-train.en.yml \\\n",
        "    --output 95/kyoto-test-result.en \\\n",
        "    --beam-size $beam_size \\\n",
        "    --quiet \\\n",
        "    < tokenized-subword/kyoto-test.ja\n",
        "\n",
        "  sed -r 's/(@@ )|(@@ ?$)//g' 95/kyoto-test-result.en\n",
        "  perl multi-bleu.perl tokenized-subword/kyoto-test.en < 95/kyoto-test-result.en >> 95-bleu.txt\n",
        "done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUhA7hshdOYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 95-4. ビーム探索 - BLUEスコアのグラフ化 \n",
        "\n",
        "def q95_4():    \n",
        "  with open('95-bleu.txt', 'r') as f:\n",
        "    bleu_multi_list = [float(x) for x in re.findall('BLEU = ([0-9\\.]+)', f.read())]\n",
        "  print(bleu_multi_list)\n",
        "  \n",
        "  # グラフで表示\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "  \n",
        "  # 損失について\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.plot([i + 1 for i in range(30)], bleu_multi_list)\n",
        "  ax.set_title('beam sizeごとのBLEUスコアの推移')\n",
        "  ax.set_xlabel('beam size')\n",
        "  ax.set_ylabel('BLEU score')\n",
        "  \n",
        "q95_4()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukMKuojJi4OD",
        "colab_type": "text"
      },
      "source": [
        "#### おまけ：サブワード化ありなしの比較"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5uE6uYCjAsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('94-bleu.txt', 'r') as f:\n",
        "  bleu_multi_list_94 = [float(x) for x in re.findall('BLEU = ([0-9\\.]+)', f.read())]\n",
        "with open('95-bleu.txt', 'r') as f:\n",
        "  bleu_multi_list_95 = [float(x) for x in re.findall('BLEU = ([0-9\\.]+)', f.read())]\n",
        "\n",
        "# グラフで表示\n",
        "fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "# 損失について\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.plot([i + 1 for i in range(30)], bleu_multi_list_94, label='サブワード化無し')\n",
        "ax.plot([i + 1 for i in range(30)], bleu_multi_list_95, label='サブワード化有り')\n",
        "ax.set_title('beam sizeごとのBLEUスコアの推移')\n",
        "ax.set_xlabel('beam size')\n",
        "ax.set_ylabel('BLEU score')\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJQhb2JnYXq6",
        "colab_type": "text"
      },
      "source": [
        "### `96.` 学習過程の可視化\n",
        "trainのログの表示はできないようなので、validだけ表示します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz-JJA3gjJ5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "writer = tbx.SummaryWriter(log_dir='./96_logs')\n",
        "\n",
        "with open('91/valid.log', 'r') as f:\n",
        "  up_ce = re.findall('Up. ([0-9]+) : cross-entropy : ([0-9\\.]+)', f.read())\n",
        "  up1 = [int(x[0]) for x in up_ce]\n",
        "  ce = [float(x[1]) for x in up_ce]\n",
        "  f.seek(0)\n",
        "  up_bleu = re.findall('Up. ([0-9]+) : bleu : ([0-9\\.]+)', f.read())\n",
        "  up2 = [int(x[0]) for x in up_bleu]\n",
        "  bleu = [float(x[1]) for x in up_bleu]\n",
        "  assert up1 == up2\n",
        "  \n",
        "for i in range(len(up1)):\n",
        "  writer.add_scalar('without_subword_segmentation/loss', ce[i], up1[i])\n",
        "  writer.add_scalar('without_subword_segmentation/bleu', bleu[i], up1[i])\n",
        "\n",
        "with open('95/valid.log', 'r') as f:\n",
        "  up_ce = re.findall('Up. ([0-9]+) : cross-entropy : ([0-9\\.]+)', f.read())\n",
        "  up1 = [int(x[0]) for x in up_ce]\n",
        "  ce = [float(x[1]) for x in up_ce]\n",
        "  f.seek(0)\n",
        "  up_bleu = re.findall('Up. ([0-9]+) : bleu : ([0-9\\.]+)', f.read())\n",
        "  up2 = [int(x[0]) for x in up_bleu]\n",
        "  bleu = [float(x[1]) for x in up_bleu]\n",
        "  assert up1 == up2\n",
        "  \n",
        "for i in range(len(up1)):\n",
        "  writer.add_scalar('with_subword_segmentation/loss', ce[i], up1[i])\n",
        "  writer.add_scalar('with_subword_segmentation/bleu', bleu[i], up1[i])\n",
        "\n",
        "writer.close()\n",
        "\n",
        "%tensorboard --logdir ./96_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOBaYfhvpixl",
        "colab_type": "text"
      },
      "source": [
        "### `97.` ハイパー・パラメータの調整"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxsCcA3AqHhA",
        "colab_type": "text"
      },
      "source": [
        "Q91のときにいろいろ頑張ったのですがデフォルト値がかなりいい感じになってていい値が出なかったのでとりあえずパスします…(´；ω；`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lATXkfu_qTir",
        "colab_type": "text"
      },
      "source": [
        "### `98.` ドメイン適応\n",
        "主なドメイン適応の方法には、大規模コーパスのあるドメインで学習させてから目標ドメインで学習するfine-tuning的な学習方法と、2つのコーパスを混合して学習するマルチドメインでの学習方法があるようです。今回は前者、すなわちJParaCrawl(大規模コーパスのドメイン)で学習してからKFTT(目標ドメイン)で学習する方法でやります。  \n",
        "  \n",
        "参考：[Domain adaptation using Marian · Issue #224 · marian-nmt/marian](https://github.com/marian-nmt/marian/issues/224)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGT-aR0FtkZc",
        "colab_type": "text"
      },
      "source": [
        "#### `98-1.` tokenizeまでの処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9KnPKh7M6nC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# コーパスデータの日本語部分と英語部分を分割\n",
        "# ただし、\"# \"が先頭の文をJUMAN++で形態素に分解しようとすると結果が空になり、以降の実験に支障をきたすので該当する行は含まない。\n",
        "!echo \"[+] JParaCrawlコーパスデータの日本語部分と英語部分を分割中...\"\n",
        "!awk -F '\\t' '{if(substr($4, 1, 2) != \"# \") print $3}' en-ja/en-ja.bicleaner05.txt > en-ja/jparacrawl.en\n",
        "!awk -F '\\t' '{if(substr($4, 1, 2) != \"# \") print $4}' en-ja/en-ja.bicleaner05.txt > en-ja/jparacrawl.ja\n",
        "\n",
        "# 英語のトークナイズ\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
        "f_in = open('en-ja/jparacrawl.en', 'r')\n",
        "f_out = open('tokenized-jparacrawl/jparacrawl-train.en', 'w')\n",
        "print('[+] JParaCrawlの英語データのトークナイズ中...')                                                                  for line in tqdm(f_in, position=0, leave=True):\n",
        "  f_out.write(' '.join([d.text for d in nlp.make_doc(line.rstrip())]) + '\\n')\n",
        "f_in.close()\n",
        "f_out.close()\n",
        "\n",
        "# 日本語のトークナイズ\n",
        "!echo \"[+] JUMAN++による日本語の形態素分解中...\"\n",
        "!jumanpp en-ja/jparacrawl.ja --output=tokenized-jparacrawl/jparacrawl-train.ja --segment\n",
        "\n",
        "# train, validの分割\n",
        "boundary=`expr \\`cat tokenized-jparacrawl/jparacrawl-all.ja | wc -l\\` \\* 90 / 100`\n",
        "!head -n $boundary tokenized-jparacrawl/jparacrawl-all.ja > tokenized-jparacrawl/jparacrawl-train.ja\n",
        "!tail -n +`expr $boundary + 1` tokenized-jparacrawl/jparacrawl-all.ja > tokenized-jparacrawl/jparacrawl-valid.ja\n",
        "!head -n $boundary tokenized-jparacrawl/jparacrawl-all.en > tokenized-jparacrawl/jparacrawl-train.en\n",
        "!tail -n +`expr $boundary + 1` tokenized-jparacrawl/jparacrawl-all.en > tokenized-jparacrawl/jparacrawl-valid.en\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xmdJuPN8E1O",
        "colab_type": "text"
      },
      "source": [
        "#### `98-2.` 機械翻訳モデルの訓練"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2Tznd2u8OIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "mkdir 98\n",
        "\n",
        "# pre-training with JParaCrawl (out-domain)\n",
        "./marian/build/marian \\\n",
        "  --model 98/model.npz \\\n",
        "  --type s2s \\\n",
        "  --train-sets tokenized-jparacrawl/jparacrawl-train.ja tokenized-jparacrawl/jparacrawl-train.en \\\n",
        "  --vocabs tokenized/vocab-kyoto-train.ja.yml tokenized/vocab-kyoto-train.en.yml \\\n",
        "  --dim-vocabs 32768 32768 \\\n",
        "  --mini-batch 64 \\\n",
        "  --learn-rate 0.0005 \\\n",
        "  --after-epochs 2 \\\n",
        "  --workspace 4096 \\\n",
        "  --valid-sets tokenized-jparacrawl/jparacrawl-valid.ja tokenized-jparacrawl/jparacrawl-valid.en \\                        --valid-metrics cross-entropy \\\n",
        "  --quiet-translation \\\n",
        "  --beam-size 6 --normalize 0.6 \\\n",
        "  --log 98/train.log --valid-log 98/valid.log \\\n",
        "  --seed 1 --exponential-smoothing\n",
        "\n",
        "# training with KFTT (in-domain)\n",
        "./marian/build/marian \\                                                                                                   --model 98/model.npz \\                                                                                                  --type s2s \\\n",
        "  --train-sets tokenized/kyoto-train.ja tokenized/kyoto-train.en \\\n",
        "  --vocabs tokenized/vocab-kyoto-train.ja.yml tokenized/vocab-kyoto-train.en.yml \\\n",
        "  --dim-vocabs 32768 32768 \\\n",
        "  --mini-batch 64 \\\n",
        "  --learn-rate 0.0005 \\\n",
        "  --after-epochs 5 \\\n",
        "  --workspace 4096 \\\n",
        "  --valid-sets tokenized/kyoto-dev.ja tokenized/kyoto-dev.en \\                                                            --valid-metrics cross-entropy \\\n",
        "  --quiet-translation \\\n",
        "  --beam-size 6 --normalize 0.6 \\\n",
        "  --log 98/train.log --valid-log 98/valid.log \\\n",
        "  --seed 1 --exponential-smoothing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP-CuBrZhv5X",
        "colab_type": "text"
      },
      "source": [
        "### `99.` 翻訳サーバの構築\n",
        "完成したらDockerコンテナにまとめて公開します。学習済みモデルのサイズが大きくGitHubには載せられないので、公開場所を探します。"
      ]
    }
  ]
}